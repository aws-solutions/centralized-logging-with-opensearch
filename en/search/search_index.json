{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>The Centralized Logging with OpenSearch solution provides comprehensive log management and analysis functions to help you simplify the build of log analytics pipelines. Built on top of Amazon OpenSearch Service, the solution allows you to streamline log ingestion, log processing, and log visualization. You can leverage the solution in multiple use cases such as to abide by security and compliance regulations, achieve refined business operations, and enhance IT troubleshooting and maintenance.</p> <p>The solution has the following features:</p> <ul> <li> <p>All-in-one log ingestion: provides a single web console to ingest both application logs and AWS service logs into the Amazon OpenSearch Service domains. For supported AWS service logs, refer to AWS Service Logs. For supported application logs, refer to Application Logs.</p> </li> <li> <p>Codeless log processor: supports log processor plugins developed by AWS. You are allowed to enrich the raw log data through a few steps on the web console.</p> </li> <li> <p>Out-of-the-box dashboard template: offers a collection of reference designs of visualization templates, for both commonly used software such as Nginx and Apache HTTP Server, and AWS services such as Amazon S3 and Amazon CloudTrail.</p> </li> </ul> <p>This guide includes a getting started chapter to walk you through the process of building log analytics pipelines, and a domain management chapter to introduce how to import Amazon OpenSearch Service domains on the Centralized Logging with OpenSearch web console.</p> <p>This implementation guide describes architectural considerations and configuration steps for deploying the Centralized Logging with OpenSearch solution in the AWS cloud. It includes links to CloudFormation templates that launches and configures the AWS services required to deploy this solution using AWS best practices for security and availability.</p> <p>The guide is intended for IT architects, developers, DevOps, data engineers with practical experience architecting on the AWS Cloud.</p>"},{"location":"implementation-guide/architecture/","title":"Architecture Overview","text":"<p>Deploying this solution with the default parameters builds the following environment in the AWS Cloud.</p> <p> Figure 1: Centralized Logging with OpenSearch architecture</p> <p>This solution deploys the AWS CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li> <p>Amazon CloudFront distributes the frontend web UI assets hosted in Amazon S3 bucket.</p> </li> <li> <p>Amazon Cognito user pool or OpenID Connector (OIDC) can be used for authentication.</p> </li> <li> <p>AWS AppSync provides the backend GraphQL APIs.</p> </li> <li> <p>Amazon DynamoDB stores the solution related information as backend database.</p> </li> <li> <p>AWS Lambda interacts with other AWS Services to process core logic of managing log pipelines or log agents, and obtains information updated in DynamoDB tables.</p> </li> <li> <p>AWS Step Functions orchestrates on-demand AWS CloudFormation deployment of a set of predefined stacks for log pipeline management. The log pipeline stacks deploy separate AWS resources and are used to collect and process logs and ingest them into Amazon OpenSearch Service for further analysis and visualization.</p> </li> <li> <p>Service Log Pipeline or Application Log Pipeline are provisioned on demand via Centralized Logging with OpenSearch console.</p> </li> <li> <p>AWS Systems Manager and Amazon EventBridge manage log agents for collecting logs from application servers, such as installing log agents (Fluent Bit) for application servers and monitoring the health status of the agents.</p> </li> <li> <p>Amazon EC2 or Amazon EKS installs Fluent Bit agents, and uploads log data to application log pipeline.</p> </li> <li> <p>Application log pipelines read, parse, process application logs and ingest them into Amazon OpenSearch domains.</p> </li> <li> <p>Service log pipelines read, parse, process AWS service logs and ingest them into Amazon OpenSearch domains.</p> </li> </ol> <p>This solution supports two types of log pipelines: Service Log Analytics Pipeline and Application Log Analytics Pipeline.</p>"},{"location":"implementation-guide/architecture/#service-log-analytics-pipeline","title":"Service log analytics pipeline","text":"<p>Centralized Logging with OpenSearch supports log analysis for AWS services, such as Amazon S3 access logs, and Application Load Balancer access logs. For a complete list of supported AWS services, refer to Supported AWS Services.</p> <p>This solution ingests different AWS service logs using different workflows.</p> <p>Note</p> <p>Centralized Logging with OpenSearch supports cross-account log ingestion. If you want to ingest the logs from another AWS account, the resources in the Sources group in the architecture diagram will be in another account.</p>"},{"location":"implementation-guide/architecture/#logs-through-amazon-s3","title":"Logs through Amazon S3","text":"<p>This section is applicable to Amazon S3 access logs, CloudFront standard logs, CloudTrail logs (S3), Application Load Balancing access logs, WAF logs, VPC Flow logs (S3), AWS Config logs, Amazon RDS/Aurora logs, and AWS Lambda Logs.</p> <p>The workflow supports two scenarios:</p> <ul> <li> <p>Logs to Amazon S3 directly</p> <p>In this scenario, the service directly sends logs to Amazon S3.</p> <p> Figure 2: Amazon S3 based service log pipeline architecture</p> </li> <li> <p>Logs to Amazon S3 via Kinesis Data Firehose</p> <p>In this scenario, the service cannot directly put their logs to Amazon S3. The logs are sent to Amazon CloudWatch, and Kinesis Data Firehose (KDF) is used to subscribe the logs from CloudWatch Log Group and then put logs into Amazon S3.</p> <p> Figure 3: Amazon S3 (via KDF) based service log pipeline architecture</p> </li> </ul> <p>The log pipeline runs the following workflow:</p> <ol> <li> <p>AWS services logs are stored in Amazon S3 bucket (Log Bucket).</p> </li> <li> <p>An event notification is sent to Amazon SQS using S3 Event Notifications when a new log file is created.</p> </li> <li> <p>Amazon SQS initiates the Log Processor Lambda to run.</p> </li> <li> <p>The log processor reads and processes the log files.</p> </li> <li> <p>The log processor ingests the logs into the Amazon OpenSearch Service.</p> </li> <li> <p>Logs that fail to be processed are exported to Amazon S3 bucket (Backup Bucket).</p> </li> </ol> <p>For cross-account ingestion, the AWS Services store logs in Amazon S3 bucket in the member account, and other resources remain in central logging account.</p>"},{"location":"implementation-guide/architecture/#logs-through-amazon-kinesis-data-streams","title":"Logs through Amazon Kinesis Data Streams","text":"<p>This section is applicable to CloudFront real-time logs, CloudTrail logs (CloudWatch), and VPC Flow logs (CloudWatch).</p> <p>The workflow supports two scenarios:</p> <ul> <li> <p>Logs to KDS directly</p> <p>In this scenario, the service directly streams logs to Amazon Kinesis Data Streams (KDS).</p> <p> Figure 4: Amazon KDS based service log pipeline architecture</p> </li> <li> <p>Logs to KDS via subscription</p> <p>In this scenario, the service delivers the logs to CloudWatch Log Group, and then CloudWatch Logs stream the logs in real-time to KDS as the subscription destination.</p> <p> Figure 5: Amazon KDS (via subscription) based service log pipeline architecture</p> </li> </ul> <p>The log pipeline runs the following workflow:</p> <ol> <li> <p>AWS Services logs are streamed to Kinesis Data Stream.</p> </li> <li> <p>KDS initiates the Log Processor Lambda to run.</p> </li> <li> <p>The log processor processes and ingests the logs into the Amazon OpenSearch Service.</p> </li> <li> <p>Logs that fail to be processed are exported to Amazon S3 bucket (Backup Bucket).</p> </li> </ol> <p>For cross-account ingestion, the AWS Services store logs on Amazon CloudWatch log group in the member account, and other resources remain in central logging account.</p> <p>Warning</p> <p>This solution does not support cross-account ingestion for CloudFront real-time logs.</p>"},{"location":"implementation-guide/architecture/#application-log-analytics-pipeline","title":"Application log analytics pipeline","text":"<p>Centralized Logging with OpenSearch supports log analysis for application logs, such as Nginx/Apache HTTP Server logs or custom application logs. </p> <p>Note</p> <p>Centralized Logging with OpenSearch supports cross-account log ingestion. If you want to ingest logs from the same account, the resources in the Sources group will be in the same account as your Centralized Logging with OpenSearch account. Otherwise, they will be in another AWS account.</p>"},{"location":"implementation-guide/architecture/#logs-from-amazon-ec2-amazon-eks","title":"Logs from Amazon EC2 / Amazon EKS","text":"<p> Figure 6: Application log pipeline architecture for EC2/EKS</p> <p>The log pipeline runs the following workflow:</p> <ol> <li> <p>Fluent Bit works as the underlying log agent to collect logs from application servers and send them to an optional Log Buffer, or ingest into OpenSearch domain directly. </p> </li> <li> <p>The Log Buffer triggers the Lambda (Log Processor) to run.</p> </li> <li> <p>The log processor reads and processes the log records and ingests the logs into the OpenSearch domain.</p> </li> <li> <p>Logs that fail to be processed are exported to an Amazon S3 bucket (Backup Bucket).</p> </li> </ol>"},{"location":"implementation-guide/architecture/#logs-from-syslog-client","title":"Logs from Syslog Client","text":"<p>Important</p> <ol> <li>Make sure your Syslog generator/sender's subnet is connected to Centralized Logging with OpenSearch' two private subnets. You need to use VPC Peering Connection or Transit Gateway to connect these VPCs.</li> <li>The NLB together with the ECS containers in the architecture diagram will be provisioned only when you create a Syslog ingestion and be automated deleted when there is no Syslog ingestion.</li> </ol> <p> Figure 7: Application log pipeline architecture for Syslog</p> <ol> <li> <p>Syslog client (like Rsyslog) send logs to a Network Load Balancer (NLB) in Centralized Logging with OpenSearch's private subnets, and NLB routes to the ECS containers running Syslog servers.</p> </li> <li> <p>Fluent Bit works as the underlying log agent in the ECS Service to parse logs, and send them to an optional Log Buffer, or ingest into OpenSearch domain directly.</p> </li> <li> <p>The Log Buffer triggers the Lambda (Log Processor) to run.</p> </li> <li> <p>The log processor reads and processes the log records and ingests the logs into the OpenSearch domain.</p> </li> <li> <p>Logs that fail to be processed are exported to an Amazon S3 bucket (Backup Bucket).</p> </li> </ol>"},{"location":"implementation-guide/considerations/","title":"Considerations","text":""},{"location":"implementation-guide/considerations/#regional-deployments","title":"Regional deployments","text":"<p>This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List. </p> <p>Centralized Logging with OpenSearch provides two types of authentication, Cognito User Pool and OpenID Connect (OIDC) Provider. You must choose to launch the solution with OpenID Connect if one of the following cases occurs:</p> <ul> <li>Cognito User Pool is not available in your AWS Region.</li> <li>You already have an OpenID Connect Provider and want to authenticate against it.</li> </ul> <p>Supported regions for deployment</p> Region Name Launch with Cognito User Pool Launch with OpenID Connect US East (N. Virginia) US East (Ohio) US West (N. California) US West (Oregon) Africa (Cape Town) Asia Pacific (Hong Kong) Asia Pacific (Mumbai) Asia Pacific (Osaka) Asia Pacific (Seoul) Asia Pacific (Singapore) Asia Pacific (Sydney) Asia Pacific (Tokyo) Canada (Central) Europe (Frankfurt) Europe (Ireland) Europe (London) Europe (Milan) Europe (Paris) Europe (Stockholm) Middle East (Bahrain) South America (Sao Paulo) China (Beijing) Region Operated by Sinnet China (Ningxia) Regions operated by NWCD <p>Important</p> <p>You can have only one active Centralized Logging with OpenSearch solution stack in one region. If your deployment failed, make sure you have deleted the failed stack before retrying the deployment. </p>"},{"location":"implementation-guide/cost/","title":"Cost Estimation","text":"<p>Important</p> <p>The following cost estimations are examples and may vary depending on your environment. </p> <p>You will be responsible for the cost of the AWS services used when running the solution. The main factors affecting the solution cost include:</p> <ul> <li>Type of logs to be ingested</li> <li>Volume of logs to be ingested/processed</li> <li>Size of the log message</li> <li>Location of logs</li> <li>Additional features</li> </ul> <p>As of March 2023, the following examples demonstrate the cost estimation of 10/100/1000 GB daily log ingestion for running this solution with default settings in the US East (N. Virginia) Region. The total cost is composed of Amazon OpenSearch Cost, Processing Cost, and Additional Features Cost.  </p>"},{"location":"implementation-guide/cost/#amazon-opensearch-cost","title":"Amazon OpenSearch Cost","text":"<ul> <li>OD: On Demand</li> <li>AURI_1: All Upfront Reserved Instance 1 Year</li> <li>Tiering: The days stored in each tier. For example, 7H + 23W + 60C indicates that the log is stored in hot tier for 7 days, warm tier for 23 days, and cold tier for 60 days.</li> <li>Replica: The number of shard replicas.</li> </ul> Daily log Volume (GB) Retention (days) Tiering Replica OD Monthly (USD) AURI_1 Monthly  (USD) Dedicated Master Data Node EBS (GB) UltraWarm Nodes UltraWarm/Cold S3 Storage (GB) OD cost per GB (USD) AURI_1 cost per GB ($) 10 30 30H 0 216.28 158.54 N/A c6g.large[2] 380 N/A 0 0.72093 0.52847 10 30 30H 1 289.35 223.94 N/A m6g.large[2] 760 N/A 0 0.9645 0.74647 100 30 7H + 23W 0 989.49 825.97 m6g.large[3] m6g.large[2] 886 medium[2] 0 0.32983 0.27532 100 30 7H + 23W 1 1295.85 1066.92 m6g.large[3] m6g.large[4] 1772 medium[2] 0 0.43195 0.35564 100 90 7H + 23W + 60C 0 1133.49 969.97 m6g.large[3] m6g.large[2] 886 medium[2] 8300 0.12594 0.10777 100 90 7H + 23W + 60C 1 1439.85 1210.92 m6g.large[3] m6g.large[4] 1772 medium[2] 8300 0.15998 0.13455 100 180 7H + 23W + 150C 0 1349.49 1185.97 m6g.large[3] m6g.large[2] 886 medium[2] 17300 0.07497 0.06589 100 180 7H + 23W + 150C 1 1655.85 1426.92 m6g.large[3] m6g.large[4] 1772 medium[2] 17300 0.09199 0.07927 1000 30 7H + 23W 0 6101.15 5489.48 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 23000 0.20337 0.18298 1000 30 7H + 23W 1 8759.49 7635.8 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 23000 0.29198 0.25453 1000 90 7H + 23W + 60C 0 8027.33 7245.45 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 83000 0.08919 0.0805 1000 90 7H + 23W + 60C 1 10199.49 9075.8 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 83000 0.11333 0.10084 1000 180 7H + 23W + 150C 0 9701.15 9089.48 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 173000 0.0539 0.0505 1000 180 7H + 23W + 150C 1 12644.19 11420.86 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 173000 0.07025 0.06345"},{"location":"implementation-guide/cost/#processing-cost","title":"Processing Cost","text":""},{"location":"implementation-guide/cost/#log-ingestion-through-amazon-s3","title":"Log ingestion through Amazon S3","text":"<p>This section is applicable to:</p> <ul> <li>AWS service logs including Amazon S3 access logs, CloudFront standard logs, CloudTrail logs (S3), Application Load Balancing access logs, WAF logs, VPC Flow logs (S3), AWS Config logs, Amazon RDS/Aurora logs, and AWS Lambda Logs.</li> <li>Application Logs that use Amazon S3 as data buffer.</li> </ul> <p>Assumptions:</p> <ul> <li>The logs stored in Amazon S3 are in gzip format. </li> <li>A 4MB compressed log file in S3 is roughly 100 MB in raw log size.</li> <li>A Lambda with 1 GB memory takes about 26 seconds to process a 4 MB compressed log file, namely 260 milliseconds (ms) per MB raw logs. </li> <li>The maximum compressed log file size is 5 MB.</li> <li>Ingesting logs from S3 will incur SQS and S3 request fees which are very low, or usually within the free tier.</li> </ul> <p>You have <code>N</code> GB raw log per day, and the daily cost estimation is as follows: </p> <ul> <li>Lambda Cost = 260 ms per MB x 1024 MB x <code>N</code> GB/day x $0.0000000167 per ms</li> <li>S3 Storage Cost = $0.023 per GB x <code>N</code>GB/day x 4% (compression)</li> </ul> <p>The total monthly cost for ingesting AWS service logs is:</p> <p>Total Monthly Cost = (Lambda Cost + S3 Storage Cost) x 30 days</p> Daily Log Volume Daily Lambda Cost (USD) Daily S3 Storage Cost (USD) Monthly Cost (USD) 10 0.044 0.009 1.610 100 0.445 0.092 16.099 1000 4.446 0.920 160.986 <p>For Amazon RDS/Aurora logs and AWS Lambda Logs that deliver to CloudWatch Logs, apart from the S3 and Lambda costs listed above, there is additional cost of using Kinesis Data Firehose (KDF) to subscribe to the CloudWatch Logs Stream and put them into an Amazon S3 bucket, and KDF is charging for a 5KB increments (less than 5KB per record is billed as 5KB). </p> <p>Assuming Log size is 0.2 KB per record, then the daily KDF cost is estimated as below:</p> <ul> <li>Kinesis Data Firehose Cost = $0.029 per GB x <code>N</code> GB/day x (5KB/0.2 KB) </li> </ul> <p>For example, for 1GB logs per day, the extra monthly cost of KDF is $21.75.</p> <p>Important</p> <p>If you want to save cost charged by Kinesis Data Firehose, make sure you activate logs only when needed. For example, don't activate RDS general logs unless required. </p>"},{"location":"implementation-guide/cost/#logs-ingestion-through-amazon-kinesis-data-streams","title":"Logs ingestion through Amazon Kinesis Data Streams","text":"<p>This section is applicable to:</p> <ul> <li>AWS Services Logs including CloudFront real-time logs, CloudTrail logs (CloudWatch), and VPC Flow logs (CloudWatch).</li> <li>Application Logs that use Amazon KDS as data buffer</li> </ul> <p>Important</p> <p>The cost estimation does not include the logging cost of service. For example, CloudFront real-time logs are charged based on the number of log lines generated ($0.01 for every 1,000,000 log lines). There are also logs delivery to CloudWatch charges for CloudTrail and VPC Flow logs that enabled CloudWatch Logging. Please check the service pricing for more details.</p> <p>The cost estimation is based on the following assumptions and facts:</p> <ul> <li>The average log message size is 1 KB.</li> <li>The daily log volume is <code>L</code> GB.</li> <li>The Lambda processor memory is 1024 MB.</li> <li>Every Lambda invocation processes 1 MB logs.</li> <li>One Lambda invocation processes one shard of Kinesis, and Lambda can scale up to more concurrent invocations to process multiple shards. </li> <li>The Lambda runtime to process log less than 5 MB is 500ms.</li> <li>30% additional shards are provided to handle traffic jitter.</li> <li>One Kinesis shard intake log size is =  1 MB /second x 3600 seconds per hour x 24 hours x 0.7 = 60.48 GB/day.</li> <li>The desired Kinesis Shard number <code>S</code> is = Round_up_to_next_integer(Daily log volume <code>L</code> / 60.48).</li> </ul> <p>Based on the above assumptions, here is the daily cost estimation formula:</p> <ul> <li>Kinesis Shard Hour Cost = $0.015 / shard hour x 24 hours per day x <code>S</code> shards</li> <li>Kinesis PUT Payload Unit Cost =  $0.014 per million units x 1 millions per GB x <code>L</code> GB per day</li> <li>Lambda Cost = $0.0000000167 per 1ms x 500 ms per invocation x 1,000 invocations per GB x <code>L</code> GB per day</li> </ul> <p>Total Monthly Cost = (Kinesis Shard Hour Cost + Kinesis PUT Payload Unit Cost + Lambda Cost) x 30 days</p> Daily Log Volume (GB) Shards Daily Kinesis Shard Hour Cost ($) Daily Kinesis PUT Payload Unit Cost ($) Daily Lambda Cost ($) Monthly Cost ($) 10 1 0.36 0.14 0.0835 17.505 100 2 0.72 1.4 0.835 88.65 1000 17 6.12 14 8.35 854.1"},{"location":"implementation-guide/cost/#additional-features-cost","title":"Additional Features Cost","text":"<p>Note</p> <p>You will not be charged if you do not use the additional features in the Centralized Logging with OpenSearch console.</p>"},{"location":"implementation-guide/cost/#access-proxy","title":"Access Proxy","text":"<p>If you deploy the Access Proxy through Centralized Logging with OpenSearch, additional charges will apply. The total cost varies depending on the instance type and number of instances. As of March 2023, the following are two examples for the cost estimation in the US East (N. Virginia) Region.</p>"},{"location":"implementation-guide/cost/#example-1-instance-type-t3nano-instance-number-2","title":"Example 1: Instance Type - t3.nano, Instance Number - 2","text":"<ul> <li>EC2 cost = t3.nano 1Y RI All Upfront price $26.28 x 2 / 12 months = $4.38/month</li> <li>EBS Cost = EBS $0.1 GB/month x 8 GB x 2 = $1.6/month (The EBS attached to the EC2 instance is 8 GB)</li> <li>Elastic Load Balancer Cost = $0.0225 per ALB-hour x 720 hours/month = $16.2/month</li> </ul> <p>Total Monthly Cost = $4.38 EC2 Cost + $1.6 EBS Cost + $16.2 Elastic Load Balancer Cost = $22.18</p>"},{"location":"implementation-guide/cost/#example-2-instance-type-t3large-instance-number-2","title":"Example 2: Instance Type - t3.large, Instance Number - 2","text":"<ul> <li>EC2 Cost = t3.large 1Y RI All Upfront $426.612 x 2  / 12 months  = $71.1/month</li> <li>EBS Cost = $0.1 GB/month x 8 GB x 2 = $1.6/month (The EBS attached to the EC2 instance is 8 GB)</li> <li>Elastic Load Balancer Cost = $0.0225 per ALB-hour x 720 hours/month = $16.2/month</li> </ul> <p>Total Monthly Cost = $71.1 EC2 Cost + $1.6 EBS Cost + $16.2 Elastic Load Balancer Cost = $88.9</p>"},{"location":"implementation-guide/cost/#alarms","title":"Alarms","text":"<p>If you deploy the Alarms through Centralized Logging with OpenSearch, the CloudWatch Price will apply.</p>"},{"location":"implementation-guide/faq/","title":"Frequently Asked Questions","text":""},{"location":"implementation-guide/faq/#general","title":"General","text":"<p>Q:  What is Centralized Logging with OpenSearch solution? Centralized Logging with OpenSearch is an AWS Solution that simplifies the building of log analytics pipelines. It provides to customers, as complementary of Amazon OpenSearch Service, capabilities to ingest and process both application logs and AWS service logs without writing code, and create visualization dashboards from out-of-the-box templates. Centralized Logging with OpenSearch automatically assembles the underlying AWS services, and provides you a web console to manage log analytics pipelines.</p> <p>Q: What are the supported logs in this solution? Centralized Logging with OpenSearch supports both AWS service logs and EC2/EKS application logs. Refer to the supported AWS services, and the supported application log formats and sources for more details.</p> <p>Q: Does Centralized Logging with OpenSearch support ingesting logs from multiple AWS accounts? Yes. Centralized Logging with OpenSearch supports ingesting AWS service logs and application logs from a different AWS account in the same region. For more information, see cross-account ingestion.</p> <p>Q: Does Centralized Logging with OpenSearch support ingesting logs from multiple AWS Regions? Currently, Centralized Logging with OpenSearch does not automate the log ingestion from a different AWS Region. You need to ingest logs from other regions into pipelines provisioned by Centralized Logging with OpenSearch. For AWS services which store the logs in S3 bucket, you can leverage the S3 Cross-Region Replication to copy the logs to the Centralized Logging with OpenSearch deployed region, and import incremental logs using the manual mode by specifying the log location in the S3 bucket. For application logs on EC2 and EKS, you need to set up the networking (for example, Kinesis VPC endpoint, VPC Peering), install agents, and configure the agents to ingest logs to Centralized Logging with OpenSearch pipelines.</p> <p>Q: What is the license of this solution? This solution is provided under the Apache-2.0 license. It is a permissive free software license written by the Apache Software Foundation. It allows users to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software under the terms of the license, without concern for royalties.</p> <p>Q: How can I find the roadmap of this solution? This solution uses GitHub project to manage the roadmap. You can find the roadmap here.</p> <p>Q: How can I submit a feature request or bug report? You can submit feature requests and bug report through the GitHub issues. Here are the templates for feature request, bug report.</p>"},{"location":"implementation-guide/faq/#setup-and-configuration","title":"Setup and configuration","text":"<p>Q: Can I deploy Centralized Logging with OpenSearch on AWS in any AWS Region? Centralized Logging with OpenSearch provides two deployment options: option 1 with Cognito User Pool, and option 2 with OpenID Connect. For option 1, customers can deploy the solution in AWS Regions where Amazon Cognito User Pool, AWS AppSync, Amazon Kinesis Data Firehose (optional) are available. For option 2, customers can deploy the solution in AWS Regions where AWS AppSync, Amazon Kinesis Data Firehose (optional) are available. Refer to supported regions for deployment for more information.</p> <p>Q: What are the prerequisites of deploying this solution? Centralized Logging with OpenSearch does not provision Amazon OpenSearch clusters, and you need to import existing OpenSearch clusters through the web console. The clusters must meet the requirements specified in prerequisites.</p> <p>Q: Why do I need a domain name with ICP recordal when deploying the solution in AWS China Regions? The Centralized Logging with OpenSearch console is served via CloudFront distribution which is considered as an Internet information service. According to the local regulations, any Internet information service must bind to a domain name with ICP recordal.</p> <p>Q: What versions of OpenSearch does the solution work with? Centralized Logging with OpenSearch supports Amazon OpenSearch Service, with engine version Elasticsearch 7.10 and later, Amazon OpenSearch 1.0 and later.</p> <p>Q: What are the index name rules for OpenSearch created by the Log Analytics Pipeline?</p> <p>You can change the index name if needed when using the Centralized Logging with OpenSearch console to create a log analytics pipeline.</p> <p>If the log analytics pipeline is created for service logs, the index name is composed of <code>&lt;Index Prefix&gt;</code>-<code>&lt;service-type&gt;</code>-<code>&lt;Index Suffix&gt;</code>-&lt;00000x&gt;, where you can define a name for Index Prefix and service-type is automatically generated by the solution according to the service type you have chosen. Moreover,  you can choose different index suffix types to adjust index rollover time window.</p> <ul> <li>YYYY-MM-DD-HH: Amazon OpenSearch will roll the index by hour.</li> <li>YYYY-MM-DD: Amazon OpenSearch will roll the index by 24 hours.</li> <li>YYYY-MM: Amazon OpenSearch will roll the index by 30 days.</li> <li>YYYY: Amazon OpenSearch will roll the index by 365 days.</li> </ul> <p>It should be noted that in OpenSearch, the time is in UTC 0 time zone.</p> <p>Regarding the 00000x part, Amazon OpenSearch will automatically append a 6-digit suffix to the index name, where the first index rule is 000001, rollover according to the index, and increment backwards, such as 000002, 000003.</p> <p>If the log analytics pipeline is created for application log, the index name is composed of <code>&lt;Index Prefix&gt;</code>-<code>&lt;Index Suffix&gt;</code>-&lt;00000x&gt;. The rules for index prefix and index suffix, 00000x are the same as those for service logs.</p> <p>Q: What are the index rollover rules for OpenSearch created by the Log Analytics Pipeline?</p> <p>Index rollover is determined by two factors. One is the Index Suffix in the index name. If you enable the index rollover by capacity, Amazon OpenSearch will roll your index when the index capacity equals or exceeds the specified size, regardless of the rollover time window. Note that if one of these two factors matches, index rollover can be triggered.</p> <p>For example, we created an application log pipeline on January 1, 2023, deleted the application log pipeline at 9:00 on January 4, 2023, and the index name is nginx-YYYY-MM-DD-&lt;00000x&gt;. At the same time, we enabled the index rollover by capacity and entered 300GB. If the log data volume increases suddenly after creation, it can reach 300GB every hour, and the duration is 2 hours and 10 minutes. After that, it returns to normal, and the daily data volume is 90GB. Then OpenSearch creates three indexes on January 1, the index names are nginx-2023-01-01-000001, nginx-2023-01-01-000002, nginx-2023-01-01-000003, and then creates one every day Indexes respectively: nginx-2023-01-02-000004, nginx-2023-01-03-000005, nginx-2023-01-04-000006.</p> <p>Q: Can I deploy the solution in an existing VPC? Yes. You can either launch the solution with a new VPC or launch the solution with an existing VPC. When using an existing VPC, you need to select the VPC and the corresponding subnets. Refer to launch with Cognito User Pool or launch with OpenID Connect for more details.</p> <p>Q: I did not receive the email containing the temporary password when launching the solution with Cognito User Pool. How can I resend the password? Your account is managed by the Cognito User Pool. To resend the temporary password, you can find the user pool created by the solution, delete and recreate the user using the same email address. If you still have the same issue, try with another email address.</p> <p>Q: How can I create more users for this solution? If you launched the solution with Cognito User Pool, go to the AWS console, find the user pool created by the solution, and you can create more users. If you launched the solution with OpenID Connect (OIDC), you should add more users in the user pool managed by the OIDC provider. Note that all users have the same privileges.</p>"},{"location":"implementation-guide/faq/#pricing","title":"Pricing","text":"<p>Q: How will I be charged and billed for the use of this solution? The solution is free to use, and you are responsible for the cost of AWS services used while running this solution. You pay only for what you use, and there are no minimum or setup fees. Refer to the Centralized Logging with OpenSearch Cost section for detailed cost estimation.</p> <p>Q: Will there be additional cost for cross-account ingestion? No. The cost will be same as ingesting logs within the same AWS account.</p>"},{"location":"implementation-guide/faq/#log-ingestion","title":"Log Ingestion","text":"<p>Q: What is the log agent used in the Centralized Logging with OpenSearch solution? Centralized Logging with OpenSearch uses AWS for Fluent Bit, a distribution of Fluent Bit maintained by AWS. The solution uses this distribution to ingest logs from Amazon EC2 and Amazon EKS.</p> <p>Q: I have already stored the AWS service logs of member accounts in a centralized logging account. How should I create service log ingestion for member accounts? In this case, you need to deploy the Centralized Logging with OpenSearch solution in the centralized logging account, and ingest AWS service logs using the Manual mode from the logging account. Refer to this guide for ingesting Application Load Balancer logs with Manual mode. You can do the same with other supported AWS services which output logs to S3.</p> <p>Q: Why there are some duplicated records in OpenSearch when ingesting logs via Kinesis Data Streams? This is usually because there is no enough Kinesis Shards to handle the incoming requests. When threshold error occurs in Kinesis, the Fluent Bit agent will retry that chunk. To avoid this issue, you need to estimate your log throughput and set a proper Kinesis shard number. Please refer to the Kinesis Data Streams quotas and limits. Centralized Logging with OpenSearch provides a built-in feature to scale-out and scale-in the Kinesis shards, and it would take a couple of minutes to scale out to the desired number.</p> <p>Q: How to install log agent on CentOS 7?</p> <ol> <li> <p>Log in to your CentOS 7 machine and install SSM Agent manually.</p> <pre><code>sudo yum install -y http://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\nsudo systemctl enable amazon-ssm-agent\nsudo systemctl start amazon-ssm-agent\n</code></pre> </li> <li> <p>Go to the Instance Group panel of Centralized Logging with OpenSearch console, create Instance Group, select the CentOS 7 machine, choose Install log agent and wait for its status to be offline.</p> </li> <li> <p>Log in to CentOS 7 and install fluent-bit 1.9.3 manually.</p> <p><pre><code>export RELEASE_URL=${FLUENT_BIT_PACKAGES_URL:-https://packages.fluentbit.io}\nexport RELEASE_KEY=${FLUENT_BIT_PACKAGES_KEY:-https://packages.fluentbit.io/fluentbit.key}\n\nsudo rpm --import $RELEASE_KEY\ncat &lt;&lt; EOF | sudo tee /etc/yum.repos.d/fluent-bit.repo\n[fluent-bit]\nname = Fluent Bit\nbaseurl = $RELEASE_URL/centos/VERSION_ARCH_SUBSTR\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=$RELEASE_KEY\nenabled=1\nEOF\nsudo sed -i 's|VERSION_ARCH_SUBSTR|\\$releasever/\\$basearch/|g' /etc/yum.repos.d/fluent-bit.repo\nsudo yum install -y fluent-bit-1.9.3-1\n\n# Modify the configuration file\nsudo sed -i 's/ExecStart.*/ExecStart=\\/opt\\/fluent-bit\\/bin\\/fluent-bit -c \\/opt\\/fluent-bit\\/etc\\/fluent-bit.conf/g' /usr/lib/systemd/system/fluent-bit.service\nsudo systemctl daemon-reload\nsudo systemctl enable fluent-bit\nsudo systemctl start fluent-bit\n</code></pre> 4. Go back to the Instance Groups panel of the Centralized Logging with OpenSearch console and wait for the CentOS 7 machine status to be Online and proceed to create the instance group.</p> </li> </ol>"},{"location":"implementation-guide/faq/#log-visualization","title":"Log Visualization","text":"<p>Q: How can I find the built-in dashboards in OpenSearch? Please refer to the AWS Service Logs and Application Logs to find out if there is a built-in dashboard supported. You also need to turn on the Sample Dashboard option when creating a log analytics pipeline. The dashboard will be inserted into the Amazon OpenSearch Service under Global Tenant. You can switch to the Global Tenant from the top right coder of the OpenSearch Dashboards.</p>"},{"location":"implementation-guide/release-notes/","title":"Release Notes","text":"Date Change June 2023 Fix the EKS Fluent-Bit deployment configuration generation issue. April 2023 Fix deployment failure due to S3 ACL changes. March 2023 Initial release."},{"location":"implementation-guide/security/","title":"Security","text":"<p>When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared responsibility model  reduces your operational burden because AWS operates, manages, and controls the components including the host  operating system, the virtualization layer, and the physical security of the facilities in which the services operate.  For more information about AWS security, see AWS Cloud Security.</p>"},{"location":"implementation-guide/security/#iam-roles","title":"IAM Roles","text":"<p>AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions  to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions,  AWS AppSync and Amazon Cognito access to create regional resources.</p>"},{"location":"implementation-guide/security/#security-groups","title":"Security Groups","text":"<p>The security groups created in this solution are designed to control and isolate network traffic between the solution  components. We recommend that you review the security groups and further restrict access as needed once the deployment  is up and running.</p>"},{"location":"implementation-guide/security/#amazon-cloudfront","title":"Amazon CloudFront","text":"<p>This solution deploys a web console hosted in an Amazon Simple Storage Service (Amazon S3) bucket. To help reduce  latency and improve security, this solution includes an Amazon CloudFront distribution with an origin access identity,  which is a CloudFront user that provides public access to the solution\u2019s website bucket contents.  For more information, refer to Restricting Access to Amazon S3 Content by Using an Origin Access Identity in the  Amazon CloudFront Developer Guide.</p>"},{"location":"implementation-guide/solution-components/","title":"Solution components","text":"<p>The solution consists of the following components:</p>"},{"location":"implementation-guide/solution-components/#domain-management","title":"Domain Management","text":"<p>This solution uses Amazon OpenSearch Service as the underlying engine to store and analyze logs. You can import an existing Amazon OpenSearch Service domain for log ingestion, and provide an access proxy to the Amazon OpenSearch Service dashboards within VPC. Moreover, you can set up recommended CloudWatch alarms for Amazon OpenSearch Service.</p>"},{"location":"implementation-guide/solution-components/#analytics-pipelines","title":"Analytics Pipelines","text":"<p>A log pipeline includes a series of log processing steps, including collecting logs from sources, processing and sending them to Amazon OpenSearch Service for further analysis. Centralized Logging with OpenSearch supports AWS Service log ingestion and server-side application log ingestion.</p>"},{"location":"implementation-guide/solution-components/#service-log-pipeline","title":"Service Log Pipeline","text":"<p>This solution supports out of the box log analysis for AWS service logs, such as Amazon S3 access logs, and ELB access logs. The component is designed to reduce the complexities of building log analytics pipelines for different AWS services with different formats. </p>"},{"location":"implementation-guide/solution-components/#application-log-pipeline","title":"Application Log Pipeline","text":"<p>This solution supports out of the box log analysis for application logs, such as Nginx/Apache logs or general application logs via regex parser. The component uses Fluent Bit as the underlying log agent to collect logs from application servers, and allows you to easily install log agent and monitor the agent health via System Manager.</p>"},{"location":"implementation-guide/trouble-shooting/","title":"Troubleshooting","text":"<p>The following help you to fix errors or problems that you might encounter when using Centralized Logging with OpenSearch.</p>"},{"location":"implementation-guide/trouble-shooting/#error-failed-to-assume-service-linked-role-arnxxxawsserviceroleforappsync","title":"Error: Failed to assume service-linked role <code>arn:x:x:x:/AWSServiceRoleForAppSync</code>","text":"<p>The reason for this error is that the account has never used the AWS AppSync service. You can deploy the solution's CloudFormation template again. AWS has already created the role automatically when you encountered the error. </p> <p>You can also go to AWS CloudShell or the local terminal and run the following AWS CLI command to Link AppSync Role</p> <pre><code>aws iam create-service-linked-role --aws-service-name appsync.amazonaws.com\n</code></pre>"},{"location":"implementation-guide/trouble-shooting/#error-unable-to-add-backend-role","title":"Error: Unable to add backend role","text":"<p>Centralized Logging with OpenSearch only supports Amazon OpenSearch Service domain with Fine-grained access control enabled. You need to go to Amazon OpenSearch Service console, and edit the Access policy for the Amazon OpenSearch Service domain.</p>"},{"location":"implementation-guide/trouble-shooting/#erroruser-xxx-is-not-authorized-to-perform-stsassumerole-on-resource","title":"Error\uff1aUser xxx is not authorized to perform sts:AssumeRole on resource","text":"<p>If you see this error, please make sure you have entered the correct information during cross account setup, and then please wait for several minutes.</p> <p>Centralized Logging with OpenSearch uses AssumeRole for cross-account access. This is the best practice to temporary access the AWS resources in your sub-account.  However, these roles created during cross account setup take seconds or minutes to be affective.</p>"},{"location":"implementation-guide/trouble-shooting/#error-putrecords-api-responded-with-errorinvalidsignatureexception","title":"Error: PutRecords API responded with error='InvalidSignatureException'","text":"<p>Fluent-bit agent reports PutRecords API responded with error='InvalidSignatureException', message='The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.'</p> <p>Please restart the fluent-bit agent. For example, on EC2 with Amazon Linux2, run command: <pre><code>sudo service fluent-bit restart\n</code></pre></p>"},{"location":"implementation-guide/trouble-shooting/#error-putrecords-api-responded-with-erroraccessdeniedexception","title":"Error: PutRecords API responded with error='AccessDeniedException'","text":"<p>Fluent-bit agent deployed on EKS Cluster reports \"AccessDeniedException\" when sending records to Kinesis. Verify that  the IAM role trust relations are correctly set. With the Centralized Logging with OpenSearch console:</p> <ol> <li>Open the Centralized Logging with OpenSearch console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Choose the EKS Cluster that you want to check.</li> <li>Click the IAM Role ARN which will open the IAM Role in AWS Console.</li> <li>Choose the Trust relationships to verify that the OIDC Provider, the service account namespace and conditions are correctly set.</li> </ol> <p>You can get more information from Amazon EKS IAM role configuration</p>"},{"location":"implementation-guide/trouble-shooting/#my-cloudformation-stack-is-stuck-on-deleting-an-awslambdafunction-resource-when-i-update-the-stack-how-to-resolve-it","title":"My CloudFormation stack is stuck on deleting an <code>AWS::Lambda::Function</code> resource when I update the stack. How to resolve it?","text":"<p> The Lambda function resides in a VPC, and you need to wait for the associated ENI resource to be deleted.</p>"},{"location":"implementation-guide/trouble-shooting/#the-agent-status-is-offline-after-i-restart-the-ec2-instance-how-can-i-make-it-auto-start-on-instance-restart","title":"The agent status is offline after I restart the EC2 instance, how can I make it auto start on instance restart?","text":"<p>This usually happens if you have installed the log agent, but restart the instance before you create any Log Ingestion. The log agent will auto restart if there is at least one Log Ingestion. If you have a log ingestion, but the problem still exists, you can use <code>systemctl status fluent-bit</code> to check its status inside the instance.</p>"},{"location":"implementation-guide/trouble-shooting/#i-have-switched-to-global-tenant-however-i-still-cannot-find-the-dashboard-in-opensearch","title":"I have switched to Global tenant. However, I still cannot find the dashboard in OpenSearch.","text":"<p>This is usually because Centralized Logging with OpenSearch received 403 error from OpenSearch when creating the index template and dashboard. This  can be fixed by re-run the Lambda function manually by following the steps below:</p> <p>With the Centralized Logging with OpenSearch console:</p> <ol> <li>Open the Centralized Logging with OpenSearch console, and find the AWS Service Log pipeline which has this issue.</li> <li>Copy the first 5 characters from the ID section. Eg. you should copy <code>c169c</code> from ID <code>c169cb23-88f3-4a7e-90d7-4ab4bc18982c</code></li> <li>Go to AWS Console &gt; Lambda. Paste in function filters. This will filter in all the lambda function created for this AWS Service Log ingestion.</li> <li>Click the Lambda function whose name contains \"OpenSearchHelperFn\".</li> <li>In the Test tab, create a new event with any Event name.</li> <li>Click the Test button to trigger the Lambda, and wait the lambda function to complete.</li> <li>The dashboard should be available in OpenSearch.</li> </ol>"},{"location":"implementation-guide/trouble-shooting/#error-from-fluent-bit-agent-version-glibc_225-not-found","title":"Error from Fluent-bit agent: <code>version `GLIBC_2.25' not found</code>","text":"<p>This error is caused by old version of <code>glibc</code>. Centralized Logging with OpenSearch with version later than 1.2 requires glibc-2.25 or above. So you must upgrade the existing version in EC2 first. The upgrade command for different kinds of OS is shown as follows:</p> <p>Important</p> <p>We strongly recommend you run the commands with environments first. Any upgrade failure may cause severe loss.</p>"},{"location":"implementation-guide/trouble-shooting/#redhat-79","title":"Redhat 7.9","text":"<p>For Redhat 7.9, the whole process will take about 2 hours,and at least 10 GB storage is needed.</p> <pre><code># install library\nyum install -y gcc gcc-c++ m4 python3 bison  fontconfig-devel  libXpm-devel texinfo bzip2 wget \necho /usr/local/lib  &gt;&gt; /etc/ld.so.conf\n\n# create tmp directory\nmkdir -p /tmp/library\ncd /tmp/library\n\n# install gmp-6.1.0\nwget https://ftp.gnu.org/gnu/gmp/gmp-6.1.0.tar.bz2\ntar xjvf gmp-6.1.0.tar.bz2\ncd gmp-6.1.0\n./configure --prefix=/usr/local\nmake &amp;&amp; make install\nldconfig\ncd ..\n\n# install mpfr-3.1.4\nwget https://gcc.gnu.org/pub/gcc/infrastructure/mpfr-3.1.4.tar.bz2\ntar xjvf mpfr-3.1.4.tar.bz2\ncd mpfr-3.1.4\n./configure --with-gmp=/usr/local --prefix=/usr/local\nmake &amp;&amp; make install\nldconfig\ncd ..\n\n# install mpc-1.0.3\nwget https://gcc.gnu.org/pub/gcc/infrastructure/mpc-1.0.3.tar.gz\ntar xzvf mpc-1.0.3.tar.gz\ncd mpc-1.0.3\n./configure --prefix=/usr/local\nmake &amp;&amp; make install\nldconfig\ncd ..\n\n# install gcc-9.3.0\nwget https://ftp.gnu.org/gnu/gcc/gcc-9.3.0/gcc-9.3.0.tar.gz\ntar xzvf gcc-9.3.0.tar.gz\ncd gcc-9.3.0\nmkdir build\ncd build/\n../configure --enable-checking=release --enable-language=c,c++ --disable-multilib --prefix=/usr\nmake -j4 &amp;&amp; make install\nldconfig\ncd ../..\n\n# install make-4.3\nwget https://ftp.gnu.org/gnu/make/make-4.3.tar.gz\ntar xzvf make-4.3.tar.gz\ncd make-4.3\nmkdir build\ncd build\n../configure --prefix=/usr\nmake &amp;&amp; make install\ncd ../..\n\n# install glibc-2.31\nwget https://ftp.gnu.org/gnu/glibc/glibc-2.31.tar.gz\ntar xzvf glibc-2.31.tar.gz\ncd glibc-2.31\nmkdir build\ncd build/\n../configure  --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin --disable-sanity-checks --disable-werror\nmake all &amp;&amp; make install\nmake localedata/install-locales\n\n# clean tmp directory\ncd /tmp\nrm -rf /tmp/library\n</code></pre>"},{"location":"implementation-guide/trouble-shooting/#ubuntu-22","title":"Ubuntu 22","text":"<pre><code>sudo ln -s /snap/core20/1623/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1\nsudo ln -s /snap/core20/1623/usr/lib/x86_64-linux-gnu/libssl.so.1.1 /usr/lib/x86_64-linux-gnu/libssl.so.1.1\nsudo ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/uninstall/","title":"Uninstall the Centralized Logging with OpenSearch","text":"<p>Warning</p> <p>You will encounter IAM role missing error if you delete the Centralized Logging with OpenSearch main stack before you delete the log pipelines. Centralized Logging with OpenSearch console launches additional CloudFormation stacks to ingest logs. If you want to uninstall the Centralized Logging with OpenSearch solution.  We recommend you to delete log pipelines (incl. AWS Service log pipelines and application log pipelines) before uninstall the solution. </p>"},{"location":"implementation-guide/uninstall/#step-1-delete-application-log-pipelines","title":"Step 1. Delete Application Log Pipelines","text":"<p>Important</p> <p>Please delete all the log ingestion before deleting an application log pipeline.</p> <ol> <li>Go to the Centralized Logging with OpenSearch console, in the left sidebar, choose Application Log.</li> <li>Click the application log pipeline to view details.</li> <li>In the ingestion tab, delete all the application log ingestion in the pipeline.</li> <li>Uninstall/Disable the Fluent Bit agent.<ul> <li>EC2 (Optional): after removing the log ingestion from EC2 instance group. Fluent Bit will automatically stop ship logs, it is optional for you to stop the Fluent Bit in your instances. Here are the command for stopping Fluent Bit agent.       <pre><code>   sudo service fluent-bit stop\n   sudo systemctl disable fluent-bit.service\n</code></pre></li> <li>EKS DaemonSet (Mandatory): if you have chosen to deploy the Fluent Bit agent using DaemonSet, you need to delete your Fluent Bit agent. Otherwise, the agent will continue ship logs to Centralized Logging with OpenSearch pipelines.       <pre><code>   kubectl delete -f ~/fluent-bit-logging.yaml\n</code></pre></li> <li>EKS SideCar (Mandatory): please remove the fluent-bit agent in your <code>.yaml</code> file, and restart your pod.</li> </ul> </li> <li>Delete the Application Log pipeline.</li> <li>Repeat step 2 to Step 5 to delete all your application log pipelines.</li> </ol>"},{"location":"implementation-guide/uninstall/#step-2-delete-aws-service-log-pipelines","title":"Step 2. Delete AWS Service Log Pipelines","text":"<ol> <li>Go to the Centralized Logging with OpenSearch console, in the left sidebar, choose AWS Service Log.</li> <li>Select and delete the AWS Service Log Pipeline one by one.</li> </ol>"},{"location":"implementation-guide/uninstall/#step-3-clean-up-imported-opensearch-domains","title":"Step 3. Clean up imported OpenSearch domains","text":"<ol> <li>Delete Access Proxy, if you have created the proxy using Centralized Logging with OpenSearch console.</li> <li>Delete Alarms, if you have created alarms using Centralized Logging with OpenSearch console.</li> <li>Delete VPC peering Connection between Centralized Logging with OpenSearch's VPC and OpenSearch's VPC.<ol> <li>Go to AWS VPC Console.</li> <li>Choose Peering connections in left sidebar.</li> <li>Find and delete the VPC peering connection between the Centralized Logging with OpenSearch's VPC and OpenSearch's VPC. You may not have Peering Connections if you did not use the \"Automatic\" mode when importing OpenSearch domains.</li> </ol> </li> <li>(Optional) Remove imported OpenSearch Domains. (This will not delete the Amazon OpenSearch domain in the AWS account.)</li> </ol>"},{"location":"implementation-guide/uninstall/#step-4-delete-centralized-logging-with-opensearch-stack","title":"Step 4. Delete Centralized Logging with OpenSearch stack","text":"<ol> <li>Go to the CloudFormation console.</li> <li>Find CloudFormation Stack of the Centralized Logging with OpenSearch solution.</li> <li> <p>(Optional) Delete S3 buckets created by Centralized Logging with OpenSearch.</p> <p>Important</p> <p>The S3 bucket whose name contains LoggingBucket is the centralized bucket for your AWS service log. You might have enabled AWS Services to send logs to this S3 bucket. Deleting this bucket will cause AWS Services failed to send logs.</p> <ol> <li>Choose the CloudFormation stack of the Centralized Logging with OpenSearch solution, and select the Resources tab.</li> <li>In search bar, enter <code>AWS::S3::Bucket</code>. This will show all the S3 buckets created by Centralized Logging with OpenSearch solution, and the Physical ID field is the S3 bucket name.</li> <li>Go to S3 console, and find the S3 bucket using the bucket name. Empty and Delete the S3 bucket.</li> </ol> </li> <li> <p>Delete the CloudFormation Stack of the Centralized Logging with OpenSearch solution</p> </li> </ol>"},{"location":"implementation-guide/applications/","title":"Application Log Analytics Pipelines","text":"<p>Centralized Logging with OpenSearch supports ingesting application logs from EC2 instances, EKS clusters, and Syslog.</p> <ul> <li>For EC2 instances, Centralized Logging with OpenSearch will automatically install log agent (Fluent Bit 1.9), collect application logs on EC2 instances and then send logs into Amazon OpenSearch.</li> <li>For EKS clusters, Centralized Logging with OpenSearch will generate all-in-one configuration file for customers to deploy the log agent (Fluent Bit 1.9) as a DaemonSet or Sidecar. After log agent is deployed, Centralized Logging with OpenSearch will start collecting pod logs and send to Amazon OpenSearch.</li> <li>For Syslog, Centralized Logging with OpenSearch will collect syslog logs through UDP or TCP protocol.</li> </ul>"},{"location":"implementation-guide/applications/#supported-log-formats-and-sources","title":"Supported Log Formats and Sources","text":"Log Format EC2 Instance Group EKS Cluster Syslog Nginx Yes Yes No Apache HTTP Server Yes Yes No JSON Yes Yes Yes Single-line Text Yes Yes Yes Multi-line Text Yes Yes No Multi-line Text (Spring Boot) Yes Yes No Syslog RFC5424/RFC3164 No No Yes Syslog Custom No No Yes <p>In this chapter, you will learn how to create log ingestion for the following log formats:</p> <ul> <li>Apache HTTP server logs</li> <li>Nginx logs</li> <li>Single-line Text logs</li> <li>Multi-line Text logs</li> <li>JSON logs</li> <li>Syslog logs</li> </ul> <p>Before creating log ingestion, you need to:</p> <ul> <li>Create a log source (not applicable for Syslog)</li> <li>Create an application log pipeline</li> </ul>"},{"location":"implementation-guide/applications/#concepts","title":"Concepts","text":"<p>The following introduce concepts that help you to understand how the application log ingestion works.</p>"},{"location":"implementation-guide/applications/#application-log-analytics-pipeline","title":"Application Log Analytics Pipeline","text":"<p>To collect application logs, a data pipeline is needed. The pipeline not only buffers the data in transmit but also cleans or pre-processes data. For example, transforming IP to Geo location. Currently, Kinesis Data Stream is used as data buffering for EC2 log source.</p>"},{"location":"implementation-guide/applications/#log-ingestion","title":"Log Ingestion","text":"<p>A log ingestion configures the Log Source, Log Type and the Application Log Analytics Pipeline for the log agent used by Centralized Logging with OpenSearch. After that, Centralized Logging with OpenSearch will start collecting certain type of logs from the log source and sending them to Amazon OpenSearch.</p>"},{"location":"implementation-guide/applications/#log-agent","title":"Log Agent","text":"<p>A log agent is a program that reads logs from one location and sends them to another location (for example, OpenSearch).  Currently, Centralized Logging with OpenSearch only supports Fluent Bit 1.9 log agent which is installed automatically. The Fluent Bit agent has a dependency of OpenSSL 1.1. To learn how to install OpenSSL on Linux instances, refer to OpenSSL installation. To find the supported platforms by Fluent Bit, refer to this link.</p>"},{"location":"implementation-guide/applications/#log-buffer","title":"Log Buffer","text":"<p>Log Buffer is a buffer layer between the Log Agent and OpenSearch clusters. The agent uploads logs into the buffer layer before being processed and delivered into the OpenSearch clusters. A buffer layer is a way to protect OpenSearch clusters from overwhelming. This solution provides the following types of buffer layers.</p> <ul> <li> <p>Amazon S3. Use this option if you can bear minutes-level latency for log ingestion. The log agent periodically uploads logs to an Amazon S3 bucket. The frequency of data delivery to  Amazon S3 is determined by Buffer size (default value is 50 MiB) and Buffer interval (default value is 60 seconds) value  that you configured when creating the application log analytics pipelines. The condition satisfied first triggers data delivery to Amazon S3. </p> </li> <li> <p>Amazon Kinesis Data Streams. Use this option if you need real-time log ingestion. The log agent uploads logs to Amazon Kinesis Data Stream in seconds. The frequency  of data delivery to Kinesis Data Streams is determined by Buffer size (10 MiB) and Buffer interval (5 seconds). The  condition satisfied first triggers data delivery to Kinesis Data Streams. </p> </li> </ul> <p>Log Buffer is optional when creating an application log analytics pipeline. For all types of application logs, this  solution allows you to ingest logs without any buffer layers. However, we only recommend this option when you have small log volume, and you are confident that the logs will not exceed the thresholds at the OpenSearch side.</p>"},{"location":"implementation-guide/applications/#log-source","title":"Log Source","text":"<p>A Log Source refers to a location where you want Centralized Logging with OpenSearch to collect application logs from. Supported log sources includes:</p> <ul> <li>Instance Group</li> <li>EKS Cluster</li> <li>Syslog</li> </ul>"},{"location":"implementation-guide/applications/#instance-group","title":"Instance Group","text":"<p>An instance group is a collection of EC2 instances from which you want to collect application logs.  Centralized Logging with OpenSearch can help you install the log agent in each instance within a group. You can select arbitrary instances through the user interface, or choose an EC2 Auto Scaling Group.</p>"},{"location":"implementation-guide/applications/#eks-cluster","title":"EKS Cluster","text":"<p>The EKS Cluster in Centralized Logging with OpenSearch refers to the Amazon EKS from which you want to collect pod logs. Centralized Logging with OpenSearch  will guide you to deploy the log agent as a DaemonSet or Sidecar in the EKS Cluster.</p>"},{"location":"implementation-guide/applications/#syslog","title":"Syslog","text":"<p>Centralized Logging with OpenSearch supports collecting syslog logs through UDP or TCP protocol.</p>"},{"location":"implementation-guide/applications/#log-config","title":"Log Config","text":"<p>A Log Config is a configuration that is telling Centralized Logging with OpenSearch where the logs had been stored on Log Source, which types of logs you want to collect, what fields a line of log contains, and types of each field. </p>"},{"location":"implementation-guide/applications/apache/","title":"Apache HTTP server logs","text":"<p>Apache HTTP Server (httpd) is capable of writing error and access log files to a local directory. You can configure Centralized Logging with OpenSearch to ingest Apache HTTP server logs.</p>"},{"location":"implementation-guide/applications/apache/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an Amazon OpenSearch Service domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/apache/#step-1-create-an-apache-http-server-log-config","title":"Step 1: Create an Apache HTTP server log config","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose Apache HTTP server in the log type dropdown menu.</li> <li> <p>In the Apache Log Format section, paste your Apache HTTP server log format configuration. It is in the format of <code>/etc/httpd/conf/httpd.conf</code> and starts with <code>LogFormat</code>.</p> <p>For example: <pre><code>LogFormat \"%h %l %u %t \\\"%r\\\" %&gt;s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined\n</code></pre></p> </li> <li> <p>(Optional) In the Sample log parsing section, paste a sample Apache HTTP server log to verify if the log parsing is successful.</p> <p>For example: <pre><code>127.0.0.1 - - [22/Dec/2021:06:48:57 +0000] \"GET /xxx HTTP/1.1\" 404 196 \"-\" \"curl/7.79.1\"\n</code></pre></p> </li> <li> <p>Choose Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/apache/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":""},{"location":"implementation-guide/applications/apache/#instance-group-as-log-source","title":"Instance Group as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Choose the application pipeline that has been created during the Prerequisites.</li> <li>Go to Permission tab and copy the provided JSON policy.</li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column, and </p> <ol> <li>Choose Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your account id.</li> <li>Choose Next, Next, then enter the name for this policy.</li> <li>Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your launch template or launch configuration.</li> </ol> </li> <li> <p>Click the Create an Ingestion dropdown menu, and select From Instance Group.</p> </li> <li>Select Choose exists and choose Next.</li> <li>Select the instance group you have created during the Prerequisites and choose Next.</li> <li>(Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template.</li> <li>Select Choose exists and select the log config created in previous setup.</li> <li>Choose Next, then choose Create.</li> </ol>"},{"location":"implementation-guide/applications/apache/#eks-cluster-as-log-source","title":"EKS Cluster as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Choose the EKS Cluster that has been imported as Log Source during the Prerequisites.</li> <li>Go to App Log Ingestion tab and choose Create an Ingestion.<ol> <li>Select Choose exists and choose the application pipeline created during the Prerequisites. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol> </li> <li>Deploy Fluent-bit log agent following the guide generated by Centralized Logging with OpenSearch. <ol> <li>Select the App Log Ingestion just created.</li> <li>Follow DaemonSet or Sidecar Guide to deploy the log agent.</li> </ol> </li> </ol>"},{"location":"implementation-guide/applications/apache/#step-3-check-built-in-apache-http-server-dashboard-in-opensearch","title":"Step 3: Check built-in Apache HTTP server dashboard in OpenSearch","text":"<p>For Apache HTTP server logs, Centralized Logging with OpenSearch will create a built-in sample dashboard.</p> <ol> <li>Open OpenSearch dashboard in your browser.</li> <li>Go to Dashboard section in the left sidebar.</li> <li>Find the dashboard whose name starts with <code>&lt;the application pipeline&gt;</code>.</li> </ol>"},{"location":"implementation-guide/applications/apache/#sample-dashboard","title":"Sample Dashboard","text":""},{"location":"implementation-guide/applications/create-applog-pipeline/","title":"Create an application pipeline","text":"<ol> <li> <p>Sign in to the Centralized Logging with OpenSearch Console.</p> </li> <li> <p>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</p> </li> <li> <p>Click the Create a pipeline.</p> </li> <li> <p>Specify Index name in lowercase.</p> </li> <li> <p>In the Buffer section, choose S3 or Kinesis Data Streams. If you don't want the buffer layer, choose None. Refer to the Log Buffer for more information about choosing the appropriate buffer layer.</p> <ul> <li>S3 buffer parameters</li> </ul> Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. S3 Bucket Prefix <code>AppLogs/&lt;index-prefix&gt;/year=%Y/month=%m/day=%d</code> The log agent appends the prefix when delivering the log files to the S3 bucket. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency. Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency. Compression for data records <code>Gzip</code> The log agent compresses records before delivering them to the S3 bucket. <ul> <li>Kinesis Data Streams buffer parameters</li> </ul> Parameter Default Description Shard number <code>&lt;Requires input&gt;</code> The number of shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. Enable auto scaling <code>No</code> This solution monitors the utilization of Kinesis Data Streams every 5 minutes, and scale in/out the number of shards automatically. The solution will scale in/out for a maximum of 8 times within 24 hours. Maximum Shard number <code>&lt;Requires input&gt;</code> Required if auto scaling is enabled. The maximum number of shards. <p>Important</p> <p>You may observe duplicate logs in OpenSearch if threshold error occurs in Kinesis Data Streams (KDS). This is because the Fluent Bit log agent uploads logs in chunk (contains multiple records), and will retry the chunk if upload failed. Each KDS shard can support up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. Please estimate your log volume and choose an appropriate shard number.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</p> </li> <li> <p>In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>Add tags if needed.</p> </li> <li> <p>Choose Create.</p> </li> <li> <p>Wait for the application pipeline turning to \"Active\" state.</p> </li> </ol>"},{"location":"implementation-guide/applications/create-log-ingestion/","title":"Create log ingestion","text":""},{"location":"implementation-guide/applications/create-log-ingestion/#instance-group-as-log-source","title":"Instance Group as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Choose the application pipeline that has been created during the Prerequisites.</li> <li>Go to Permission tab and copy the provided JSON policy.</li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column, and </p> <ol> <li>Choose Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your account id.</li> <li>Choose Next, Next, then enter the name for this policy.</li> <li>Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your launch template or launch configuration.</li> </ol> </li> <li> <p>Click the Create an Ingestion dropdown menu, and select From Instance Group.</p> </li> <li>Select Choose exists and choose Next.</li> <li>Select the instance group you have created during the Prerequisites and choose Next.</li> <li>(Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template.</li> <li>Select Choose exists and select the log config created in previous setup.</li> <li>Choose Next, then choose Create.</li> </ol>"},{"location":"implementation-guide/applications/create-log-ingestion/#eks-cluster-as-log-source","title":"EKS Cluster as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Choose the EKS Cluster that has been imported as Log Source during the Prerequisites.</li> <li>Go to App Log Ingestion tab and choose Create an Ingestion.<ol> <li>Select Choose exists and choose the application pipeline created during the Prerequisites. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol> </li> <li>Deploy Fluent-bit log agent following the guide generated by Centralized Logging with OpenSearch. <ol> <li>Select the App Log Ingestion just created.</li> <li>Follow DaemonSet or Sidecar Guide to deploy the log agent.</li> </ol> </li> </ol>"},{"location":"implementation-guide/applications/create-log-ingestion/#syslog-as-log-source","title":"Syslog as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Choose the application pipeline that has been created during the Prerequisites.</li> <li>Choose the Create an Ingestion dropdown menu, and select From Syslog.</li> <li>Fill in all the form fields to specify Syslog Source. You can use UDP or TCP with custom port number. Choose Next.</li> <li>Select the log config created in the previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol>"},{"location":"implementation-guide/applications/create-log-source/","title":"Create a log source","text":"<p>You need to create a log source first before collecting application logs. Centralized Logging with OpenSearch supports the following log sources:</p> <ul> <li>Amazon EC2 instance group</li> <li>Amazon EKS cluster</li> <li>Syslog</li> </ul> <p>For more information, see concepts.</p>"},{"location":"implementation-guide/applications/create-log-source/#amazon-ec2-instance-group","title":"Amazon EC2 Instance Group","text":"<p>An instance group represents a group of EC2 Linux instances, which enables the solution to associate a Log Config with multiple EC2 instances quickly. Centralized Logging with OpenSearch uses Systems Manager Agent(SSM Agent) to install/configure Fluent Bit agent, and sends log data to Kinesis Data Streams. </p>"},{"location":"implementation-guide/applications/create-log-source/#prerequisites","title":"Prerequisites","text":"<p>Make sure the instances meet the following requirements:</p> <ul> <li>SSM agent is installed on instances. Refer to install SSM agent on EC2 instances for Linux for more details.</li> <li>The <code>AmazonSSMManagedInstanceCore</code> policy is being associated with the instances.</li> <li>The OpenSSL 1.1 or later is installed. Refer to OpenSSL Installation for more details.</li> <li>The instances have network access to AWS Systems Manager.</li> <li>The instances have network access to Amazon Kinesis Data Streams, if you use it as the Log Buffer.</li> <li>The instances have network access to Amazon S3, if you use it as the Log Buffer.</li> <li>The operating system of the instances are supported by Fluent Bit. Refer to Supported Platform.</li> </ul>"},{"location":"implementation-guide/applications/create-log-source/#option-1-select-instances-to-create-an-instance-group","title":"(Option 1) Select instances to create an Instance Group","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Source, choose Instance Group.</li> <li>Click the Create an instance group button.</li> <li>In the Settings section, specify a group name.</li> <li>In the Configuration section, select Instances. You can use up to 5 tags to filter the instances.</li> <li>Verify that all the selected instances \"Pending Status\" is Online.</li> <li>(Optional) If the selected instances \"Pending Status\" are empty, click the Install log agent button and wait for \"Pending Status\" to become Online.</li> <li>(Optional) If you want to ingest logs from another account, select a linked account in the Account Settings section to create an instance group log source from another account.</li> <li>Choose Create.</li> </ol> <p>Important</p> <p>Use the Centralized Logging with OpenSearch console to install Fluent Bit agent on Ubuntu instances in Beijing (cn-north-1) and Ningxia (cn-northwest-1) Region will cause installation error. The Fluent Bit assets cannot be downloaded successfully. You need to install the Fluent Bit agent by yourself.</p>"},{"location":"implementation-guide/applications/create-log-source/#option-2-select-an-auto-scaling-group-to-create-an-instance-group","title":"(Option 2) Select an Auto Scaling group to create an Instance Group","text":"<p>When creating an Instance Group with Amazon EC2 Auto Scaling group, the solution will generate a shell script which you should include in the EC2 User Data.  </p> <ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Source, choose Instance Group.</li> <li>Click the Create an instance group button.</li> <li>In the Settings section, specify a group name.</li> <li>In the Configuration section, select Auto Scaling Groups.</li> <li>In the Auto Scaling groups section, select the auto scaling group from which you want to collect logs.</li> <li>(Optional) If you want to ingest logs from another account, select a linked account in the Account Settings section to create an instance group log source from another account.</li> <li>Choose Create. After you created a Log Ingestion using the Instance Group, you can find the generated Shell Script in the details page.</li> <li>Copy the shell script and update the User Data of the Auto Scaling Group's launch configurations or launch template. The shell script will automatically install Fluent Bit, SSM agent if needed, and download Fluent Bit configurations. </li> <li>Once you have updated the launch configurations or launch template, you need to start an instance refresh to update the instances within the Auto Scaling group.  The newly launched instances will ingest logs to the OpenSearch cluster or the Log Buffer layer.</li> </ol>"},{"location":"implementation-guide/applications/create-log-source/#amazon-eks-cluster","title":"Amazon EKS cluster","text":"<p>The EKS Cluster in Centralized Logging with OpenSearch refers to the Amazon Elastic Kubernetes Service (Amazon EKS) from which you want to collect pod logs. Centralized Logging with OpenSearch will guide you to deploy the log agent as a DaemonSet or Sidecar in the EKS Cluster.</p> <p>Important</p> <ul> <li>Centralized Logging with OpenSearch does not support sending logs in one EKS cluster to more than one Amazon OpenSearch domain at the same time.</li> <li>Make sure your EKS cluster's VPC is connected to Amazon OpenSearch Service cluster's VPC so that log can be ingested. Refer to VPC Connectivity for more details regarding approaches to connect VPCs.</li> </ul> <ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Source, choose EKS Cluster.</li> <li>Click the Import a Cluster button.</li> <li>Choose the EKS Cluster where Centralized Logging with OpenSearch collects logs from.  (Optional) If you want to ingest logs from another account, select a linked account from the Account dropdown to import an EKS log source from another account.</li> <li>Select DaemonSet or Sidecar as log agent's deployment pattern. </li> <li>Choose Next.</li> <li>Specify the Amazon OpenSearch where Centralized Logging with OpenSearch sends the logs to.</li> <li>Follow the guidance to establish a VPC peering connection between EKS's VPC and OpenSearch's VPC.<ul> <li>Create and accept VPC peering connections</li> <li>Update your route tables for a VPC peering connection</li> <li>Update your security groups to reference peer VPC groups</li> </ul> </li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/applications/create-log-source/#syslog","title":"Syslog","text":"<p>Important</p> <p>To ingest logs, make sure your Syslog generator/sender\u2019s subnet is connected to Centralized Logging with OpenSearch\u2019s two private subnets. Refer to VPC Connectivity for more details about how to connect VPCs.</p> <p>You can use UDP or TCP custom port number to collect syslog in Centralized Logging with OpenSearch. Syslog refers to logs generated by Linux instance, routers or network equipment. For more information, see Syslog in Wikipedia. </p>"},{"location":"implementation-guide/applications/include-prerequisites/","title":"Include prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an Amazon OpenSearch Service domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/include-supported-app-logs/","title":"Include supported app logs","text":"Log Format EC2 Instance Group EKS Cluster Syslog Nginx Yes Yes No Apache HTTP Server Yes Yes No JSON Yes Yes Yes Single-line Text Yes Yes Yes Multi-line Text Yes Yes No Multi-line Text (Spring Boot) Yes Yes No Syslog RFC5424/RFC3164 No No Yes Syslog Custom No No Yes"},{"location":"implementation-guide/applications/json/","title":"JSON format logs","text":"<p>You can configure Centralized Logging with OpenSearch to ingest JSON logs.</p>"},{"location":"implementation-guide/applications/json/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an Amazon OpenSearch Service domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/json/#step-1-create-a-json-config","title":"Step 1: Create a JSON config","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose JSON in the log type dropdown list.</li> <li> <p>In the Sample log parsing section, paste a sample JSON log and click Parse log to verify if the log parsing is successful.</p> <p>For example: <pre><code>{\"host\":\"81.95.250.9\", \"user-identifier\":\"-\", \"time\":\"08/Mar/2022:06:28:03 +0000\", \"method\": \"PATCH\", \"request\": \"/clicks-and-mortar/24%2f7\", \"protocol\":\"HTTP/2.0\", \"status\":502, \"bytes\":24337, \"referer\": \"http://www.investorturn-key.net/functionalities/innovative/integrated\"}\n</code></pre></p> </li> <li> <p>Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/json/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":"<p>The steps are similar to creating an application log ingestion for single-line text. Refer to Single-line Text for details.</p>"},{"location":"implementation-guide/applications/json/#step-3-view-your-logs-in-opensearch","title":"Step 3: View your logs in OpenSearch","text":"<ol> <li>Open OpenSearch dashboard in your browser.</li> <li>Create an Index Pattern     <ul> <li>Choose the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and enter the index pattern name. Choose Next step.</li> <li>Specify time field, and choose Create index pattern. </li> </ul> </li> <li>Go to Discover section in the left sidebar.</li> <li>Change active index pattern to <code>&lt;Index name&gt;-*</code>.</li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/","title":"Multi-line Text","text":"<p>You can configure Centralized Logging with OpenSearch to ingest multi-line text logs. Currently, Centralized Logging with OpenSearch supports Spring Boot  style logs or customize the log format using Regular Expression.</p>"},{"location":"implementation-guide/applications/multi-line-text/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an Amazon OpenSearch Service domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/#step-1-create-a-multi-line-text-config","title":"Step 1: Create a Multi-line text config","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose Multi-line Text in the log type dropdown menu.</li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/#java-spring-boot","title":"Java - Spring Boot","text":"<ol> <li> <p>For Java Spring Boot logs, you could provide a simple log format. For example:</p> <pre><code>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%thread] %logger : %msg%n\n</code></pre> </li> <li> <p>Paste a sample multi-line log. For example:</p> <pre><code>2022-02-18 10:32:26.400 ERROR [http-nio-8080-exec-1] org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.ArithmeticException: / by zero] with root cause\njava.lang.ArithmeticException: / by zero\n   at com.springexamples.demo.web.LoggerController.logs(LoggerController.java:22)\n   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke\n</code></pre> </li> <li> <p>Choose Parse Log.</p> </li> <li> <p>Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/#custom","title":"Custom","text":"<ol> <li> <p>For other kinds of logs, you could specify the first line regex pattern. For example:</p> <pre><code>(?&lt;time&gt;\\d{4}-\\d{2}-\\d{2}\\s*\\d{2}:\\d{2}:\\d{2}.\\d{3})\\s*(?&lt;level&gt;[\\S]+)\\s*\\[(?&lt;thread&gt;.+)\\]\\s*(?&lt;logger&gt;\\S+)\\s*:\\s*(?&lt;message&gt;[\\s\\S]+)\n</code></pre> </li> <li> <p>Paste a sample multi-line log. For example:</p> <pre><code>2022-02-18 10:32:26.400 ERROR [http-nio-8080-exec-1] org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.ArithmeticException: / by zero] with root cause\njava.lang.ArithmeticException: / by zero\n   at com.springexamples.demo.web.LoggerController.logs(LoggerController.java:22)\n   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke\n</code></pre> </li> <li> <p>Choose Parse Log.</p> </li> <li> <p>Check if each field type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":"<p>The steps are similar to creating an application log ingestion for single-line text. Refer to Single-line Text for details.</p> <p>Note</p> <p>Currently, Centralized Logging with OpenSearch does not support collecting multi-line text logs from S3 Bucket as Log Source.</p>"},{"location":"implementation-guide/applications/multi-line-text/#step-3-view-logs-in-opensearch","title":"Step 3: View logs in OpenSearch","text":"<ol> <li>Open OpenSearch console in your browser.</li> <li>Create an Index Pattern     <ul> <li>Choose the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and enter the index pattern name. Choose Next step.</li> <li>Specify time field, and choose Create index pattern. </li> </ul> </li> <li>Go to Discover section in the left sidebar.</li> <li>Change active index pattern to <code>&lt;the application pipeline&gt;-*</code>.</li> </ol>"},{"location":"implementation-guide/applications/nginx/","title":"Nginx","text":"<p>Nginx is capable of writing error and access log files to a local directory. You can configure Centralized Logging with OpenSearch to ingest Nginx logs.</p>"},{"location":"implementation-guide/applications/nginx/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an Amazon OpenSearch Service domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/nginx/#step-1-create-a-nginx-log-config","title":"Step 1: Create a Nginx log config","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose Nginx in the log type dropdown menu.</li> <li> <p>In the Nginx Log Format section, paste your Nginx log format configuration. It is in the format of <code>/etc/nginx/nginx.conf</code> and starts with <code>log_format</code>.</p> <p>For example:    <pre><code>log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n'$status $body_bytes_sent \"$http_referer\" '\n'\"$http_user_agent\" \"$http_x_forwarded_for\"';\n</code></pre></p> </li> <li> <p>(Optional) In the Sample log parsing section, paste a sample Nginx log to verify if the log parsing is successful.</p> <p>For example:    <pre><code>127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\"\n</code></pre></p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. </p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/nginx/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":""},{"location":"implementation-guide/applications/nginx/#instance-group-as-log-source","title":"Instance Group as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Choose the application pipeline that has been created during the Prerequisites.</li> <li>Go to Permission tab and copy the provided JSON policy.</li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column, and </p> <ol> <li>Choose Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your account id.</li> <li>Choose Next, Next, then enter the name for this policy.</li> <li>Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your launch template or launch configuration.</li> </ol> </li> <li> <p>Click the Create an Ingestion dropdown menu, and select From Instance Group.</p> </li> <li>Select Choose exists and choose Next.</li> <li>Select the instance group you have created during the Prerequisites and choose Next.</li> <li>(Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template.</li> <li>Select Choose exists and select the log config created in previous setup.</li> <li>Choose Next, then choose Create.</li> </ol>"},{"location":"implementation-guide/applications/nginx/#eks-cluster-as-log-source","title":"EKS Cluster as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Choose the EKS Cluster that has been imported as Log Source during the Prerequisites.</li> <li>Go to App Log Ingestion tab and choose Create an Ingestion.<ol> <li>Select Choose exists and choose the application pipeline created during the Prerequisites. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol> </li> <li>Deploy Fluent-bit log agent following the guide generated by Centralized Logging with OpenSearch. <ol> <li>Select the App Log Ingestion just created.</li> <li>Follow DaemonSet or Sidecar Guide to deploy the log agent.</li> </ol> </li> </ol>"},{"location":"implementation-guide/applications/nginx/#step-3-check-built-in-nginx-dashboard-in-opensearch","title":"Step 3: Check built-in Nginx dashboard in OpenSearch","text":"<p>For Nginx logs, Centralized Logging with OpenSearch creates a built-in sample dashboard.</p> <ol> <li>Open OpenSearch dashboard in your browser.</li> <li>Go to Dashboard section in the left sidebar.</li> <li>Find the dashboard whose name starts with <code>&lt;the application pipeline&gt;</code>.</li> </ol>"},{"location":"implementation-guide/applications/nginx/#sample-dashboard","title":"Sample Dashboard","text":""},{"location":"implementation-guide/applications/single-line-text/","title":"Single-line Text","text":"<p>Centralized Logging with OpenSearch uses custom Ruby Regular Expression to parse logs. It supports both single-line log format and multiple input format.</p> <p>You can configure Centralized Logging with OpenSearch to ingest single-line text logs.</p>"},{"location":"implementation-guide/applications/single-line-text/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an Amazon OpenSearch Service domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#step-1-create-a-single-line-text-config","title":"Step 1: Create a Single-line text config","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose Single-line Text in the log type dropdown menu.</li> <li> <p>Write the regular expression in Rubular to validate first and enter the value. For example:</p> <pre><code>(?&lt;remote_addr&gt;\\S+)\\s*-\\s*(?&lt;remote_user&gt;\\S+)\\s*\\[(?&lt;time_local&gt;\\d+/\\S+/\\d+:\\d+:\\d+:\\d+)\\s+\\S+\\]\\s*\"(?&lt;request_method&gt;\\S+)\\s+(?&lt;request_uri&gt;\\S+)\\s+\\S+\"\\s*(?&lt;status&gt;\\S+)\\s*(?&lt;body_bytes_sent&gt;\\S+)\\s*\"(?&lt;http_referer&gt;[^\"]*)\"\\s*\"(?&lt;http_user_agent&gt;[^\"]*)\"\\s*\"(?&lt;http_x_forwarded_for&gt;[^\"]*)\".*\n</code></pre> </li> <li> <p>In the Sample log parsing section, paste a sample Single-line text log and click Parse log to verify if the log parsing is successful. For example:</p> <pre><code>127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\"\n</code></pre> </li> <li> <p>Check if each fields type mapping is correct. Change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types. </p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this manual for details.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":""},{"location":"implementation-guide/applications/single-line-text/#instance-group-as-log-source","title":"Instance Group as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Choose the application pipeline that has been created during the Prerequisites.</li> <li>Go to Permission tab and copy the provided JSON policy.</li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column, and </p> <ol> <li>Choose Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your account id.</li> <li>Choose Next, Next, then enter the name for this policy.</li> <li>Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your launch template or launch configuration.</li> </ol> </li> <li> <p>Click the Create an Ingestion dropdown menu, and select From Instance Group.</p> </li> <li>Select Choose exists and choose Next.</li> <li>Select the instance group you have created during the Prerequisites and choose Next.</li> <li>(Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template.</li> <li>Select Choose exists and select the log config created in previous setup.</li> <li>Choose Next, then choose Create.</li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#eks-cluster-as-log-source","title":"EKS Cluster as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Choose the EKS Cluster that has been imported as Log Source during the Prerequisites.</li> <li>Go to App Log Ingestion tab and choose Create an Ingestion.<ol> <li>Select Choose exists and choose the application pipeline created during the Prerequisites. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol> </li> <li>Deploy Fluent-bit log agent following the guide generated by Centralized Logging with OpenSearch. <ol> <li>Select the App Log Ingestion just created.</li> <li>Follow DaemonSet or Sidecar Guide to deploy the log agent.</li> </ol> </li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#syslog-as-log-source","title":"Syslog as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Choose the application pipeline that has been created during the Prerequisites.</li> <li>Choose the Create an Ingestion dropdown menu, and select From Syslog.</li> <li>Fill in all the form fields to specify Syslog Source. You can use UDP or TCP with custom port number. Choose Next.</li> <li>Select the log config created in the previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#step-3-view-logs-in-opensearch","title":"Step 3: View logs in OpenSearch","text":"<ol> <li>Open OpenSearch console in your browser.</li> <li>Create an Index Pattern     <ul> <li>Choose the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and enter the index pattern name. Choose Next step.</li> <li>Specify time field, and choose Create index pattern. </li> </ul> </li> <li>Go to Discover section in the left sidebar.</li> <li>Change active index pattern to <code>&lt;the application pipeline&gt;-*</code>.</li> </ol>"},{"location":"implementation-guide/applications/syslog/","title":"Syslog","text":"<p>Syslog is a standard for message logging, and is widely used to generate, forward and collect logs produced on a Linux instance, routers or network equipment. You can configure Centralized Logging with OpenSearch to ingest syslog logs.</p> <p>Important</p> <p>Make sure your Syslog generator/sender's subnet is connected to Centralized Logging with OpenSearch' two private subnets so that logs can be ingested. You need to use VPC Peering Connection or Transit Gateway to connect these VPCs.</p>"},{"location":"implementation-guide/applications/syslog/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an Amazon OpenSearch Service domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/syslog/#step-1-create-a-syslog-config","title":"Step 1: Create a Syslog config","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Choose Syslog in the log type dropdown menu. Note that Centralized Logging with OpenSearch also supports Syslog with JSON format and single-line text format.</li> </ol>"},{"location":"implementation-guide/applications/syslog/#rfc5424","title":"RFC5424","text":"<ol> <li> <p>Paste a sample RFC5424 log. For example:</p> <pre><code>&lt;35&gt;1 2013-10-11T22:14:15Z client_machine su - - - 'su root' failed for joe on /dev/pts/2\n</code></pre> </li> <li> <p>Choose Parse Log.</p> </li> <li> <p>Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this manual for details. For example:</p> <pre><code>%Y-%m-%dT%H:%M:%SZ\n</code></pre> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/syslog/#rfc3164","title":"RFC3164","text":"<ol> <li> <p>Paste a sample RFC3164 log. For example:</p> <pre><code>&lt;35&gt;Oct 12 22:14:15 client_machine su: 'su root' failed for joe on /dev/pts/2\n</code></pre> </li> <li> <p>Choose Parse Log.</p> </li> <li> <p>Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> <p>Since there is no year in the timestamp of RFC3164, it cannot be displayed as a time histogram in the Discover interface of Amazon OpenSearch.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details. For example:</p> <pre><code>%b %m %H:%M:%S\n</code></pre> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/syslog/#custom","title":"Custom","text":"<ol> <li> <p>In the Syslog Format section, paste your Syslog log format configuration. It is in the format of <code>/etc/rsyslog.conf</code> and starts with <code>template</code> or <code>$template</code>. The format syntax follows Syslog Message Format.  For example:</p> <pre><code>&lt;%pri%&gt;1 %timestamp:::date-rfc3339% %HOSTNAME% %app-name% %procid% %msgid% %msg%\\n\n</code></pre> </li> <li> <p>In the Sample log parsing section, paste a sample Nginx log to verify if the log parsing is successful. For example:     <pre><code>&lt;35&gt;1 2013-10-11T22:14:15.003Z client_machine su - - 'su root' failed for joe on /dev/pts/2\n</code></pre></p> </li> <li> <p>Check if each fields type mapping is correct. Change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types. </p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this manual for details.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/syslog/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":""},{"location":"implementation-guide/applications/syslog/#syslog-as-log-source","title":"Syslog as Log Source","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Choose the application pipeline that has been created during the Prerequisites.</li> <li>Choose the Create an Ingestion dropdown menu, and select From Syslog.</li> <li>Fill in all the form fields to specify Syslog Source. You can use UDP or TCP with custom port number. Choose Next.</li> <li>Select the log config created in the previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol>"},{"location":"implementation-guide/applications/syslog/#step-3-configure-the-syslog-generator-to-send-the-logs-to-centralized-logging-with-opensearch","title":"Step 3: Configure the Syslog generator to send the logs to Centralized Logging with OpenSearch","text":"<ol> <li>Choose the Ingestion ID that has been created during Step 2.</li> <li>For Rsyslog user, follow the Syslog Configuration Guide to configure the Rsyslog agent. You can also get the NLB DNS Name on this page.</li> </ol>"},{"location":"implementation-guide/applications/syslog/#step-4-view-logs-in-opensearch","title":"Step 4: View logs in OpenSearch","text":"<ol> <li>Open OpenSearch console in your browser.</li> <li>Create an Index Pattern.     <ul> <li>Choose the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and enter the index pattern name. Choose Next step.</li> <li>Specify time field, and choose Create index pattern. </li> </ul> </li> <li>Go to Discover section in the left sidebar.</li> <li>Change active index pattern to <code>&lt;the application pipeline&gt;-*</code>.</li> </ol>"},{"location":"implementation-guide/aws-services/","title":"Build AWS Service Log Analytics Pipelines","text":"<p>Centralized Logging with OpenSearch supports ingesting AWS service logs into Amazon OpenSearch Service through log analytics pipelines, which you can build using the Centralized Logging with OpenSearch web console or via a standalone CloudFormation template. </p> <p>Centralized Logging with OpenSearch reads the data source, parse, cleanup/enrich and ingest logs into Amazon OpenSearch Service domains for analysis. Moreover, the solution provides templated dashboards to facilitate log visualization.</p> <p>Important</p> <ul> <li>AWS managed services must be in the same region as Centralized Logging with OpenSearch. To ingest logs from different AWS regions, we recommend using S3 cross-region replication.</li> <li>The solution will rotate the index on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/#supported-aws-services","title":"Supported AWS Services","text":"<p>Most of AWS managed services output logs to Amazon CloudWatch Logs, Amazon S3, Amazon Kinesis Data Streams or Amazon Kinesis Firehose. </p> <p>The following table lists the supported AWS services and the corresponding features.</p> AWS Service Log Type Log Location Automatic Ingestion Built-in Dashboard Amazon CloudTrail N/A S3 Yes Yes Amazon S3 Access logs S3 Yes Yes Amazon RDS/Aurora MySQL Logs CloudWatch Logs Yes Yes Amazon CloudFront Standard access logs S3 Yes Yes Application Load Balancer Access logs S3 Yes Yes AWS WAF Web ACL logs S3 Yes Yes AWS Lambda N/A CloudWatch Logs Yes Yes Amazon VPC Flow logs S3 Yes Yes AWS Config N/A S3 Yes Yes <ul> <li>Automatic Ingestion: The solution detects the log location of the resource automatically and then reads the logs.</li> <li>Built-in Dashboard: An out-of-box dashboard for the specified AWS service. The solution will automatically ingest a dashboard into the Amazon OpenSearch Service.</li> </ul> <p>Most of supported AWS services in Centralized Logging with OpenSearch offers built-in dashboard when creating the log analytics pipelines. You go to the OpenSearch Dashboards to view the dashboards after the pipeline being provisioned.</p> <p>In this chapter, you will learn how to create log ingestion and dashboards for the following AWS services:</p> <ul> <li>Amazon CloudTrail</li> <li>Amazon S3</li> <li>Amazon RDS/Aurora</li> <li>Amazon CloudFront</li> <li>AWS Lambda</li> <li>Elastic Load Balancing</li> <li>AWS WAF</li> <li>Amazon VPC</li> <li>AWS Config</li> </ul>"},{"location":"implementation-guide/aws-services/cloudfront/","title":"Amazon CloudFront Logs","text":"<p>CloudFront standard logs provide detailed records about every request made to a distribution.</p>"},{"location":"implementation-guide/aws-services/cloudfront/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The CloudFront logging bucket must be the same region as the Centralized Logging with OpenSearch solution.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/cloudfront/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch Console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose Amazon CloudFront.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for CloudFront logs enabling. The automatic mode will detect the CloudFront log location automatically.<ul> <li>For Automatic mode, choose the CloudFront distribution and Log Type from the dropdown lists.</li> <li>For Standard Log, the solution will automatically detect the log location  if logging is enabled.</li> <li>For Real-time log, the solution will prompt you for confirmation to create or replace CloudFront real-time log configuration.</li> <li>For Manual mode, enter the CloudFront Distribution ID and CloudFront Standard Log location. (Note that CloudFront real-time log is not supported in Manual mode)</li> <li>(Optional) If you are ingesting CloudFront logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the CloudFront distribution ID.</li> <li>In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/cloudfront/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - CloudFront Standard Log Ingestion template in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Plugins <code>&lt;Optional&gt;</code> List of plugins delimited by comma. Leave it blank if there are no available plugins to use. Valid inputs are <code>user_agent</code>, <code>geo_ip</code>. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/cloudfront/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/cloudtrail/","title":"Amazon CloudTrail Logs","text":"<p>Amazon CloudTrail monitors and records account activity across your AWS infrastructure. It outputs all the data to the specified S3 bucket or a CloudWatch log group.</p>"},{"location":"implementation-guide/aws-services/cloudtrail/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The CloudTrail region must be the same as the solution region.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/cloudtrail/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose Create a log ingestion.</li> <li>In the AWS Services section, choose Amazon CloudTrail.</li> <li>Choose Next.</li> <li>Under Specify settings, for Trail, select one from the dropdown list. (Optional) If you are ingesting CloudTrail logs from another account, select a linked account from the Account dropdown list first.</li> <li>Under Log Source, Select S3 or CloudWatch as the log source.</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your trail name.</li> <li>In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/cloudtrail/#using-the-standalone-cloudformation-stack","title":"Using the standalone CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - CloudTrail Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/cloudtrail/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/config/","title":"AWS Config Logs","text":"<p>By default, AWS Config delivers configuration history and snapshot files to your Amazon S3 bucket.</p>"},{"location":"implementation-guide/aws-services/config/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>AWS Config must be enabled in the same region as the Centralized Logging with OpenSearch solution.</li> <li>The Amazon S3 bucket region must be the same as the Centralized Logging with OpenSearch solution.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/config/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch Console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose AWS Config Logs.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for Log creation.<ul> <li>For Automatic mode, make sure the S3 bucket location is correct, and enter the AWS Config Name.</li> <li>For Manual mode, enter the AWS Config Name and Log location.</li> <li>(Optional) If you are ingesting AWS Config logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix the AWS Config Name you entered in previous steps.</li> <li>In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/config/#using-the-standalone-cloudformation-stack","title":"Using the standalone CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - AWS Config Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/config/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/elb/","title":"Application Load Balancing (ALB) Logs","text":"<p>ALB Access logs provide access logs that capture detailed information about requests sent to your load balancer. ALB publishes a log file for each load balancer node every 5 minutes.</p>"},{"location":"implementation-guide/aws-services/elb/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The ELB logging bucket must be the same as the Centralized Logging with OpenSearch solution.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/elb/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch Console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose Elastic Load Balancer.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual.<ul> <li>For Automatic mode, choose an application load balancer in the dropdown list. (If the selected ALB access log is not enabled, click Enable to enable the ALB access log.)</li> <li>For Manual mode, enter the Application Load Balancer identifier and Log location.</li> <li>(Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the <code>Load Balancer Name</code>.</li> <li>In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/elb/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - ELB Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Plugins <code>&lt;Optional&gt;</code> List of plugins delimited by comma. Leave it blank if there are no available plugins to use. Valid inputs are <code>user_agent</code>, <code>geo_ip</code>. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/elb/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/include-cfn-common/","title":"Include cfn common","text":"<ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/include-cfn-plugins-common/","title":"Include cfn plugins common","text":"<ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Plugins <code>&lt;Optional&gt;</code> List of plugins delimited by comma. Leave it blank if there are no available plugins to use. Valid inputs are <code>user_agent</code>, <code>geo_ip</code>. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/include-cw-cfn-common/","title":"Include cw cfn common","text":"<ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the Centralized Logging with OpenSearch in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name to export the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the the logs. Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Log Group Names <code>&lt;Requires input&gt;</code> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <code>&lt;requires input&gt;</code> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <code>&lt;requires input&gt;</code> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for SQS encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/aws-services/include-dashboard/","title":"Include dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p>"},{"location":"implementation-guide/aws-services/include-index-pattern/","title":"Include index pattern","text":"<ul> <li>Choose the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and enter the index pattern name. Choose Next step.</li> <li>Specify time field, and choose Create index pattern.</li> </ul>"},{"location":"implementation-guide/aws-services/include-supported-service-logs/","title":"Include supported service logs","text":"<p>The following table lists the supported AWS services and the corresponding features.</p> AWS Service Log Type Log Location Automatic Ingestion Built-in Dashboard Amazon CloudTrail N/A S3 Yes Yes Amazon S3 Access logs S3 Yes Yes Amazon RDS/Aurora MySQL Logs CloudWatch Logs Yes Yes Amazon CloudFront Standard access logs S3 Yes Yes Application Load Balancer Access logs S3 Yes Yes AWS WAF Web ACL logs S3 Yes Yes AWS Lambda N/A CloudWatch Logs Yes Yes Amazon VPC Flow logs S3 Yes Yes AWS Config N/A S3 Yes Yes <ul> <li>Automatic Ingestion: The solution detects the log location of the resource automatically and then reads the logs.</li> <li>Built-in Dashboard: An out-of-box dashboard for the specified AWS service. The solution will automatically ingest a dashboard into the Amazon OpenSearch Service.</li> </ul>"},{"location":"implementation-guide/aws-services/lambda/","title":"AWS Lambda Logs","text":"<p>AWS Lambda automatically monitors Lambda functions on your behalf and sends function metrics to Amazon CloudWatch.</p>"},{"location":"implementation-guide/aws-services/lambda/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The Lambda region must be the same as the Centralized Logging with OpenSearch solution.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/lambda/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch Console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose AWS Lambda.</li> <li>Choose Next.</li> <li>Under Specify settings, choose the Lambda function from the dropdown list. (Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first.</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Lambda function name.</li> <li>In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/lambda/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Lambda Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the Centralized Logging with OpenSearch in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name to export the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the the logs. Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Log Group Names <code>&lt;Requires input&gt;</code> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <code>&lt;requires input&gt;</code> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <code>&lt;requires input&gt;</code> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for SQS encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/aws-services/lambda/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/rds/","title":"Amazon RDS/Aurora Logs","text":"<p>You can publish database instance logs to Amazon CloudWatch Logs. Then, you can perform real-time analysis of the log data, store the data in highly durable storage, and manage the data with the CloudWatch Logs Agent.</p>"},{"location":"implementation-guide/aws-services/rds/#prerequisites","title":"Prerequisites","text":"<p>Make sure your database logs are enabled. Some databases logs are not enabled by default, and you need to update your database parameters to enable the logs.</p> <p>Refer to How do I enable and monitor logs for an Amazon RDS MySQL DB instance? to learn how to output logs to CloudWatch Logs.</p> <p>The table below lists the requirements for RDS/Aurora MySQL parameters.</p> Parameter Requirement Audit Log The database instance must use a custom option group with the <code>MARIADB_AUDIT_PLUGIN</code> option. General log The database instance must use a custom parameter group with the parameter setting <code>general_log = 1</code> to enable the general log. Slow query log The database instance must use a custom parameter group with the parameter setting <code>slow_query_log = 1</code> to enable the slow query log. Log output The database instance must use a custom parameter group with the parameter setting <code>log_output = FILE</code> to write logs to the file system and publish them to CloudWatch Logs."},{"location":"implementation-guide/aws-services/rds/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The RDS and CloudWatch region must be the same as the Centralized Logging with OpenSearch solution region.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/rds/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch Console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose Amazon RDS.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for RDS log enabling. The automatic mode will detect your RDS log configurations and ingest logs from CloudWatch.<ul> <li>For Automatic mode, choose the RDS cluster from the dropdown list.</li> <li>For Manual mode, enter the DB identifier, select the Database type and input the CloudWatch log location in Log type and location.</li> <li>(Optional) If you are ingesting RDS/Aurora logs from another account, select a linked account from the Account dropdown first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the <code>Database identifier</code>.</li> <li>In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/rds/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - RDS Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the Centralized Logging with OpenSearch in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name to export the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the the logs. Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Log Group Names <code>&lt;Requires input&gt;</code> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <code>&lt;requires input&gt;</code> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <code>&lt;requires input&gt;</code> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for SQS encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/aws-services/rds/#sample-dashboards","title":"Sample Dashboards","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p>"},{"location":"implementation-guide/aws-services/rds/#rdsaurora-mysql","title":"RDS/Aurora MySQL","text":""},{"location":"implementation-guide/aws-services/s3/","title":"Amazon S3 Logs","text":"<p>Amazon S3 server access logging provides detailed records for the requests made to the bucket. S3 access logs can be enabled and saved in another S3 bucket.</p>"},{"location":"implementation-guide/aws-services/s3/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The S3 Bucket region must be the same as the Centralized Logging with OpenSearch solution region.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/s3/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch Console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose Amazon S3.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for S3 Access Log enabling. The automatic mode will enable the S3 Access Log and save the logs to a centralized S3 bucket if logging is not enabled yet.<ul> <li>For Automatic mode, choose the S3 bucket from the dropdown list.</li> <li>For Manual mode, enter the Bucket Name and S3 Access Log location.</li> <li>(Optional) If you are ingesting Amazon S3 logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your bucket name.</li> <li>In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/s3/#using-the-standalone-cloudformation-stack","title":"Using the standalone CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - S3 Access Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/s3/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/vpc/","title":"VPC Flow Logs","text":"<p>VPC Flow Logs enable you to capture information about the IP traffic going to and from network interfaces in your VPC.</p>"},{"location":"implementation-guide/aws-services/vpc/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>Centralized Logging with OpenSearch supports VPCs who publish the flow log data to an Amazon S3 bucket or a CloudWatch log group. When publishing to S3, The S3 Bucket region must be the same as the Centralized Logging with OpenSearch solution region.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/vpc/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch Console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose VPC Flow Logs.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for VPC Flow Log enabling. The automatic mode will enable the VPC Flow Log and save the logs to a centralized S3 bucket if logging is not enabled yet.<ul> <li>For Automatic mode, choose the VPC from the dropdown list.</li> <li>For Manual mode, enter the VPC Name and VPC Flow Logs location.</li> <li>(Optional) If you are ingesting VPC Flow logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Under Log Source, select S3 or CloudWatch as the source.</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your VPC name.</li> <li>In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/vpc/#using-the-standalone-cloudformation-stack","title":"Using the standalone CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - VPC Flow Logs Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Other Suffix&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <code>&lt;Optional&gt;</code> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when warm storage is enabled in OpenSearch. Age to Cold Storage <code>&lt;Optional&gt;</code> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effecitve when cold storage is enabled in OpenSearch. Age to Retain <code>&lt;Optional&gt;</code> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <code>&lt;Optional&gt;</code> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <code>&lt;Index Prefix&gt;-&lt;Log Type&gt;-&lt;Index Suffix&gt;-000001</code>. Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/vpc/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/waf/","title":"AWS WAF Logs","text":"<p>WAF Access logs provide detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched.</p>"},{"location":"implementation-guide/aws-services/waf/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>You must deploy Centralized Logging with OpenSearch solution in the same region as your Web ACLs, or you will not be able to create a WAF pipeline. For example:<ul> <li>If your Web ACL is associated with Global Cloudfront, your must deploy the solution in us-east-1.</li> <li>If your Web ACL is associated with other resources in regions like Ohio, your Centralized Logging with OpenSearch stack must also be deployed in that region.</li> </ul> </li> <li>The WAF logging bucket must be the same as the Centralized Logging with OpenSearch solution.</li> <li>WAF Classic logs are not supported in Centralized Logging with OpenSearch. Learn more about migrating rules from WAF Classic to the new AWS WAF.</li> <li>The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.</li> </ul>"},{"location":"implementation-guide/aws-services/waf/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch Console","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose AWS WAF.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual.<ul> <li>For Automatic mode, choose a Web ACL in the dropdown list.</li> <li>For Manual mode, enter the Web ACL name.</li> <li>(Optional) If you are ingesting WAF logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Specify an Ingest Options. Choose between Sampled Request or Full Request.<ul> <li>For Sampled Request, enter how often you want to ingest sampled requests in minutes.</li> <li>For Full Request, if the Web ACL log is not enabled, choose Enable to enable the access log, or enter Log location in Manual mode. Note that Centralized Logging with OpenSearch will automatically enable logging with a Kinesis Data Firehose stream as destination for your WAF.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard.</li> <li>You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the <code>Web ACL Name</code>.</li> <li>In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/waf/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - WAF Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Regions (Full Request) Template AWS China Regions (Full Request) Template AWS Regions (Sampled Request) Template AWS China Regions (Sampled Request) Template <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> <ul> <li>Parameters for Full Request only</li> </ul> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. <ul> <li>Parameters for Sampled Request only</li> </ul> Parameter Default Description WebACL Names <code>&lt;Requires input&gt;</code> The list of Web ACL names, delimited by comma. Interval <code>1</code> The default interval (in minutes) to get sampled logs. <ul> <li>Common parameters</li> </ul> Parameter Default Description Log Source Account ID <code>&lt;Optional&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/waf/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/deployment/","title":"Overview","text":"<p>Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this  guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.</p>"},{"location":"implementation-guide/deployment/#prerequisites","title":"Prerequisites","text":"<p>Review all the considerations and make sure you have the following in the target region you want to deploy the solution:</p> <ul> <li>At least one vacancy to create new VPCs, if you choose to launch with new VPC.</li> <li>At least two vacant Elastic IP (EIP) addresses, if you choose to launch with new VPC.</li> <li>At least four vacant S3 buckets.</li> </ul>"},{"location":"implementation-guide/deployment/#deployment-in-aws-regions","title":"Deployment in AWS Regions","text":"<p>Centralized Logging with OpenSearch provides two ways to authenticate and log into the Centralized Logging with OpenSearch console. For some AWS regions where Cognito User Pool is not available (for example, Hong Kong), you need to launch the solution with OpenID Connect provider. </p> <ul> <li>Launch with Cognito User Pool</li> <li>Launch with OpenID Connect</li> </ul> <p>For more information about supported regions, see Regional deployments.</p>"},{"location":"implementation-guide/deployment/#deployment-in-aws-china-regions","title":"Deployment in AWS China Regions","text":"<p>AWS China Regions do not have Cognito User Pool. You must launch the solution with OpenID Connect.</p> <ul> <li>Launch with OpenID Connect</li> </ul>"},{"location":"implementation-guide/deployment/with-cognito/","title":"Launch with Cognito User Pool","text":"<p>Time to deploy: Approximately 15 minutes</p>"},{"location":"implementation-guide/deployment/with-cognito/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Launch the stack</p> <p>Step 2. Launch the web console</p>"},{"location":"implementation-guide/deployment/with-cognito/#step-1-launch-the-stack","title":"Step 1. Launch the stack","text":"<p>This AWS CloudFormation template automatically deploys the Centralized Logging with OpenSearch solution on AWS.</p> <ol> <li> <p>Sign in to the AWS Management Console and select the button to launch the AWS CloudFormation template.</p> Launch in AWS Console Launch with a new VPC Launch with an existing VPC </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Centralized Logging with OpenSearch solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li>If you are launching the solution in a new VPC, this solution uses the following parameters:</li> </ul> Parameter Default Description Admin User Email <code>&lt;Requires input&gt;</code> Specify the email of the Administrator. This email address will receive a temporary password to access the Centralized Logging with OpenSearch web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. <ul> <li>If you are launching the solution in an existing VPC, this solution uses the following parameters:</li> </ul> Parameter Default Description Admin User Email <code>&lt;Requires input&gt;</code> Specify the email of the Administrator. This email address will receive a temporary password to access the Centralized Logging with OpenSearch web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. VPC ID <code>&lt;Requires input&gt;</code> Specify the existing VPC ID in which you are launching the Centralized Logging with OpenSearch solution. Public Subnet IDs <code>&lt;Requires input&gt;</code> Specify the two public subnets in the selected VPC. The subnets must have routes point to an Internet Gateway. Private Subnet IDs <code>&lt;Requires input&gt;</code> Specify the two private subnets in the selected VPC. The subnets must have routes point to an NAT Gateway. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Select the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/deployment/with-cognito/#step-2-launch-the-web-console","title":"Step 2. Launch the web Console","text":"<p>After the stack is successfully created, this solution generates a CloudFront domain name that gives you access to the Centralized Logging with OpenSearch web console. Meanwhile, an auto-generated temporary password (excluding the last digit <code>.</code>) will be sent to your email address.</p> <ol> <li> <p>Sign in to the AWS CloudFormation console.</p> </li> <li> <p>On the Stacks page, select the solution\u2019s stack.</p> </li> <li> <p>Choose the Outputs tab and record the domain name.</p> </li> <li> <p>Open the WebConsoleUrl using a web browser, and navigate to a sign-in page.</p> </li> <li> <p>Enter the Email and the temporary password.</p> <p>a. Set a new account password.</p> <p>b. (Optional) Verify your email address for account recovery.</p> </li> <li> <p>After the verification is complete, the system opens the Centralized Logging with OpenSearch web console.</p> </li> </ol> <p>Once you have logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain and build log analytics pipelines.</p>"},{"location":"implementation-guide/deployment/with-oidc/","title":"Launch with OpenID Connect (OIDC)","text":"<p>Time to deploy: Approximately 30 minutes</p>"},{"location":"implementation-guide/deployment/with-oidc/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>The Centralized Logging with OpenSearch console is served via CloudFront distribution which is considered as an Internet information service. If you are deploying the solution in AWS China Regions, the domain must have a valid ICP Recordal.</p> <ul> <li>A domain. You will use this domain to access the Centralized Logging with OpenSearch console (Required for AWS China Regions, optional for AWS Regions).</li> <li>An SSL certificate in AWS IAM. The SSL must be associated with the given domain. Follow this guide to upload SSL certificate to IAM. Note that this is required for AWS China Regions, but is not recommended for AWS Regions.</li> <li>Make sure to request or import the ACM certificate in the US East (N. Virginia) Region (us-east-1). Note that this is not required for AWS China Regions, and is optional for AWS Regions.</li> </ul>"},{"location":"implementation-guide/deployment/with-oidc/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Create OIDC client</p> <p>Step 2. Launch the stack</p> <p>Step 3. Setup DNS Resolver</p> <p>Step 4. Launch the web console</p>"},{"location":"implementation-guide/deployment/with-oidc/#step-1-create-oidc-client","title":"Step 1. Create OIDC client","text":"<p>You can use different kinds of OpenID Connector (OIDC) providers. This section introduces Option 1 to Option 4.</p> <ul> <li>(Option 1) Using Amazon Cognito from another region as OIDC provider.</li> <li>(Option 2) Authing, which is an example of a third-party authentication provider.</li> <li>(Option 3) Keycloak, which is a solution maintained by AWS and can serve as an authentication identity provider.</li> <li>(Option 4) ADFS, which is a service offered by Microsoft.</li> <li>(Option 5) Other third-party authentication platforms such as Auth0.</li> </ul> <p>Follow the steps below to create an OIDC client, and obtain the <code>client_id</code> and <code>issuer</code>.</p>"},{"location":"implementation-guide/deployment/with-oidc/#option-1-using-cognito-user-pool-from-another-region","title":"(Option 1) Using Cognito User Pool from another region","text":"<p>You can leverage the Cognito User Pool in a supported AWS Standard Region as the OIDC provider.</p> <ol> <li>Go to the Amazon Cognito console in an AWS Standard Region.</li> <li>Set up the hosted UI with the Amazon Cognito console based on this guide.</li> <li>Choose Public client when selecting the App type.</li> <li>Enter the Callback URL and Sign out URL using your domain name for Centralized Logging with OpenSearch console. If your hosted UI is set up, you should be able to see something like below.        </li> <li>Save the App client ID, User pool ID and the AWS Region to a file, which will be used later.         </li> </ol> <p>In Step 2. Launch the stack, the OidcClientID is the <code>App client ID</code>, and OidcProvider is <code>https://cognito-idp.${REGION}.amazonaws.com/${USER_POOL_ID}</code>.</p>"},{"location":"implementation-guide/deployment/with-oidc/#option-2-authingcn-oidc-client","title":"(Option 2) Authing.cn OIDC client","text":"<ol> <li>Go to the Authing console.</li> <li>Create a user pool if you don't have one.</li> <li>Select the user pool.</li> <li>On the left navigation bar, select Self-built App under Applications.</li> <li>Click the Create button.</li> <li>Enter the Application Name, and Subdomain.</li> <li> <p>Save the <code>App ID</code> (that is, <code>client_id</code>) and <code>Issuer</code> to a text file from Endpoint Information, which will be used later.     </p> </li> <li> <p>Update the <code>Login Callback URL</code> and <code>Logout Callback URL</code> to your IPC recorded domain name.</p> </li> <li> <p>Set the Authorization Configuration.     </p> </li> </ol> <p>You have successfully created an authing self-built application.</p>"},{"location":"implementation-guide/deployment/with-oidc/#option-3-keycloak-oidc-client","title":"(Option 3) Keycloak OIDC client","text":"<ol> <li> <p>Deploy the Keycloak solution in AWS China Regions following this guide.</p> </li> <li> <p>Sign in to the Keycloak console.</p> </li> <li> <p>On the left navigation bar, select Add realm. Skip this step if you already have a realm.</p> </li> <li> <p>Go to the realm setting page. Choose Endpoints, and then OpenID Endpoint Configuration from the list.</p> <p></p> </li> <li> <p>In the JSON file that opens up in your browser, record the issuer value which will be used later.</p> <p></p> </li> <li> <p>Go back to Keycloak console and select Clients on the left navigation bar, and choose Create.</p> </li> <li>Enter a Client ID, which must contain 24 letters (case-insensitive) or numbers. Record the Client ID which will be used later.</li> <li> <p>Change client settings. Enter <code>https://&lt;Centralized Logging with OpenSearch Console domain&gt;</code> in Valid Redirect URIs\uff0cand enter <code>*</code> and <code>+</code> in Web Origins.</p> </li> <li> <p>In the Advanced Settings, set the Access Token Lifespan to at least 5 minutes.</p> </li> <li>Select Users on the left navigation bar.</li> <li>Click Add user and enter Username.</li> <li>After the user is created, select Credentials, and enter Password.</li> </ol> <p>The issuer value is <code>https://&lt;KEYCLOAK_DOMAIN_NAME&gt;/auth/realms/&lt;REALM_NAME&gt;</code>.</p>"},{"location":"implementation-guide/deployment/with-oidc/#option-4-adfs-openid-connect-client","title":"(Option 4) ADFS OpenID Connect Client","text":"<ol> <li>Make sure your ADFS is installed. For information about how to install ADFS, refer to this guide.</li> <li>Make sure you can log in to the ADFS Sign On page. The URL should be <code>https://adfs.domain.com/adfs/ls/idpinitiatedSignOn.aspx</code>, and you need to replace adfs.domain.com with your real ADFS domain.</li> <li>Log on your Domain Controller, and open Active Directory Users and Computers.</li> <li> <p>Create a Security Group for Centralized Logging with OpenSearch Users, and add your planned Centralized Logging with OpenSearch users to this Security Group.</p> </li> <li> <p>Log on to ADFS server, and open ADFS Management.</p> </li> <li> <p>Right click Application Groups, choose Application Group, and enter the name for the Application Group. Select Web browser accessing a web application option under Client-Server Applications, and choose Next.</p> </li> <li> <p>Record the Client Identifier (<code>client_id</code>) under Redirect URI, enter your Centralized Logging with OpenSearch domain (for example, <code>xx.domain.com</code>), and choose Add, and then choose Next.</p> </li> <li> <p>In the Choose Access Control Policy window, select Permit specific group, choose parameters under Policy part, add the created Security Group in Step 4, then click Next. You can configure other access control policy based on your requirements.</p> </li> <li> <p>Under Summary window, choose Next, and choose Close.</p> </li> <li> <p>Open the Windows PowerShell on ADFS Server, and run the following commands to configure ADFS to allow CORS for your planned URL.</p> <pre><code>Set-AdfsResponseHeaders -EnableCORS $true\nSet-AdfsResponseHeaders -CORSTrustedOrigins https://&lt;your-centralized-logging-with-opensearch-domain&gt;\n</code></pre> </li> <li> <p>Under Windows PowerShell on ADFS server, run the following command to get the Issuer (<code>issuer</code>) of ADFS, which is similar to <code>https://adfs.domain.com/adfs</code>.</p> <pre><code>Get-ADFSProperties | Select IdTokenIssuer\n</code></pre> <p></p> </li> </ol>"},{"location":"implementation-guide/deployment/with-oidc/#step-2-launch-the-stack","title":"Step 2. Launch the stack","text":"<p>Important</p> <p>You can only have one active Centralized Logging with OpenSearch solution stack in one region of an AWS account. If your deployment failed (for example, not meeting the requirements in prerequisites), make sure you have deleted the failed stack before retrying the deployment.</p> <ol> <li> <p>Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template.</p> Launch in AWS Console Launch with a new VPC in AWS Regions Launch with an existing VPC in AWS Regions Launch with a new VPC in AWS China Regions Launch with an existing VPC in AWS China Regions </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Centralized Logging with OpenSearch solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide.</li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li>If you are launching the solution in a new VPC, this solution uses the following parameters:</li> </ul> Parameter Default Description OidcClientId <code>&lt;Requires input&gt;</code> OpenID Connector client Id. OidcProvider <code>&lt;Requires input&gt;</code> OpenID Connector provider issuer. The issuer must begin with <code>https://</code> Domain <code>&lt;Optional&gt;</code> Custom domain for Centralized Logging with OpenSearch console. Do NOT add <code>http(s)</code> prefix. IamCertificateID <code>&lt;Optional&gt;</code> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the <code>list-server-certificates</code> command to retrieve the ID. AcmCertificateArn <code>&lt;Optional&gt;</code> Arn for ACM certificates requested (or imported) the certificate in the US East (N. Virginia) Region (us-east-1). <ul> <li>If you are launching the solution in an existing VPC, this solution uses the following parameters:</li> </ul> Parameter Default Description OidcClientId <code>&lt;Requires input&gt;</code> OpenID Connector client Id. OidcProvider <code>&lt;Requires input&gt;</code> OpenID Connector provider issuer. The issuer must begin with <code>https://</code> Domain <code>&lt;Optional&gt;</code> Custom domain for Centralized Logging with OpenSearch console. Do NOT add <code>http(s)</code> prefix. IamCertificateID <code>&lt;Optional&gt;</code> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the <code>list-server-certificates</code> command to retrieve the ID. AcmCertificateArn <code>&lt;Optional&gt;</code> Arn for ACM certificates requested (or imported) the certificate in the US East (N. Virginia) Region (us-east-1). VPC ID <code>&lt;Requires input&gt;</code> Specify the existing VPC ID in which you are launching the solution. Public Subnet IDs <code>&lt;Requires input&gt;</code> Specify the two public subnets in the selected VPC. The subnets must have routes pointing to an Internet Gateway. Private Subnet IDs <code>&lt;Requires input&gt;</code> Specify the two private subnets in the selected VPC. The subnets must have routes pointing to an NAT Gateway. <p>Important</p> <ul> <li>If you are deploying the solution in AWS China Regions, you must enter Domain, and IamCertificateID.</li> <li>If you are deploying the solution in AWS Regions,<ul> <li>when a custom domain name is required, you must enter Domain, and AcmCertificateArn.</li> <li>when no custom domain name is required, leave it blank for Domain, IamCertificateID, and AcmCertificateArn.</li> </ul> </li> </ul> </li> <li> <p>Choose Next.</p> </li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</li> <li>Choose Create stack  to deploy the stack.</li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/deployment/with-oidc/#step-3-setup-dns-resolver","title":"Step 3. Setup DNS Resolver","text":"<p>This solution provisions a CloudFront distribution that gives you access to the Centralized Logging with OpenSearch console.</p> <ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select the solution's stack.</li> <li>Choose the Outputs tab.</li> <li>Obtain the WebConsoleUrl as the endpoint.</li> <li>Create a CNAME record in DNS resolver, which points to the endpoint address.</li> </ol>"},{"location":"implementation-guide/deployment/with-oidc/#step-4-launch-the-web-console","title":"Step 4. Launch the web console","text":"<p>Important</p> <p>You login credentials is managed by the OIDC provider. Before signing in to the Centralized Logging with OpenSearch console, make sure you have created at least one user in the OIDC provider's user pool.</p> <ol> <li>Use the previous assigned CNAME to open the OIDC Customer Domain URL using a web browser.</li> <li>Choose Sign in to Centralized Logging with OpenSearch, and navigate to OIDC provider.</li> <li>Enter sign-in credentials. You may be asked to change your default password for first-time login, which depends on your OIDC provider's policy.</li> <li>After the verification is complete, the system opens the Centralized Logging with OpenSearch web console.</li> </ol> <p>Once you have logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain and build log analytics pipelines.</p>"},{"location":"implementation-guide/domains/","title":"Overview","text":"<p>This chapter describes how to manage Amazon OpenSearch Service domains through the Centralized Logging with OpenSearch console. An Amazon OpenSearch Service domain is synonymous with an Amazon OpenSearch Service cluster.</p> <p>In this chapter, you will learn: </p> <ul> <li>Import &amp; remove an Amazon OpenSearch Service Domain</li> <li>Create an access proxy</li> <li>Create recommended alarms</li> </ul> <p>You can read the Getting Started chapter first and walk through the basic steps for using the Centralized Logging with OpenSearch solution.</p>"},{"location":"implementation-guide/domains/alarms/","title":"Recommended Alarms","text":"<p>Amazon OpenSearch provides a set of recommended CloudWatch alarms to monitor the health of Amazon OpenSearch Service domains. Centralized Logging with OpenSearch helps you to create the alarms automatically, and send notification to your email (or SMS) via SNS.</p>"},{"location":"implementation-guide/domains/alarms/#create-alarms","title":"Create alarms","text":""},{"location":"implementation-guide/domains/alarms/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch console","text":"<ol> <li>Log in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Under General configuration, choose Enable at the Alarms label.</li> <li>Enter the Email.</li> <li>Choose the alarms you want to create and adjust the settings if necessary.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/domains/alarms/#using-the-cloudformation-stack","title":"Using the CloudFormation stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Alarms solution in the AWS Cloud.</p> <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template.</p> <p></p> <p>You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Endpoint <code>&lt;Requires input&gt;</code> The endpoint of the OpenSearch domain, for example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code>. DomainName <code>&lt;Requires input&gt;</code> The name of the OpenSearch domain. Email <code>&lt;Requires input&gt;</code> The notification email address. Alarms will be sent to this email address via SNS. ClusterStatusRed <code>Yes</code> Whether to enable alarm when at least one primary shard and its replicas are not allocated to a node. ClusterStatusYellow <code>Yes</code> Whether to enable alarm when at least one replica shard is not allocated to a node. FreeStorageSpace <code>10</code> Whether to enable alarm when a node in your cluster is down to the free storage space you entered in GiB. We recommend setting it to 25% of the storage space for each node. <code>0</code> means the alarm is disabled. ClusterIndexWritesBlocked <code>1</code> Index writes blocked error occurs for &gt;= x times in 5 minutes, 1 consecutive time. Input <code>0</code> to disable this alarm. UnreachableNodeNumber <code>3</code> Nodes minimum is &lt; x for 1 day, 1 consecutive time. <code>0</code> means the alarm is disabled. AutomatedSnapshotFailure <code>Yes</code> Whether to enable alarm when automated snapshot failed. AutomatedSnapshotFailure maximum is &gt;= 1 for 1 minute, 1 consecutive time. CPUUtilization <code>Yes</code> Whether to enable alarm when sustained high usage of CPU occurred. CPUUtilization or WarmCPUUtilization maximum is &gt;= 80% for 15 minutes, 3 consecutive times. JVMMemoryPressure <code>Yes</code> Whether to enable alarm when JVM RAM usage peak occurred. JVMMemoryPressure or WarmJVMMemoryPressure maximum is &gt;= 80% for 5 minutes, 3 consecutive times. MasterCPUUtilization <code>Yes</code> Whether to enable alarm when sustained high usage of CPU occurred in master nodes. MasterCPUUtilization maximum is &gt;= 50% for 15 minutes, 3 consecutive times. MasterJVMMemoryPressure <code>Yes</code> Whether to enable alarm when JVM RAM usage peak occurred in master nodes. MasterJVMMemoryPressure maximum is &gt;= 80% for 15 minutes, 1 consecutive time. KMSKeyError <code>Yes</code> Whether to enable alarm when KMS encryption key is disabled. KMSKeyError is &gt;= 1 for 1 minute, 1 consecutive time. KMSKeyInaccessible <code>Yes</code> Whether to enable alarm when KMS encryption key has been deleted or has revoked its grants to OpenSearch Service. KMSKeyInaccessible is &gt;= 1 for 1 minute, 1 consecutive time. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 5 minutes.</p> <p>Once you have created the alarms, a confirmation email will be sent to your email address. You need to click the Confirm link in the email.</p> <p>Go to the CloudWatch Alarms page by choosing the General configuration &gt; Alarms &gt; CloudWatch Alarms link on the Centralized Logging with OpenSearch console, and the link location is shown as follows:</p> <p></p> <p>Make sure that all the alarms are in OK status because you might have missed the notification if alarms have changed its status before subscription.</p> <p>Note</p> <p>The alarm will not send SNS notification to your email address if triggered before subscription. We recommend you check the alarms status after enabling the OpenSearch alarms. If you see any alarm which is in In Alarm status, you should fix that issue first.</p>"},{"location":"implementation-guide/domains/alarms/#delete-alarms","title":"Delete alarms","text":"<ol> <li>Log in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Choose the Alarms tab.</li> <li>Choose the Delete.</li> <li>On the confirmation prompt, choose Delete.</li> </ol>"},{"location":"implementation-guide/domains/import/","title":"Domain Operations","text":"<p>Once logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain. </p>"},{"location":"implementation-guide/domains/import/#prerequisite","title":"Prerequisite","text":"<ol> <li>Centralized Logging with OpenSearch supports Amazon OpenSearch Service, engine version Elasticsearch 7.10 or later, and engine version OpenSearch 1.0 or later.</li> <li>Centralized Logging with OpenSearch supports OpenSearch clusters within VPC. If you don't have an Amazon OpenSearch Service domain yet, you can create an Amazon OpenSearch Service domain within VPC. See Launching your Amazon OpenSearch Service domains within a VPC.</li> <li>Centralized Logging with OpenSearch supports OpenSearch clusters with fine-grained access control only. In the security configuration, the Access policy should look like the image below:    </li> </ol>"},{"location":"implementation-guide/domains/import/#import-an-amazon-opensearch-service-domain","title":"Import an Amazon OpenSearch Service Domain","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch console.</li> <li>In the left navigation panel, under Domains, choose Import OpenSearch Domain.</li> <li>On the Select domain page, choose a domain from the dropdown list. The dropdown list will display only domains in the same region as the solution.</li> <li>Choose Next.</li> <li>On the Configure network page, under Network creation, choose Manual and click Next; or choose Automatic, and go to step 9.</li> <li>Under VPC, choose a VPC from the list. By default, the solution creates a standalone VPC, and you can choose the one named <code>LogHubVpc/DefaultVPC</code>. You can also choose the same VPC as your Amazon OpenSearch Service domains.</li> <li>Under Log Processing Subnet Group, select at least 2 subnets from the dropdown list. By default, the solution creates two private subnets. You can choose subnets named <code>LogHubVpc/DefaultVPC/privateSubnet1</code> and <code>LogHubVpc/DefaultVPC/privateSubnet2</code>.</li> <li>Under Log Processing Security Group, select one from the dropdown list. By default, the solution creates one Security Group named <code>ProcessSecurityGroup</code>.</li> <li>On the Create tags page, add tags if needed.</li> <li>Choose Import.</li> </ol>"},{"location":"implementation-guide/domains/import/#set-up-vpc-peering","title":"Set up VPC Peering","text":"<p>By default, the solution creates a standalone VPC. You need to create VPC Peering to allow the log processing layer to have access to your Amazon OpenSearch Service domains.</p> <p>Note</p> <p>Automatic mode will create VPC peering and configure route table automatically. You do not need to set up VPC peering again.</p> <p></p> <p>Follow this section to create VPC peering, update security group and update route tables.</p>"},{"location":"implementation-guide/domains/import/#create-vpc-peering-connection","title":"Create VPC Peering Connection","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch console.</li> <li>In the left navigation panel, under Domains, select OpenSearch Domains.</li> <li>Find the domain you imported and select the domain name.</li> <li>Choose the Network tab.</li> <li>Copy the VPC ID in both sections OpenSearch domain network and Log processing network. You will create Peering Connection between these two VPCs.</li> <li>Navigate to VPC Console Peering Connections.</li> <li>Select the Create peering connection button.</li> <li>On the Create peering connection page, enter a name.</li> <li>For the Select a local VPC to peer with, VPC ID (Requester), select the VPC ID of the Log processing network.</li> <li>For the Select another VPC to peer with, VPC ID (Accepter), select the VPC ID of the OpenSearch domain network.</li> <li>Choose Create peering connection, and navigate to the peering connection detail page.</li> <li>Click the Actions button and choose Accept request.</li> </ol>"},{"location":"implementation-guide/domains/import/#update-route-tables","title":"Update Route Tables","text":"<ol> <li>Go to the Centralized Logging with OpenSearch console.</li> <li>In the OpenSearch domain network section, click the subnet under AZs and Subnets to open the subnet console in a new tab.</li> <li>Select the subnet, and choose the Route table tab.</li> <li>Select the associated route table of the subnet to open the route table configuration page.</li> <li>Select the Routes tab, and choose Edit routes.</li> <li>Add a route <code>10.255.0.0/16</code> (the CIDR of Centralized Logging with OpenSearch, if you created the solution with existing VPC, please change this value) pointing to the Peering Connection you just created.</li> <li>Go back to the Centralized Logging with OpenSearch console.</li> <li>Click the VPC ID under the OpenSearch domain network section.</li> <li>Select the VPC ID on the VPC Console and find its IPv4 CIDR.</li> <li>On the Centralized Logging with OpenSearch console, in the Log processing network section, click the subnets under AZs and Subnets to open the subnets in new tabs.</li> <li>Repeat step 3, 4, 5, 6 to add an opposite route. Namely, configure the IPv4 CIDR of the OpenSearch VPC to point to the Peering Connection. You need to repeat the steps for each subnet of Log processing network.</li> </ol>"},{"location":"implementation-guide/domains/import/#update-security-group-of-opensearch-domain","title":"Update Security Group of OpenSearch Domain","text":"<ol> <li>On the Centralized Logging with OpenSearch console, under the OpenSearch domain network section, select the Security Group ID in Security Groups to open the Security Group in a new tab.</li> <li>On the console, select Edit inbound rules.</li> <li>Add the rule <code>ALLOW TCP/443 from 10.255.0.0/16</code> (the CIDR of Centralized Logging with OpenSearch, if you created Centralized Logging with OpenSearch with existing VPC, change this value).</li> <li>Choose Save rules.</li> </ol>"},{"location":"implementation-guide/domains/import/#remove-an-amazon-opensearch-service-domain","title":"Remove an Amazon OpenSearch Service domain","text":"<p>If needed, you can remove the Amazon OpenSearch Service domains. </p> <p>Important</p> <p>Removing the domain from Centralized Logging with OpenSearch will NOT delete the Amazon OpenSearch Service domain in your AWS account. It will NOT impact any existing log analytics pipelines.</p> <ol> <li>Sign in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose OpenSearch Domains.</li> <li>Select the domain from the table.</li> <li>Choose Remove.</li> <li>In the confirmation dialog box, choose Remove.</li> </ol>"},{"location":"implementation-guide/domains/proxy/","title":"Access Proxy","text":"<p>By default, an Amazon OpenSearch Service domain within VPC cannot be accessed from the Internet. Centralized Logging with OpenSearch creates a highly available Nginx cluster which allows you to access the OpenSearch Dashboards from the Internet. Alternatively, you can choose to access the Amazon OpenSearch Service domains using SSH Tunnel.</p> <p>This section introduces the proxy stack architecture and how to complete the following:</p> <ol> <li>Create a proxy</li> <li>Create an associated DNS record</li> <li>Access Amazon OpenSearch Service via proxy</li> <li>Delete a proxy</li> </ol>"},{"location":"implementation-guide/domains/proxy/#architecture","title":"Architecture","text":"<p>Centralized Logging with OpenSearch creates an Auto Scaling Group (ASG) together with an Application Load Balancer (ALB).</p> <p></p> <p>The workflow is as follows:</p> <ol> <li> <p>Users access the custom domain for the proxy, and the domain needs to be resolved via DNS service (for example, using Route 53 on AWS).</p> </li> <li> <p>The DNS service routes the traffic to internet-facing ALB.</p> </li> <li> <p>The ALB distributes traffic to backend Nginx server running on Amazon EC2 within ASG.</p> </li> <li> <p>The Nginx server redirects the requests to OpenSearch Dashboards.</p> </li> <li> <p>(optional) VPC peering is required if the VPC for the proxy is not the same as the OpenSearch service.</p> </li> </ol>"},{"location":"implementation-guide/domains/proxy/#create-a-proxy","title":"Create a proxy","text":"<p>You can create the Nginx-based proxy using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack.</p> <p>Prerequisites</p> <ul> <li>Make sure an Amazon OpenSearch Service domain within VPC is available.</li> <li>The domain associated SSL certificate is created or uploaded in Amazon Certificate Manager (ACM).</li> <li>Make sure you have the EC2 private key (.pem) file.</li> </ul>"},{"location":"implementation-guide/domains/proxy/#using-the-centralized-logging-with-opensearch-console","title":"Using the Centralized Logging with OpenSearch console","text":"<ol> <li>Log in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li> <p>Under General configuration, choose Enable at the Access Proxy label.</p> <p>Note</p> <p>Once the access proxy is enabled, a link to the access proxy will be available.</p> </li> <li> <p>On the Create access proxy page, under Public access proxy, select at least 2 subnets for Public Subnets. You can choose 2 public subnets named <code>LogHubVPC/DefaultVPC/publicSubnet</code>, which are created by Centralized Logging with OpenSearch by default.</p> </li> <li>Choose a Security Group of the ALB in Public Security Group. You can choose a security group named <code>ProxySecurityGroup</code>, which is created by Centralized Logging with OpenSearch default.</li> <li>Enter the Domain Name.</li> <li>Choose Load Balancer SSL Certificate associated with the domain name.</li> <li>Choose the Nginx Instance Key Name.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/domains/proxy/#using-the-cloudformation-stack","title":"Using the CloudFormation stack","text":"<p>This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Nginx access proxy solution in the AWS Cloud.</p> <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template.</p> <p></p> <p>You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description VPCId <code>&lt;Requires input&gt;</code> The VPC to deploy the Nginx proxy resources, for example, <code>vpc-bef13dc7</code>. PublicSubnetIds <code>&lt;Requires input&gt;</code> The public subnets where ELB are deployed. You need to select at least two public subnets, for example, <code>subnet-12345abc, subnet-54321cba</code>. ELBSecurityGroupId <code>&lt;Requires input&gt;</code> The Security group being associated with the ELB, for example, <code>sg-123456</code>. ELBDomain <code>&lt;Requires input&gt;</code> The custom domain name of the ELB, for example, <code>dashboard.example.com</code>. ELBDomainCertificateArn <code>&lt;Requires input&gt;</code> The SSL certificate ARN associated with the ELBDomain. The certificate must be created from Amazon Certificate Manager (ACM). PrivateSubnetIds <code>&lt;Requires input&gt;</code> The private subnets where Nginx instances are deployed. You need to select at least two private subnets, for example, <code>subnet-12345abc, subnet-54321cba</code>. NginxSecurityGroupId <code>&lt;Requires input&gt;</code> The Security group associated with the Nginx instances. The security group must allow access from ELB security group. KeyName <code>&lt;Requires input&gt;</code> The PEM key name of the Nginx instances. EngineType OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint, for example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code>. CognitoEndpoint <code>&lt;Optional&gt;</code> The Cognito User Pool endpoint URL of the OpenSearch domain, for example, <code>mydomain.auth.us-east-1.amazoncognito.com</code>. Leave empty if your OpenSearch domain is not authenticated through Cognito User Pool. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/domains/proxy/#recommended-proxy-configuration","title":"Recommended Proxy Configuration","text":"<p>The following table provides a list of recommended proxy configuration examples for different number of concurrent users. You can create proxy according to your own use cases.</p> Number of Concurrent Users Proxy Instance Type Number of Proxy Instances 4 t3.nano 1 6 t3.micro 1 8 t3.nano 2 10 t3.small 1 12 t3.micro 2 20 t3.small 2 25 t3.large 1 50+ t3.large 2"},{"location":"implementation-guide/domains/proxy/#create-an-associated-dns-record","title":"Create an associated DNS record","text":"<p>After provisioning the proxy infrastructure, you need to create an associated DNS record in your DNS resolver. The following introduces how to find the ALB domain, and then create a CNAME record pointing to this domain.</p> <ol> <li>Log in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Choose the Access Proxy tab.    You can see Load Balancer Domain which is the ALB domain.</li> <li>Go to the DNS resolver, create a CNAME record pointing to this domain.     If your domain is managed by Amazon Route 53, refer to Creating records by using the Amazon Route 53 console.</li> </ol>"},{"location":"implementation-guide/domains/proxy/#access-amazon-opensearch-service-via-proxy","title":"Access Amazon OpenSearch Service via proxy","text":"<p>After the DNS record takes effect, you can access the Amazon OpenSearch Service built-in dashboard from anywhere via proxy. You can enter the domain of the proxy in your browser, or click the Link button under Access Proxy in the General Configuration section.</p> <p></p>"},{"location":"implementation-guide/domains/proxy/#delete-a-proxy","title":"Delete a Proxy","text":"<ol> <li>Log in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Choose the Access Proxy tab.</li> <li>Choose the Delete.</li> <li>On the confirmation prompt, choose Delete.</li> </ol>"},{"location":"implementation-guide/getting-started/","title":"Getting Started","text":"<p>After deploying the solution, refer to this section to quickly learn how to leverage Centralized Logging with OpenSearch for log ingestion (Amazon CloudTrail logs as an example), and log visualization. </p> <p>You can also choose to start with Domain management , then build AWS Service Log Analytics Pipelines and Application Log Analytics Pipelines.</p>"},{"location":"implementation-guide/getting-started/#steps","title":"Steps","text":"<ul> <li>Step 1: Import an Amazon OpenSearch Service domain. Import an existing Amazon OpenSearch Service domain into the solution.</li> <li>Step 2: Create Access Proxy. Create a public access proxy which allows you to access the templated dashboard from anywhere.</li> <li>Step 3: Ingest CloudTrail Logs. Ingest CloudTrail logs into the specified Amazon OpenSearch Service domain.</li> <li>Step 4: Access built-in dashboard. View the dashboard of CloudTrail logs.  </li> </ul>"},{"location":"implementation-guide/getting-started/1.import-domain/","title":"Step 1: Import an Amazon OpenSearch domain","text":"<p>To use the Centralized Logging with OpenSearch solution for the first time, you must import Amazon OpenSearch Service domains first.</p> <p>Centralized Logging with OpenSearch supports Amazon OpenSearch domain with fine-grained access control enabled within a VPC only. </p> <p>Important</p> <p>Currently, Centralized Logging with OpenSearch supports Amazon OpenSearch Service with engine version Elasticsearch 7.10 or later, and OpenSearch 1.0 or later.</p>"},{"location":"implementation-guide/getting-started/1.import-domain/#prerequisite","title":"Prerequisite","text":"<p>At least one Amazon OpenSearch Service domain within VPC. If you don't have an Amazon OpenSearch Service domain yet, you can create an Amazon OpenSearch Service domain within VPC. See Launching your Amazon OpenSearch Service domains within a VPC. </p>"},{"location":"implementation-guide/getting-started/1.import-domain/#steps","title":"Steps","text":"<p>Use the following procedure to import an Amazon OpenSearch Service domain through the Centralized Logging with OpenSearch console.</p> <ol> <li>Sign in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose Import OpenSearch Domain. </li> <li>On the Step 1. Select domain page, choose a domain from the dropdown list. </li> <li>Choose Next.</li> <li>On the Step 2. Configure network page, under Network creation, choose Automatic. If your Centralized Logging with OpenSearch and OpenSearch domains reside in two different VPCs, the Automatic mode will create a VPC Peering Connection between them, and update route tables. See details in Set up VPC Peering. </li> <li>On the Step 3. Create tags page, choose Import.</li> </ol>"},{"location":"implementation-guide/getting-started/2.create-proxy/","title":"Step 2: Create Access Proxy","text":"<p>Note</p> <p>Access proxy is optional and it incurs additional cost. If you can connect to Amazon OpenSearch's VPC (such as through VPN connection), you don't need to activate access proxy. You need to use it only if you want to connect to Amazon OpenSearch dashboard from public Internet. </p> <p>You can create a Nginx proxy and create an DNS record pointing to the proxy, so that you can access the Amazon OpenSearch Service dashboard securely from public network. For more information, refer to Access Proxy in the Domain Management chapter.</p>"},{"location":"implementation-guide/getting-started/2.create-proxy/#create-a-nginx-proxy","title":"Create a Nginx proxy","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Under General configuration, choose Enable at the Access Proxy label.</li> <li>On the Create access proxy page, under Public access proxy, select at least 2 subnets which contain <code>LogHubVpc/DefaultVPC/publicSubnetX</code> for the Public Subnets.</li> <li>For Public Security Group, choose the Security Group which contains <code>ProxySecurityGroup</code>.</li> <li>Enter the Domain Name.</li> <li>Choose the associated Load Balancer SSL Certificate which applies to the domain name.</li> <li>Choose the Nginx Instance Key Name.     </li> <li>Choose Create.</li> </ol> <p>After provisioning the proxy infrastructure, you need to create an associated DNS record in your DNS resolver. The following introduces how to find the Application Load Balancing (ALB) domain, and then create a CNAME record pointing to this domain.</p>"},{"location":"implementation-guide/getting-started/2.create-proxy/#create-an-dns-record","title":"Create an DNS record","text":"<ol> <li>Sign in to the Centralized Logging with OpenSearch console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Choose the Access Proxy tab. Find Load Balancer Domain, which is the ALB domain.</li> <li>Go to the DNS resolver, and create a CNAME record pointing to this domain. If your domain is managed by Amazon Route 53, refer to Creating records by using the Amazon Route 53 console.</li> </ol>"},{"location":"implementation-guide/getting-started/3.build-cloudtrail-pipeline/","title":"Step 3: Ingest Amazon CloudTrail Logs","text":"<p>You can build a log analytics pipeline to ingest Amazon CloudTrail logs.</p> <p>Important</p> <p>Make sure your CloudTrail and Centralized Logging with OpenSearch are in the same AWS Region.</p> <ol> <li>Sign in to the Centralized Logging with OpenSearch Console.</li> <li>In the navigation pane, select AWS Service Log Analytics Pipelines.</li> <li>Choose Create a log ingestion.</li> <li>In the AWS Services section, choose Amazon CloudTrail.</li> <li>Choose Next.</li> <li>Under Specify settings, for Trail, select one from the dropdown list.</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select the imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard. </li> <li>Keep default values and choose Next.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/getting-started/4.view-dashboard/","title":"Step 4: Access built-in Dashboard","text":"<p>After the DNS record takes effect, you can access the built-in dashboard from anywhere via proxy. </p> <ol> <li>Enter the domain of the proxy in your browser. Alternatively, click the Link button under Access Proxy in the General Configuration section of the domain.</li> <li>Enter your credentials to log in to Amazon OpenSearch Dashboard.</li> <li>Click the username icon of Amazon OpenSearch Service dashboard from the top right corner. </li> <li>Choose Switch Tenants.</li> <li>On the Select your tenant page, choose Global, and click Confirm.</li> <li>On the left navigation panel, choose Dashboards. </li> <li>Choose the dashboard created automatically and start to explore your data.</li> </ol>"},{"location":"implementation-guide/link-account/","title":"Cross-Account Ingestion","text":"<p>Centralized Logging with OpenSearch supports ingesting AWS Service logs and Application logs in different AWS accounts within the same region. After deploying Centralized Logging with OpenSearch in one account (main account), you can launch the CloudFormation stack in a different account (member account), and associate the two accounts (main account and member account) to implement cross-account ingestion.</p>"},{"location":"implementation-guide/link-account/#concepts","title":"Concepts","text":"<ul> <li>Main account: One account in which you deployed the Centralized Logging with OpenSearch console. The OpenSearch cluster(s) must also be in the same account.</li> <li>Member account: Another account from which you want to ingest AWS Service logs or application logs. </li> </ul> <p>The CloudFormation stack in the member account has the least privileges. Centralized Logging with OpenSearch need to provision some AWS resources in the member account to collect logs, and will assume an IAM role provisioned in the member account to list or create resources. </p> <p>For more information, refer to the Architecture section.</p>"},{"location":"implementation-guide/link-account/#add-a-member-account","title":"Add a member account","text":""},{"location":"implementation-guide/link-account/#step-1-launch-a-cloudformation-stack-in-the-member-account","title":"Step 1. Launch a CloudFormation stack in the member account","text":"<ol> <li> <p>Sign in to the Centralized Logging with OpenSearch console.</p> </li> <li> <p>In the navigation pane, under Resources, choose Cross-Account Ingestion. </p> </li> <li> <p>Choose the Link an Account button. It displays the steps to deploy the CloudFormation stack in the member account. </p> <p>Important</p> <p>You need to copy the template URL, which will be used later.</p> </li> <li> <p>Go to the CloudFormation console of the member account.</p> </li> <li> <p>Choose the Create stack button and choose With new resources (standard).</p> </li> <li> <p>In the Create stack page, enter the template URL you have copied in Amazon S3 URL.  </p> </li> <li> <p>Follow the steps to create the CloudFormation stack and wait until the CloudFormation stack is provisioned. </p> </li> <li> <p>Go to the Outputs tab to check the parameters which will be used in Step 2.</p> </li> </ol>"},{"location":"implementation-guide/link-account/#step-2-link-a-member-account","title":"Step 2. Link a member account","text":"<ol> <li>Go back to the Centralized Logging with OpenSearch console.</li> <li>(Optional) In the navigation panel, under Resources, choose Cross-Account Ingestion.</li> <li> <p>In Step 2. Link an account, enter the parameters using the Outputs parameters from Step 1. </p> Parameter CloudFormation Outputs Description Account Name N/A Name of the member account. Account ID N/A 12-digit AWS account ID. Cross Account Role ARN CrossAccountRoleARN Centralized Logging with OpenSearch will assume this role to operate resources in the member account. FluentBit Agent Installation Document AgentInstallDocument Centralized Logging with OpenSearch will use this SSM Document to install Fluent Bit agent on EC2 instances in the member account. FluentBit Agent Configuration Document AgentConfigDocument Centralized Logging with OpenSearch will use this SSM Document to deliver Fluent Bit configuration to EC2 instances. Cross Account S3 Bucket CrossAccountS3Bucket You can use the Centralized Logging with OpenSearch console to enable some AWS Service logs and output them to Amazon S3. The logs will be stored in this account. Cross Account Stack ID CrossAccountStackId CloudFormation stack ID in the member account. Cross Account KMS Key CrossAccountKMSKeyARN Centralized Logging with OpenSearch will use the Key Management Services (KMS) key to encrypt Simple Queue Service (SQS). </li> <li> <p>Click the Link button.</p> </li> </ol>"},{"location":"implementation-guide/resources/aws-services/","title":"AWS Services","text":"<ul> <li>AWS CloudFormation</li> <li>Amazon OpenSearch Service</li> <li>Amazon S3</li> <li>AWS Lambda</li> <li>Amazon CloudFront</li> <li>AWS AppSync</li> <li>Amazon Cognito User Pool</li> <li>AWS Step Functions</li> <li>Amazon DynamoDB</li> <li>AWS Systems Manager</li> <li>Amazon EventBridge</li> <li>Amazon Kinesis Data Streams</li> <li>Amazon Kinesis Data Firehose</li> </ul>"},{"location":"implementation-guide/resources/open-ssl/","title":"OpenSSL 1.1 Installation","text":"<p>Centralized Logging with OpenSearch uses Fluent Bit as the log agent, which requires OpenSSL 1.1 or later. You can install the dependency according to your operating system (OS). It is recommended to make your own AMI with OpenSSL 1.1 installed.</p> <p>Important</p> <p>If your OS is not listed below, you can follow the official installation guide to install OpenSSL.</p>"},{"location":"implementation-guide/resources/open-ssl/#amazon-linux-2","title":"Amazon Linux 2","text":"<pre><code>sudo yum install openssl11\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#ubuntu","title":"Ubuntu","text":""},{"location":"implementation-guide/resources/open-ssl/#2204","title":"22.04","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\nln -s /snap/core18/2667/usr/lib/x86_64-linux-gnu/libssl.so.1.1 /usr/lib/libssl.so.1.1\nln -s /snap/core18/2667/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 /usr/lib/libcrypto.so.1.1\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#2004","title":"20.04","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#1804","title":"18.04","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#debian","title":"Debian","text":""},{"location":"implementation-guide/resources/open-ssl/#gnu10","title":"GNU/10","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#gnu11","title":"GNU/11","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#red-hat-enterprise-linux","title":"Red Hat Enterprise Linux","text":""},{"location":"implementation-guide/resources/open-ssl/#8x","title":"8.X","text":"<p>OpenSSL 1.1 is installed by default.</p>"},{"location":"implementation-guide/resources/open-ssl/#7x","title":"7.X","text":"<pre><code>sudo su -\n\nyum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\n\nsystemctl enable amazon-ssm-agent\nsystemctl start amazon-ssm-agent\n\nyum install -y wget perl unzip gcc zlib-devel\nmkdir /tmp/openssl\ncd /tmp/openssl\nwget https://www.openssl.org/source/openssl-1.1.1s.tar.gz\ntar xzvf openssl-1.1.1s.tar.gz\ncd openssl-1.1.1s\n./config --prefix=/usr/local/openssl11 --openssldir=/usr/local/openssl11 shared zlib\nmake\nmake install\n\necho /usr/local/openssl11/lib/ &gt;&gt; /etc/ld.so.conf\nldconfig\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#suse-linux-enterprise-server","title":"SUSE Linux Enterprise Server","text":""},{"location":"implementation-guide/resources/open-ssl/#15","title":"15","text":"<p>OpenSSL 1.1 is installed by default.</p>"},{"location":"implementation-guide/resources/upload-ssl-certificate/","title":"Upload SSL Certificate to IAM","text":"<p>Upload the SSL certificate by running the AWS CLI command <code>upload-server-certificate</code> similar to the following:</p> <pre><code>aws iam upload-server-certificate --path /cloudfront/ \\\n--server-certificate-name YourCertificate \\\n--certificate-body file://Certificate.pem \\\n--certificate-chain file://CertificateChain.pem \\\n--private-key file://PrivateKey.pem\n</code></pre> <p>Replace the file names and Your Certificate with the names for your uploaded files and certificate. You must specify the <code>file://</code> prefix in the certificate-body, certificate-chain and private-key parameters in the API request.  Otherwise, the request fails with a <code>MalformedCertificate: Unknown</code> error message.</p> <p>Note</p> <p>You must specify a path using the --path option. The path must begin with /cloudfront and must include a   trailing slash (for example, /cloudfront/test/).</p> <p>After the certificate is uploaded, the AWS command <code>upload-server-certificate</code> returns metadata for the uploaded certificate, including the certificate's Amazon Resource Name (ARN), friendly name, identifier (ID), and expiration date.</p> <p>To view the uploaded certificate, run the AWS CLI command <code>list-server-certificates</code>:</p> <pre><code>aws iam list-server-certificates\n</code></pre> <p>For more information, see uploading a server certificate to IAM.</p>"}]}