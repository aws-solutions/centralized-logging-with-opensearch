{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Centralized Logging with OpenSearch solution provides comprehensive log management and analysis functions to help you simplify the build of log analytics pipelines. Built on top of Amazon OpenSearch Service, the solution allows you to streamline log ingestion, log processing, and log visualization. You can leverage the solution in multiple use cases such as to abide by security and compliance regulations, achieve refined business operations, and enhance IT troubleshooting and maintenance. Use this navigation table to quickly find answers to these questions: If you want to \u2026 Read\u2026 Know the cost for running this solution Cost Understand the security considerations for this solution Security Know which AWS Regions are supported for this solution Supported AWS Regions Get started with the solution quickly to import an Amazon OpenSearch Service domain, build a log analytics pipeline, and access the built-in dashboard Getting started Learn the operations related to Amazon OpenSearch Service domains Domain management Walk through the processes of building log analytics pipelines AWS Services logs and Applications logs This implementation guide describes architectural considerations and configuration steps for deploying the Centralized Logging with OpenSearch solution in the AWS cloud. It includes links to CloudFormation templates that launches and configures the AWS services required to deploy this solution using AWS best practices for security and availability. The guide is intended for IT architects, developers, DevOps, data engineers with practical experience architecting on the AWS Cloud.","title":"Overview"},{"location":"implementation-guide/alarm/","text":"There are different types of log alarms: log processor alarms, buffer layer alarms, and source alarms (only for application log pipeline). The alarms will be triggered when the defined condition is met. Log alarm type Log alarm condition Description Log processor alarms Error invocation # >= 10 for 5 minutes, 1 consecutive time When the number of log processor Lambda error calls is greater than or equal to 10 within 5 minutes (including 5 minutes), an email alarm will be triggered. Log processor alarms Failed record # >= 1 for 1 minute, 1 consecutive time When the number of failed records is greater than or equal to 1 within a 1-minute window, an alarm will be triggered. Log processor alarms Average execution duration in last 5 minutes >= 60000 milliseconds In the last 5 minutes, when the average execution time of log processor Lambda is greater than or equal to 60 seconds, an email alarm will be triggered. Buffer layer alarms SQS Oldest Message Age >= 30 minutes When the age of the oldest SQS message is greater than or equal to 30 minutes, it means that the message has not been consumed for at least 30 minutes, an email alarm will be triggered. Source alarms (only for application log pipeline) Fluent Bit output_retried_record_total >= 100 for last 5 minutes When the total number of retry records output by Fluent Bit in the past 5 minutes is greater than or equal to 100, an email alarm will be triggered. You can choose to enable log alarms or disable them according to your needs. Enable log alarms Sign in to the Centralized Logging with OpenSearch console. In the left navigation bar, under Log Analytics Pipelines , choose AWS Service Log or Application Log . Select the log pipeline created and choose View details . Select the Alarm tab. Switch on Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , you need to provide email address for the newly-created SNS topic to notify. Disable log alarms Sign in to the Centralized Logging with OpenSearch console. In the left navigation bar, under Log Analytics Pipelines , choose AWS Service Log or Application Log . Select the log pipeline created and choose View details . Select the Alarm tab. Switch off Alarms .","title":"Log alarms"},{"location":"implementation-guide/alarm/#enable-log-alarms","text":"Sign in to the Centralized Logging with OpenSearch console. In the left navigation bar, under Log Analytics Pipelines , choose AWS Service Log or Application Log . Select the log pipeline created and choose View details . Select the Alarm tab. Switch on Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , you need to provide email address for the newly-created SNS topic to notify.","title":"Enable log alarms"},{"location":"implementation-guide/alarm/#disable-log-alarms","text":"Sign in to the Centralized Logging with OpenSearch console. In the left navigation bar, under Log Analytics Pipelines , choose AWS Service Log or Application Log . Select the log pipeline created and choose View details . Select the Alarm tab. Switch off Alarms .","title":"Disable log alarms"},{"location":"implementation-guide/architecture/","text":"Deploying this solution with the default parameters builds the following environment in the AWS Cloud. Centralized Logging with OpenSearch architecture This solution deploys the AWS CloudFormation template in your AWS Cloud account and completes the following settings. Amazon CloudFront distributes the frontend web UI assets hosted in Amazon S3 bucket. Amazon Cognito user pool or OpenID Connector (OIDC) can be used for authentication. AWS AppSync provides the backend GraphQL APIs. Amazon DynamoDB stores the solution related information as backend database. AWS Lambda interacts with other AWS Services to process core logic of managing log pipelines or log agents, and obtains information updated in DynamoDB tables. AWS Step Functions orchestrates on-demand AWS CloudFormation deployment of a set of predefined stacks for log pipeline management. The log pipeline stacks deploy separate AWS resources and are used to collect and process logs and ingest them into Amazon OpenSearch Service for further analysis and visualization. Service Log Pipeline or Application Log Pipeline are provisioned on demand via Centralized Logging with OpenSearch console. AWS Systems Manager and Amazon EventBridge manage log agents for collecting logs from application servers, such as installing log agents (Fluent Bit) for application servers and monitoring the health status of the agents. Amazon EC2 or Amazon EKS installs Fluent Bit agents, and uploads log data to application log pipeline. Application log pipelines read, parse, process application logs and ingest them into Amazon OpenSearch domains or Light Engine. Service log pipelines read, parse, process AWS service logs and ingest them into Amazon OpenSearch domains or Light Engine. After deploying the solution, you can use AWS WAF to protect CloudFront or AppSync. Moreover, you can follow this guide to configure your WAF settings to prevent GraphQL schema introspection. This solution supports two types of log pipelines: Service Log Analytics Pipeline and Application Log Analytics Pipeline . Service log analytics pipeline Centralized Logging with OpenSearch supports log analysis for AWS services, such as Amazon S3 access logs, and Application Load Balancer access logs. For a complete list of supported AWS services, refer to Supported AWS Services . This solution ingests different AWS service logs using different workflows. Note Centralized Logging with OpenSearch supports cross-account log ingestion . If you want to ingest the logs from another AWS account, the resources in the Sources group in the architecture diagram will be in another account. Logs through Amazon S3 This section is applicable to Amazon S3 access logs, CloudFront standard logs, CloudTrail logs (S3), Application Load Balancing access logs, WAF logs, VPC Flow logs (S3), AWS Config logs, Amazon RDS/Aurora logs, and AWS Lambda Logs. The workflow supports two scenarios: Logs to Amazon S3 directly\uff08OpenSearch as log processor\uff09 In this scenario, the service directly sends logs to Amazon S3. Amazon S3 based service log pipeline architecture Logs to Amazon S3 via Kinesis Data Firehose\uff08OpenSearch as log processor\uff09 In this scenario, the service cannot directly put their logs to Amazon S3. The logs are sent to Amazon CloudWatch, and Kinesis Data Firehose ( KDF ) is used to subscribe the logs from CloudWatch Log Group and then put logs into Amazon S3. Amazon S3 (via KDF) based service log pipeline architecture The log pipeline runs the following workflow: AWS services logs are stored in Amazon S3 bucket (Log Bucket). An event notification is sent to Amazon SQS using S3 Event Notifications when a new log file is created. Amazon SQS initiates the Log Processor Lambda to run. The log processor reads and processes the log files. The log processor ingests the logs into the Amazon OpenSearch Service. Logs that fail to be processed are exported to Amazon S3 bucket (Backup Bucket). For cross-account ingestion, the AWS Services store logs in Amazon S3 bucket in the member account, and other resources remain in central logging account. Logs to Amazon S3 directly\uff08Light Engine as log processor\uff09 In this scenario, the service directly sends logs to Amazon S3. Amazon S3 based service log pipeline architecture The log pipeline runs the following workflow: AWS service logs are stored in an Amazon S3 bucket (Log Bucket). An event notification is sent to Amazon SQS using S3 Event Notifications when a new log file is created. Amazon SQS initiates AWS Lambda. AWS Lambda copies objects from the log bucket to the staging bucket. The Log Processor, AWS Step Functions, processes raw log files stored in the staging bucket in batches. It converts them into Apache Parquet format and automatically partitions all incoming data based on criteria including time and region. Logs through Amazon Kinesis Data Streams This section is applicable to CloudFront real-time logs, CloudTrail logs (CloudWatch), and VPC Flow logs (CloudWatch). The workflow supports two scenarios: Logs to KDS directly In this scenario, the service directly streams logs to Amazon Kinesis Data Streams ( KDS ). Amazon KDS based service log pipeline architecture Logs to KDS via subscription In this scenario, the service delivers the logs to CloudWatch Log Group, and then CloudWatch Logs stream the logs in real-time to KDS as the subscription destination. Amazon KDS (via subscription) based service log pipeline architecture The log pipeline runs the following workflow: AWS Services logs are streamed to Kinesis Data Stream. KDS initiates the Log Processor Lambda to run. The log processor processes and ingests the logs into the Amazon OpenSearch Service. Logs that fail to be processed are exported to Amazon S3 bucket (Backup Bucket). For cross-account ingestion, the AWS Services store logs on Amazon CloudWatch log group in the member account, and other resources remain in central logging account. Warning This solution does not support cross-account ingestion for CloudFront real-time logs. Application log analytics pipeline Centralized Logging with OpenSearch supports log analysis for application logs, such as Nginx/Apache HTTP Server logs or custom application logs. Note Centralized Logging with OpenSearch supports cross-account log ingestion . If you want to ingest logs from the same account, the resources in the Sources group will be in the same account as your Centralized Logging with OpenSearch account. Otherwise, they will be in another AWS account. Logs from Amazon EC2 / Amazon EKS Logs from Amazon EC2/ Amazon EKS(OpenSearch as log processor) Application log pipeline architecture for EC2/EKS The log pipeline runs the following workflow: Fluent Bit works as the underlying log agent to collect logs from application servers and send them to an optional Log Buffer , or ingest into OpenSearch domain directly. The Log Buffer triggers the Lambda (Log Processor) to run. The log processor reads and processes the log records and ingests the logs into the OpenSearch domain. Logs that fail to be processed are exported to an Amazon S3 bucket (Backup Bucket). Logs from Amazon EC2/ Amazon EKS(Light Engine as log processor) Application log pipeline architecture for EC2/EKS The log pipeline runs the following workflow: Fluent Bit works as the underlying log agent to collect logs from application servers and send them to an optional Log Buffer. The Log Buffer triggers the Lambda to copy objects from log bucket to staging bucket. Log Processor, AWS Step Functions, processes raw log files stored in the staging bucket in batches, converts them to Apache Parquet, and automatically partitions all incoming data by criteria including time and region. Logs from Syslog Client Important Make sure your Syslog generator/sender's subnet is connected to Centralized Logging with OpenSearch' two private subnets. You need to use VPC Peering Connection or Transit Gateway to connect these VPCs. The NLB together with the ECS containers in the architecture diagram will be provisioned only when you create a Syslog ingestion and be automated deleted when there is no Syslog ingestion. Application log pipeline architecture for Syslog Syslog client (like Rsyslog ) send logs to a Network Load Balancer (NLB) in Centralized Logging with OpenSearch's private subnets, and NLB routes to the ECS containers running Syslog servers. Fluent Bit works as the underlying log agent in the ECS Service to parse logs, and send them to an optional Log Buffer , or ingest into OpenSearch domain directly. The Log Buffer triggers the Lambda (Log Processor) to run. The log processor reads and processes the log records and ingests the logs into the OpenSearch domain. Logs that fail to be processed are exported to an Amazon S3 bucket (Backup Bucket).","title":"Architecture diagram"},{"location":"implementation-guide/architecture/#service-log-analytics-pipeline","text":"Centralized Logging with OpenSearch supports log analysis for AWS services, such as Amazon S3 access logs, and Application Load Balancer access logs. For a complete list of supported AWS services, refer to Supported AWS Services . This solution ingests different AWS service logs using different workflows. Note Centralized Logging with OpenSearch supports cross-account log ingestion . If you want to ingest the logs from another AWS account, the resources in the Sources group in the architecture diagram will be in another account.","title":"Service log analytics pipeline"},{"location":"implementation-guide/architecture/#logs-through-amazon-s3","text":"This section is applicable to Amazon S3 access logs, CloudFront standard logs, CloudTrail logs (S3), Application Load Balancing access logs, WAF logs, VPC Flow logs (S3), AWS Config logs, Amazon RDS/Aurora logs, and AWS Lambda Logs. The workflow supports two scenarios: Logs to Amazon S3 directly\uff08OpenSearch as log processor\uff09 In this scenario, the service directly sends logs to Amazon S3. Amazon S3 based service log pipeline architecture Logs to Amazon S3 via Kinesis Data Firehose\uff08OpenSearch as log processor\uff09 In this scenario, the service cannot directly put their logs to Amazon S3. The logs are sent to Amazon CloudWatch, and Kinesis Data Firehose ( KDF ) is used to subscribe the logs from CloudWatch Log Group and then put logs into Amazon S3. Amazon S3 (via KDF) based service log pipeline architecture The log pipeline runs the following workflow: AWS services logs are stored in Amazon S3 bucket (Log Bucket). An event notification is sent to Amazon SQS using S3 Event Notifications when a new log file is created. Amazon SQS initiates the Log Processor Lambda to run. The log processor reads and processes the log files. The log processor ingests the logs into the Amazon OpenSearch Service. Logs that fail to be processed are exported to Amazon S3 bucket (Backup Bucket). For cross-account ingestion, the AWS Services store logs in Amazon S3 bucket in the member account, and other resources remain in central logging account. Logs to Amazon S3 directly\uff08Light Engine as log processor\uff09 In this scenario, the service directly sends logs to Amazon S3. Amazon S3 based service log pipeline architecture The log pipeline runs the following workflow: AWS service logs are stored in an Amazon S3 bucket (Log Bucket). An event notification is sent to Amazon SQS using S3 Event Notifications when a new log file is created. Amazon SQS initiates AWS Lambda. AWS Lambda copies objects from the log bucket to the staging bucket. The Log Processor, AWS Step Functions, processes raw log files stored in the staging bucket in batches. It converts them into Apache Parquet format and automatically partitions all incoming data based on criteria including time and region.","title":"Logs through Amazon S3"},{"location":"implementation-guide/architecture/#logs-through-amazon-kinesis-data-streams","text":"This section is applicable to CloudFront real-time logs, CloudTrail logs (CloudWatch), and VPC Flow logs (CloudWatch). The workflow supports two scenarios: Logs to KDS directly In this scenario, the service directly streams logs to Amazon Kinesis Data Streams ( KDS ). Amazon KDS based service log pipeline architecture Logs to KDS via subscription In this scenario, the service delivers the logs to CloudWatch Log Group, and then CloudWatch Logs stream the logs in real-time to KDS as the subscription destination. Amazon KDS (via subscription) based service log pipeline architecture The log pipeline runs the following workflow: AWS Services logs are streamed to Kinesis Data Stream. KDS initiates the Log Processor Lambda to run. The log processor processes and ingests the logs into the Amazon OpenSearch Service. Logs that fail to be processed are exported to Amazon S3 bucket (Backup Bucket). For cross-account ingestion, the AWS Services store logs on Amazon CloudWatch log group in the member account, and other resources remain in central logging account. Warning This solution does not support cross-account ingestion for CloudFront real-time logs.","title":"Logs through Amazon Kinesis Data Streams"},{"location":"implementation-guide/architecture/#application-log-analytics-pipeline","text":"Centralized Logging with OpenSearch supports log analysis for application logs, such as Nginx/Apache HTTP Server logs or custom application logs. Note Centralized Logging with OpenSearch supports cross-account log ingestion . If you want to ingest logs from the same account, the resources in the Sources group will be in the same account as your Centralized Logging with OpenSearch account. Otherwise, they will be in another AWS account.","title":"Application log analytics pipeline"},{"location":"implementation-guide/architecture/#logs-from-amazon-ec2-amazon-eks","text":"Logs from Amazon EC2/ Amazon EKS(OpenSearch as log processor) Application log pipeline architecture for EC2/EKS The log pipeline runs the following workflow: Fluent Bit works as the underlying log agent to collect logs from application servers and send them to an optional Log Buffer , or ingest into OpenSearch domain directly. The Log Buffer triggers the Lambda (Log Processor) to run. The log processor reads and processes the log records and ingests the logs into the OpenSearch domain. Logs that fail to be processed are exported to an Amazon S3 bucket (Backup Bucket). Logs from Amazon EC2/ Amazon EKS(Light Engine as log processor) Application log pipeline architecture for EC2/EKS The log pipeline runs the following workflow: Fluent Bit works as the underlying log agent to collect logs from application servers and send them to an optional Log Buffer. The Log Buffer triggers the Lambda to copy objects from log bucket to staging bucket. Log Processor, AWS Step Functions, processes raw log files stored in the staging bucket in batches, converts them to Apache Parquet, and automatically partitions all incoming data by criteria including time and region.","title":"Logs from Amazon EC2 / Amazon EKS"},{"location":"implementation-guide/architecture/#logs-from-syslog-client","text":"Important Make sure your Syslog generator/sender's subnet is connected to Centralized Logging with OpenSearch' two private subnets. You need to use VPC Peering Connection or Transit Gateway to connect these VPCs. The NLB together with the ECS containers in the architecture diagram will be provisioned only when you create a Syslog ingestion and be automated deleted when there is no Syslog ingestion. Application log pipeline architecture for Syslog Syslog client (like Rsyslog ) send logs to a Network Load Balancer (NLB) in Centralized Logging with OpenSearch's private subnets, and NLB routes to the ECS containers running Syslog servers. Fluent Bit works as the underlying log agent in the ECS Service to parse logs, and send them to an optional Log Buffer , or ingest into OpenSearch domain directly. The Log Buffer triggers the Lambda (Log Processor) to run. The log processor reads and processes the log records and ingests the logs into the OpenSearch domain. Logs that fail to be processed are exported to an Amazon S3 bucket (Backup Bucket).","title":"Logs from Syslog Client"},{"location":"implementation-guide/faq/","text":"Frequently Asked Questions General Q: What is Centralized Logging with OpenSearch solution? Centralized Logging with OpenSearch is an AWS Solution that simplifies the building of log analytics pipelines. It provides to customers, as complementary of Amazon OpenSearch Service, capabilities to ingest and process both application logs and AWS service logs without writing code, and create visualization dashboards from out-of-the-box templates. Centralized Logging with OpenSearch automatically assembles the underlying AWS services, and provides you a web console to manage log analytics pipelines. Q: What are the supported logs in this solution? Centralized Logging with OpenSearch supports both AWS service logs and EC2/EKS application logs. Refer to the supported AWS services , and the supported application log formats and sources for more details. Q: Does Centralized Logging with OpenSearch support ingesting logs from multiple AWS accounts? Yes. Centralized Logging with OpenSearch supports ingesting AWS service logs and application logs from a different AWS account in the same region. For more information, see cross-account ingestion . Q: Does Centralized Logging with OpenSearch support ingesting logs from multiple AWS Regions? Currently, Centralized Logging with OpenSearch does not automate the log ingestion from a different AWS Region. You need to ingest logs from other regions into pipelines provisioned by Centralized Logging with OpenSearch. For AWS services which store the logs in S3 bucket, you can leverage the S3 Cross-Region Replication to copy the logs to the Centralized Logging with OpenSearch deployed region, and import incremental logs using the manual mode by specifying the log location in the S3 bucket. For application logs on EC2 and EKS, you need to set up the networking (for example, Kinesis VPC endpoint, VPC Peering), install agents, and configure the agents to ingest logs to Centralized Logging with OpenSearch pipelines. Q: What is the license of this solution? This solution is provided under the Apache-2.0 license . It is a permissive free software license written by the Apache Software Foundation. It allows users to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software under the terms of the license, without concern for royalties. Q: How can I find the roadmap of this solution? This solution uses GitHub project to manage the roadmap. You can find the roadmap here . Q: How can I submit a feature request or bug report? You can submit feature requests and bug report through the GitHub issues. Here are the templates for feature request , bug report . Q: How can I use stronger TLS Protocols to secure traffic, namely TLS 1.2 and above? By default, CloudFront uses the TLSv1 security policy along with a default certificate. Changing the TLS settings for CloudFront depends on the presence of your SSL certificates. If you don't have your own SSL certificates, you won't be able to alter the TLS setting for CloudFront. In order to configure TLS 1.2 or above, you will need a custom domain. This setup will enable you to enforce stronger TLS protocols for your traffic. To learn how to configure a custom domain and enable TLS 1.2+ for your service, you can follow the guide provided here: Use a Custom Domain with AWS AppSync, Amazon CloudFront, and Amazon Route 53 . Setup and configuration Q: Can I deploy Centralized Logging with OpenSearch on AWS in any AWS Region? Centralized Logging with OpenSearch provides two deployment options: option 1 with Cognito User Pool, and option 2 with OpenID Connect. For option 1, customers can deploy the solution in AWS Regions where Amazon Cognito User Pool, AWS AppSync, Amazon Kinesis Data Firehose (optional) are available. For option 2, customers can deploy the solution in AWS Regions where AWS AppSync, Amazon Kinesis Data Firehose (optional) are available. Refer to supported regions for deployment for more information. Q: What are the prerequisites of deploying this solution? Centralized Logging with OpenSearch does not provision Amazon OpenSearch clusters, and you need to import existing OpenSearch clusters through the web console. The clusters must meet the requirements specified in prerequisites . Q: Why do I need a domain name with ICP recordal when deploying the solution in AWS China Regions? The Centralized Logging with OpenSearch console is served via CloudFront distribution which is considered as an Internet information service. According to the local regulations, any Internet information service must bind to a domain name with ICP recordal . Q: What versions of OpenSearch does the solution work with? Centralized Logging with OpenSearch supports Amazon OpenSearch Service, with engine version Elasticsearch 7.10 and later, Amazon OpenSearch 1.0 and later. Q: What are the index name rules for OpenSearch created by the Log Analytics Pipeline? You can change the index name if needed when using the Centralized Logging with OpenSearch console to create a log analytics pipeline. If the log analytics pipeline is created for service logs, the index name is composed of <Index Prefix> - <service-type> - <Index Suffix> -<00000x>, where you can define a name for Index Prefix and service-type is automatically generated by the solution according to the service type you have chosen. Moreover, you can choose different index suffix types to adjust index rollover time window. YYYY-MM-DD-HH: Amazon OpenSearch will roll the index by hour. YYYY-MM-DD: Amazon OpenSearch will roll the index by 24 hours. YYYY-MM: Amazon OpenSearch will roll the index by 30 days. YYYY: Amazon OpenSearch will roll the index by 365 days. It should be noted that in OpenSearch, the time is in UTC 0 time zone. Regarding the 00000x part, Amazon OpenSearch will automatically append a 6-digit suffix to the index name, where the first index rule is 000001, rollover according to the index, and increment backwards, such as 000002, 000003. If the log analytics pipeline is created for application log, the index name is composed of <Index Prefix> - <Index Suffix> -<00000x>. The rules for index prefix and index suffix, 00000x are the same as those for service logs. Q: What are the index rollover rules for OpenSearch created by the Log Analytics Pipeline? Index rollover is determined by two factors. One is the Index Suffix in the index name. If you enable the index rollover by capacity, Amazon OpenSearch will roll your index when the index capacity equals or exceeds the specified size, regardless of the rollover time window. Note that if one of these two factors matches, index rollover can be triggered. For example, we created an application log pipeline on January 1, 2023, deleted the application log pipeline at 9:00 on January 4, 2023, and the index name is nginx-YYYY-MM-DD-<00000x>. At the same time, we enabled the index rollover by capacity and entered 300GB. If the log data volume increases suddenly after creation, it can reach 300GB every hour, and the duration is 2 hours and 10 minutes. After that, it returns to normal, and the daily data volume is 90GB. Then OpenSearch creates three indexes on January 1, the index names are nginx-2023-01-01-000001, nginx-2023-01-01-000002, nginx-2023-01-01-000003, and then creates one every day Indexes respectively: nginx-2023-01-02-000004, nginx-2023-01-03-000005, nginx-2023-01-04-000006. Q: Can I deploy the solution in an existing VPC? Yes. You can either launch the solution with a new VPC or launch the solution with an existing VPC. When using an existing VPC, you need to select the VPC and the corresponding subnets. Refer to launch with Cognito User Pool or launch with OpenID Connect for more details. Q: I did not receive the email containing the temporary password when launching the solution with Cognito User Pool. How can I resend the password? Your account is managed by the Cognito User Pool. To resend the temporary password, you can find the user pool created by the solution, delete and recreate the user using the same email address. If you still have the same issue, try with another email address. Q: How can I create more users for this solution? If you launched the solution with Cognito User Pool, go to the AWS console, find the user pool created by the solution, and you can create more users. If you launched the solution with OpenID Connect (OIDC), you should add more users in the user pool managed by the OIDC provider. Note that all users have the same privileges. Pricing Q: How will I be charged and billed for the use of this solution? The solution is free to use, and you are responsible for the cost of AWS services used while running this solution. You pay only for what you use, and there are no minimum or setup fees. Refer to the Centralized Logging with OpenSearch Cost section for detailed cost estimation. Q: Will there be additional cost for cross-account ingestion? No. The cost will be same as ingesting logs within the same AWS account. Log Ingestion Q: What is the log agent used in the Centralized Logging with OpenSearch solution? Centralized Logging with OpenSearch uses AWS for Fluent Bit , a distribution of Fluent Bit maintained by AWS. The solution uses this distribution to ingest logs from Amazon EC2 and Amazon EKS. Q: I have already stored the AWS service logs of member accounts in a centralized logging account. How should I create service log ingestion for member accounts? In this case, you need to deploy the Centralized Logging with OpenSearch solution in the centralized logging account, and ingest AWS service logs using the Manual mode from the logging account. Refer to this guide for ingesting Application Load Balancer logs with Manual mode. You can do the same with other supported AWS services which output logs to S3. Q: Why there are some duplicated records in OpenSearch when ingesting logs via Kinesis Data Streams? This is usually because there is no enough Kinesis Shards to handle the incoming requests. When threshold error occurs in Kinesis, the Fluent Bit agent will retry that chunk . To avoid this issue, you need to estimate your log throughput and set a proper Kinesis shard number. Please refer to the Kinesis Data Streams quotas and limits . Centralized Logging with OpenSearch provides a built-in feature to scale-out and scale-in the Kinesis shards, and it would take a couple of minutes to scale out to the desired number. Q: How to install log agent on CentOS 7? Log in to your CentOS 7 machine and install SSM Agent manually. sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm sudo systemctl enable amazon-ssm-agent sudo systemctl start amazon-ssm-agent Go to the Instance Group panel of Centralized Logging with OpenSearch console, create Instance Group , select the CentOS 7 machine, choose Install log agent and wait for its status to be offline . Log in to CentOS 7 and install fluent-bit 1.9.3 manually. export RELEASE_URL = ${ FLUENT_BIT_PACKAGES_URL :- https ://packages.fluentbit.io } export RELEASE_KEY = ${ FLUENT_BIT_PACKAGES_KEY :- https ://packages.fluentbit.io/fluentbit.key } sudo rpm --import $RELEASE_KEY cat << EOF | sudo tee /etc/yum.repos.d/fluent-bit.repo [fluent-bit] name = Fluent Bit baseurl = $RELEASE_URL/centos/VERSION_ARCH_SUBSTR gpgcheck=1 repo_gpgcheck=1 gpgkey=$RELEASE_KEY enabled=1 EOF sudo sed -i 's|VERSION_ARCH_SUBSTR|\\$releasever/\\$basearch/|g' /etc/yum.repos.d/fluent-bit.repo sudo yum install -y fluent-bit-1.9.3-1 # Modify the configuration file sudo sed -i 's/ExecStart.*/ExecStart=\\/opt\\/fluent-bit\\/bin\\/fluent-bit -c \\/opt\\/fluent-bit\\/etc\\/fluent-bit.conf/g' /usr/lib/systemd/system/fluent-bit.service sudo systemctl daemon-reload sudo systemctl enable fluent-bit sudo systemctl start fluent-bit 4. Go back to the Instance Groups panel of the Centralized Logging with OpenSearch console and wait for the CentOS 7 machine status to be Online and proceed to create the instance group. Q: How can I consume CloudWatch custom logs? You can use Firehose to subscribe CloudWatch logs and transfer logs into Amazon S3. Firstly, create subscription filters with Amazon Kinesis Data Firehose based on this guide . Next, follow the instructions to learn how to transfer logs to Amazon S3. Then, you can use Centralized Logging with OpenSearch to ingest logs from Amazon S3 to OpenSearch. Log Visualization Q: How can I find the built-in dashboards in OpenSearch? Please refer to the AWS Service Logs and Application Logs to find out if there is a built-in dashboard supported. You also need to turn on the Sample Dashboard option when creating a log analytics pipeline. The dashboard will be inserted into the Amazon OpenSearch Service under Global Tenant . You can switch to the Global Tenant from the top right coder of the OpenSearch Dashboards.","title":"FAQ"},{"location":"implementation-guide/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"implementation-guide/faq/#general","text":"Q: What is Centralized Logging with OpenSearch solution? Centralized Logging with OpenSearch is an AWS Solution that simplifies the building of log analytics pipelines. It provides to customers, as complementary of Amazon OpenSearch Service, capabilities to ingest and process both application logs and AWS service logs without writing code, and create visualization dashboards from out-of-the-box templates. Centralized Logging with OpenSearch automatically assembles the underlying AWS services, and provides you a web console to manage log analytics pipelines. Q: What are the supported logs in this solution? Centralized Logging with OpenSearch supports both AWS service logs and EC2/EKS application logs. Refer to the supported AWS services , and the supported application log formats and sources for more details. Q: Does Centralized Logging with OpenSearch support ingesting logs from multiple AWS accounts? Yes. Centralized Logging with OpenSearch supports ingesting AWS service logs and application logs from a different AWS account in the same region. For more information, see cross-account ingestion . Q: Does Centralized Logging with OpenSearch support ingesting logs from multiple AWS Regions? Currently, Centralized Logging with OpenSearch does not automate the log ingestion from a different AWS Region. You need to ingest logs from other regions into pipelines provisioned by Centralized Logging with OpenSearch. For AWS services which store the logs in S3 bucket, you can leverage the S3 Cross-Region Replication to copy the logs to the Centralized Logging with OpenSearch deployed region, and import incremental logs using the manual mode by specifying the log location in the S3 bucket. For application logs on EC2 and EKS, you need to set up the networking (for example, Kinesis VPC endpoint, VPC Peering), install agents, and configure the agents to ingest logs to Centralized Logging with OpenSearch pipelines. Q: What is the license of this solution? This solution is provided under the Apache-2.0 license . It is a permissive free software license written by the Apache Software Foundation. It allows users to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software under the terms of the license, without concern for royalties. Q: How can I find the roadmap of this solution? This solution uses GitHub project to manage the roadmap. You can find the roadmap here . Q: How can I submit a feature request or bug report? You can submit feature requests and bug report through the GitHub issues. Here are the templates for feature request , bug report . Q: How can I use stronger TLS Protocols to secure traffic, namely TLS 1.2 and above? By default, CloudFront uses the TLSv1 security policy along with a default certificate. Changing the TLS settings for CloudFront depends on the presence of your SSL certificates. If you don't have your own SSL certificates, you won't be able to alter the TLS setting for CloudFront. In order to configure TLS 1.2 or above, you will need a custom domain. This setup will enable you to enforce stronger TLS protocols for your traffic. To learn how to configure a custom domain and enable TLS 1.2+ for your service, you can follow the guide provided here: Use a Custom Domain with AWS AppSync, Amazon CloudFront, and Amazon Route 53 .","title":"General"},{"location":"implementation-guide/faq/#setup-and-configuration","text":"Q: Can I deploy Centralized Logging with OpenSearch on AWS in any AWS Region? Centralized Logging with OpenSearch provides two deployment options: option 1 with Cognito User Pool, and option 2 with OpenID Connect. For option 1, customers can deploy the solution in AWS Regions where Amazon Cognito User Pool, AWS AppSync, Amazon Kinesis Data Firehose (optional) are available. For option 2, customers can deploy the solution in AWS Regions where AWS AppSync, Amazon Kinesis Data Firehose (optional) are available. Refer to supported regions for deployment for more information. Q: What are the prerequisites of deploying this solution? Centralized Logging with OpenSearch does not provision Amazon OpenSearch clusters, and you need to import existing OpenSearch clusters through the web console. The clusters must meet the requirements specified in prerequisites . Q: Why do I need a domain name with ICP recordal when deploying the solution in AWS China Regions? The Centralized Logging with OpenSearch console is served via CloudFront distribution which is considered as an Internet information service. According to the local regulations, any Internet information service must bind to a domain name with ICP recordal . Q: What versions of OpenSearch does the solution work with? Centralized Logging with OpenSearch supports Amazon OpenSearch Service, with engine version Elasticsearch 7.10 and later, Amazon OpenSearch 1.0 and later. Q: What are the index name rules for OpenSearch created by the Log Analytics Pipeline? You can change the index name if needed when using the Centralized Logging with OpenSearch console to create a log analytics pipeline. If the log analytics pipeline is created for service logs, the index name is composed of <Index Prefix> - <service-type> - <Index Suffix> -<00000x>, where you can define a name for Index Prefix and service-type is automatically generated by the solution according to the service type you have chosen. Moreover, you can choose different index suffix types to adjust index rollover time window. YYYY-MM-DD-HH: Amazon OpenSearch will roll the index by hour. YYYY-MM-DD: Amazon OpenSearch will roll the index by 24 hours. YYYY-MM: Amazon OpenSearch will roll the index by 30 days. YYYY: Amazon OpenSearch will roll the index by 365 days. It should be noted that in OpenSearch, the time is in UTC 0 time zone. Regarding the 00000x part, Amazon OpenSearch will automatically append a 6-digit suffix to the index name, where the first index rule is 000001, rollover according to the index, and increment backwards, such as 000002, 000003. If the log analytics pipeline is created for application log, the index name is composed of <Index Prefix> - <Index Suffix> -<00000x>. The rules for index prefix and index suffix, 00000x are the same as those for service logs. Q: What are the index rollover rules for OpenSearch created by the Log Analytics Pipeline? Index rollover is determined by two factors. One is the Index Suffix in the index name. If you enable the index rollover by capacity, Amazon OpenSearch will roll your index when the index capacity equals or exceeds the specified size, regardless of the rollover time window. Note that if one of these two factors matches, index rollover can be triggered. For example, we created an application log pipeline on January 1, 2023, deleted the application log pipeline at 9:00 on January 4, 2023, and the index name is nginx-YYYY-MM-DD-<00000x>. At the same time, we enabled the index rollover by capacity and entered 300GB. If the log data volume increases suddenly after creation, it can reach 300GB every hour, and the duration is 2 hours and 10 minutes. After that, it returns to normal, and the daily data volume is 90GB. Then OpenSearch creates three indexes on January 1, the index names are nginx-2023-01-01-000001, nginx-2023-01-01-000002, nginx-2023-01-01-000003, and then creates one every day Indexes respectively: nginx-2023-01-02-000004, nginx-2023-01-03-000005, nginx-2023-01-04-000006. Q: Can I deploy the solution in an existing VPC? Yes. You can either launch the solution with a new VPC or launch the solution with an existing VPC. When using an existing VPC, you need to select the VPC and the corresponding subnets. Refer to launch with Cognito User Pool or launch with OpenID Connect for more details. Q: I did not receive the email containing the temporary password when launching the solution with Cognito User Pool. How can I resend the password? Your account is managed by the Cognito User Pool. To resend the temporary password, you can find the user pool created by the solution, delete and recreate the user using the same email address. If you still have the same issue, try with another email address. Q: How can I create more users for this solution? If you launched the solution with Cognito User Pool, go to the AWS console, find the user pool created by the solution, and you can create more users. If you launched the solution with OpenID Connect (OIDC), you should add more users in the user pool managed by the OIDC provider. Note that all users have the same privileges.","title":"Setup and configuration"},{"location":"implementation-guide/faq/#pricing","text":"Q: How will I be charged and billed for the use of this solution? The solution is free to use, and you are responsible for the cost of AWS services used while running this solution. You pay only for what you use, and there are no minimum or setup fees. Refer to the Centralized Logging with OpenSearch Cost section for detailed cost estimation. Q: Will there be additional cost for cross-account ingestion? No. The cost will be same as ingesting logs within the same AWS account.","title":"Pricing"},{"location":"implementation-guide/faq/#log-ingestion","text":"Q: What is the log agent used in the Centralized Logging with OpenSearch solution? Centralized Logging with OpenSearch uses AWS for Fluent Bit , a distribution of Fluent Bit maintained by AWS. The solution uses this distribution to ingest logs from Amazon EC2 and Amazon EKS. Q: I have already stored the AWS service logs of member accounts in a centralized logging account. How should I create service log ingestion for member accounts? In this case, you need to deploy the Centralized Logging with OpenSearch solution in the centralized logging account, and ingest AWS service logs using the Manual mode from the logging account. Refer to this guide for ingesting Application Load Balancer logs with Manual mode. You can do the same with other supported AWS services which output logs to S3. Q: Why there are some duplicated records in OpenSearch when ingesting logs via Kinesis Data Streams? This is usually because there is no enough Kinesis Shards to handle the incoming requests. When threshold error occurs in Kinesis, the Fluent Bit agent will retry that chunk . To avoid this issue, you need to estimate your log throughput and set a proper Kinesis shard number. Please refer to the Kinesis Data Streams quotas and limits . Centralized Logging with OpenSearch provides a built-in feature to scale-out and scale-in the Kinesis shards, and it would take a couple of minutes to scale out to the desired number. Q: How to install log agent on CentOS 7? Log in to your CentOS 7 machine and install SSM Agent manually. sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm sudo systemctl enable amazon-ssm-agent sudo systemctl start amazon-ssm-agent Go to the Instance Group panel of Centralized Logging with OpenSearch console, create Instance Group , select the CentOS 7 machine, choose Install log agent and wait for its status to be offline . Log in to CentOS 7 and install fluent-bit 1.9.3 manually. export RELEASE_URL = ${ FLUENT_BIT_PACKAGES_URL :- https ://packages.fluentbit.io } export RELEASE_KEY = ${ FLUENT_BIT_PACKAGES_KEY :- https ://packages.fluentbit.io/fluentbit.key } sudo rpm --import $RELEASE_KEY cat << EOF | sudo tee /etc/yum.repos.d/fluent-bit.repo [fluent-bit] name = Fluent Bit baseurl = $RELEASE_URL/centos/VERSION_ARCH_SUBSTR gpgcheck=1 repo_gpgcheck=1 gpgkey=$RELEASE_KEY enabled=1 EOF sudo sed -i 's|VERSION_ARCH_SUBSTR|\\$releasever/\\$basearch/|g' /etc/yum.repos.d/fluent-bit.repo sudo yum install -y fluent-bit-1.9.3-1 # Modify the configuration file sudo sed -i 's/ExecStart.*/ExecStart=\\/opt\\/fluent-bit\\/bin\\/fluent-bit -c \\/opt\\/fluent-bit\\/etc\\/fluent-bit.conf/g' /usr/lib/systemd/system/fluent-bit.service sudo systemctl daemon-reload sudo systemctl enable fluent-bit sudo systemctl start fluent-bit 4. Go back to the Instance Groups panel of the Centralized Logging with OpenSearch console and wait for the CentOS 7 machine status to be Online and proceed to create the instance group. Q: How can I consume CloudWatch custom logs? You can use Firehose to subscribe CloudWatch logs and transfer logs into Amazon S3. Firstly, create subscription filters with Amazon Kinesis Data Firehose based on this guide . Next, follow the instructions to learn how to transfer logs to Amazon S3. Then, you can use Centralized Logging with OpenSearch to ingest logs from Amazon S3 to OpenSearch.","title":"Log Ingestion"},{"location":"implementation-guide/faq/#log-visualization","text":"Q: How can I find the built-in dashboards in OpenSearch? Please refer to the AWS Service Logs and Application Logs to find out if there is a built-in dashboard supported. You also need to turn on the Sample Dashboard option when creating a log analytics pipeline. The dashboard will be inserted into the Amazon OpenSearch Service under Global Tenant . You can switch to the Global Tenant from the top right coder of the OpenSearch Dashboards.","title":"Log Visualization"},{"location":"implementation-guide/include-dashboard/","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Include dashboard"},{"location":"implementation-guide/monitoring/","text":"Types of metrics The following types of metrics are available on the Centralized Logging with OpenSearch console. Log source metrics Fluent Bit FluentBitOutputProcRecords - The number of log records that this output instance has successfully sent. This is the total record count of all unique chunks sent by this output. If a record is not successfully sent, it does not count towards this metric. FluentBitOutputProcBytes - The number of bytes of log records that this output instance has successfully sent. This is the total byte size of all unique chunks sent by this output. If a record is not sent due to some error, then it will not count towards this metric. FluentBitOutputDroppedRecords - The number of log records that have been dropped by the output. This means they met an unrecoverable error or retries expired for their chunk. FluentBitOutputErrors - The number of chunks that have faced an error (either unrecoverable or retrievable). This is the number of times a chunk has failed, and does not correspond with the number of error messages you see in the Fluent Bit log output. FluentBitOutputRetriedRecords - The number of log records that experienced a retry. Note that this is calculated at the chunk level, and the count increased when an entire chunk is marked for retry. An output plugin may or may not perform multiple actions that generate many error messages when uploading a single chunk. FluentBitOutputRetriesFailed - The number of times that retries expired for a chunk. Each plugin configures a Retry_Limit which applies to chunks. Once the Retry_Limit has been reached for a chunk, it is discarded and this metric is incremented. FluentBitOutputRetries - The number of times this output instance requested a retry for a chunk. Network Load Balancer SyslogNLBActiveFlowCount - The total number of concurrent flows (or connections) from clients to targets. This metric includes connections in the SYN_SENT and ESTABLISHED states. TCP connections are not terminated at the load balancer, so a client opening a TCP connection to a target counts as a single flow. SyslogNLBProcessedBytes - The total number of bytes processed by the load balancer, including TCP/IP headers. This count includes traffic to and from targets, minus health check traffic. Buffer metrics Log Buffer is a buffer layer between the Log Agent and OpenSearch clusters. The agent uploads logs into the buffer layer before being processed and delivered into the OpenSearch clusters. A buffer layer is a way to protect OpenSearch clusters from overwhelming. Kinesis Data Stream KDSIncomingBytes \u2013 The number of bytes successfully put to the Kinesis stream over the specified time period. This metric includes bytes from PutRecord and PutRecords operations. Minimum, Maximum, and Average statistics represent the bytes in a single put operation for the stream in the specified time period. KDSIncomingRecords \u2013 The number of records successfully put to the Kinesis stream over the specified time period. This metric includes record counts from PutRecord and PutRecords operations. Minimum, Maximum, and Average statistics represent the records in a single put operation for the stream in the specified time period. KDSPutRecordBytes \u2013 The number of bytes put to the Kinesis stream using the PutRecord operation over the specified time period. KDSThrottledRecords \u2013 The number of records rejected due to throttling in a PutRecords operation per Kinesis data stream, measured over the specified time period. KDSWriteProvisionedThroughputExceeded \u2013 The number of records rejected due to throttling for the stream over the specified time period. This metric includes throttling from PutRecord and PutRecords operations. The most commonly used statistic for this metric is Average. When the Minimum statistic has a non-zero value, records will be throttled for the stream during the specified time period. When the Maximum statistic has a value of 0 (zero), no records will be throttled for the stream during the specified time period. SQS SQSNumberOfMessagesSent - The number of messages added to a queue. SQSNumberOfMessagesDeleted - The number of messages deleted from the queue. Amazon SQS emits the NumberOfMessagesDeleted metric for every successful deletion operation that uses a valid receipt handle, including duplicate deletions. The following scenarios might cause the value of the NumberOfMessagesDeleted metric to be higher than expected: - Calling the DeleteMessage action on different receipt handles that belong to the same message: If the message is not processed before the visibility timeout expires, the message becomes available to other consumers that can process it and delete it again, increasing the value of the NumberOfMessagesDeleted metric. Calling the DeleteMessage action on the same receipt handle: If the message is processed and deleted, but you call the DeleteMessage action again using the same receipt handle, a success status is returned, increasing the value of the NumberOfMessagesDeleted metric. SQSApproximateNumberOfMessagesVisible - The number of messages available for retrieval from the queue. SQSApproximateAgeOfOldestMessage - The approximate age of the oldest non-deleted message in the queue. After a message is received three times (or more) and not processed, the message is moved to the back of the queue and the ApproximateAgeOfOldestMessage metric points at the second-oldest message that hasn't been received more than three times. This action occurs even if the queue has a redrive policy. Because a single poison-pill message (received multiple times but never deleted) can distort this metric, the age of a poison-pill message isn't included in the metric until the poison-pill message is consumed successfully. When the queue has a redrive policy, the message is moved to a dead-letter queue after the configured Maximum Receives . When the message is moved to the dead-letter queue, the ApproximateAgeOfOldestMessage metric of the dead-letter queue represents the time when the message was moved to the dead-letter queue (not the original time the message was sent). Log processor metrics The Log Processor Lambda is responsible for performing final processing on the data and bulk writing it to OpenSearch. TotalLogs \u2013 The total number of log records or events processed by the Lambda function. ExcludedLogs \u2013 The number of log records or events that were excluded from processing, which could be due to filtering or other criteria. LoadedLogs \u2013 The number of log records or events that were successfully processed and loaded into OpenSearch. FailedLogs \u2013 The number of log records or events that failed to be processed or loaded into OpenSearch. ConcurrentExecutions \u2013 The number of function instances that are processing events. If this number reaches your concurrent executions quota for the Region, or the reserved concurrency limit on the function, then Lambda throttles additional invocation requests. Duration \u2013 The amount of time that your function code spends processing an event. The billed duration for an invocation is the value of Duration rounded up to the nearest millisecond. Throttles \u2013 The number of invocation requests that are throttled. When all function instances are processing requests and no concurrency is available to scale up, Lambda rejects additional requests with a TooManyRequestsException error. Throttled requests and other invocation errors don't count as either Invocations or Errors. Invocations \u2013 The number of times that your function code is invoked, including successful invocations and invocations that result in a function error. Invocations aren't recorded if the invocation request is throttled or otherwise results in an invocation error. The value of Invocations equals the number of requests billed.","title":"Monitoring"},{"location":"implementation-guide/monitoring/#types-of-metrics","text":"The following types of metrics are available on the Centralized Logging with OpenSearch console.","title":"Types of metrics"},{"location":"implementation-guide/monitoring/#log-source-metrics","text":"","title":"Log source metrics"},{"location":"implementation-guide/monitoring/#fluent-bit","text":"FluentBitOutputProcRecords - The number of log records that this output instance has successfully sent. This is the total record count of all unique chunks sent by this output. If a record is not successfully sent, it does not count towards this metric. FluentBitOutputProcBytes - The number of bytes of log records that this output instance has successfully sent. This is the total byte size of all unique chunks sent by this output. If a record is not sent due to some error, then it will not count towards this metric. FluentBitOutputDroppedRecords - The number of log records that have been dropped by the output. This means they met an unrecoverable error or retries expired for their chunk. FluentBitOutputErrors - The number of chunks that have faced an error (either unrecoverable or retrievable). This is the number of times a chunk has failed, and does not correspond with the number of error messages you see in the Fluent Bit log output. FluentBitOutputRetriedRecords - The number of log records that experienced a retry. Note that this is calculated at the chunk level, and the count increased when an entire chunk is marked for retry. An output plugin may or may not perform multiple actions that generate many error messages when uploading a single chunk. FluentBitOutputRetriesFailed - The number of times that retries expired for a chunk. Each plugin configures a Retry_Limit which applies to chunks. Once the Retry_Limit has been reached for a chunk, it is discarded and this metric is incremented. FluentBitOutputRetries - The number of times this output instance requested a retry for a chunk.","title":"Fluent Bit"},{"location":"implementation-guide/monitoring/#network-load-balancer","text":"SyslogNLBActiveFlowCount - The total number of concurrent flows (or connections) from clients to targets. This metric includes connections in the SYN_SENT and ESTABLISHED states. TCP connections are not terminated at the load balancer, so a client opening a TCP connection to a target counts as a single flow. SyslogNLBProcessedBytes - The total number of bytes processed by the load balancer, including TCP/IP headers. This count includes traffic to and from targets, minus health check traffic.","title":"Network Load Balancer"},{"location":"implementation-guide/monitoring/#buffer-metrics","text":"Log Buffer is a buffer layer between the Log Agent and OpenSearch clusters. The agent uploads logs into the buffer layer before being processed and delivered into the OpenSearch clusters. A buffer layer is a way to protect OpenSearch clusters from overwhelming.","title":"Buffer metrics"},{"location":"implementation-guide/monitoring/#kinesis-data-stream","text":"KDSIncomingBytes \u2013 The number of bytes successfully put to the Kinesis stream over the specified time period. This metric includes bytes from PutRecord and PutRecords operations. Minimum, Maximum, and Average statistics represent the bytes in a single put operation for the stream in the specified time period. KDSIncomingRecords \u2013 The number of records successfully put to the Kinesis stream over the specified time period. This metric includes record counts from PutRecord and PutRecords operations. Minimum, Maximum, and Average statistics represent the records in a single put operation for the stream in the specified time period. KDSPutRecordBytes \u2013 The number of bytes put to the Kinesis stream using the PutRecord operation over the specified time period. KDSThrottledRecords \u2013 The number of records rejected due to throttling in a PutRecords operation per Kinesis data stream, measured over the specified time period. KDSWriteProvisionedThroughputExceeded \u2013 The number of records rejected due to throttling for the stream over the specified time period. This metric includes throttling from PutRecord and PutRecords operations. The most commonly used statistic for this metric is Average. When the Minimum statistic has a non-zero value, records will be throttled for the stream during the specified time period. When the Maximum statistic has a value of 0 (zero), no records will be throttled for the stream during the specified time period.","title":"Kinesis Data Stream"},{"location":"implementation-guide/monitoring/#sqs","text":"SQSNumberOfMessagesSent - The number of messages added to a queue. SQSNumberOfMessagesDeleted - The number of messages deleted from the queue. Amazon SQS emits the NumberOfMessagesDeleted metric for every successful deletion operation that uses a valid receipt handle, including duplicate deletions. The following scenarios might cause the value of the NumberOfMessagesDeleted metric to be higher than expected: - Calling the DeleteMessage action on different receipt handles that belong to the same message: If the message is not processed before the visibility timeout expires, the message becomes available to other consumers that can process it and delete it again, increasing the value of the NumberOfMessagesDeleted metric. Calling the DeleteMessage action on the same receipt handle: If the message is processed and deleted, but you call the DeleteMessage action again using the same receipt handle, a success status is returned, increasing the value of the NumberOfMessagesDeleted metric. SQSApproximateNumberOfMessagesVisible - The number of messages available for retrieval from the queue. SQSApproximateAgeOfOldestMessage - The approximate age of the oldest non-deleted message in the queue. After a message is received three times (or more) and not processed, the message is moved to the back of the queue and the ApproximateAgeOfOldestMessage metric points at the second-oldest message that hasn't been received more than three times. This action occurs even if the queue has a redrive policy. Because a single poison-pill message (received multiple times but never deleted) can distort this metric, the age of a poison-pill message isn't included in the metric until the poison-pill message is consumed successfully. When the queue has a redrive policy, the message is moved to a dead-letter queue after the configured Maximum Receives . When the message is moved to the dead-letter queue, the ApproximateAgeOfOldestMessage metric of the dead-letter queue represents the time when the message was moved to the dead-letter queue (not the original time the message was sent).","title":"SQS"},{"location":"implementation-guide/monitoring/#log-processor-metrics","text":"The Log Processor Lambda is responsible for performing final processing on the data and bulk writing it to OpenSearch. TotalLogs \u2013 The total number of log records or events processed by the Lambda function. ExcludedLogs \u2013 The number of log records or events that were excluded from processing, which could be due to filtering or other criteria. LoadedLogs \u2013 The number of log records or events that were successfully processed and loaded into OpenSearch. FailedLogs \u2013 The number of log records or events that failed to be processed or loaded into OpenSearch. ConcurrentExecutions \u2013 The number of function instances that are processing events. If this number reaches your concurrent executions quota for the Region, or the reserved concurrency limit on the function, then Lambda throttles additional invocation requests. Duration \u2013 The amount of time that your function code spends processing an event. The billed duration for an invocation is the value of Duration rounded up to the nearest millisecond. Throttles \u2013 The number of invocation requests that are throttled. When all function instances are processing requests and no concurrency is available to scale up, Lambda rejects additional requests with a TooManyRequestsException error. Throttled requests and other invocation errors don't count as either Invocations or Errors. Invocations \u2013 The number of times that your function code is invoked, including successful invocations and invocations that result in a function error. Invocations aren't recorded if the invocation request is throttled or otherwise results in an invocation error. The value of Invocations equals the number of requests billed.","title":"Log processor metrics"},{"location":"implementation-guide/release-notes/","text":"Date Changes March 2023 Initial release. April 2023 Released version 1.0.1 Fixed deployment failure due to S3 ACL changes. June 2023 Released version 1.0.3 Fixed the EKS Fluent Bit deployment configuration generation issue. Aug 2023 Released version 2.0.0 Added feature of ingesting log from S3 bucket continuously or on-demand Added log pipeline monitoring dashboard into the solution console Supported one-click enablement of pipeline alarms Added an option to automatically attach required IAM policies when creating an Instance Group Displayed an error message on the console when the installation of log agent fails Updated Application log pipeline creation process by allowing customer to specify a log source Added validations to OpenSearch domain when importing a domain or selecting a domain to create log pipeline Supported installing log agent on AL2023 instances Supported ingesting WAF (associated with CloudFront) sampled logs to OpenSearch in other regions except us-east-1 Allowed the same index name in different OpenSearch domains September 2023 Released version 2.0.1 Fixed the following issues: Automatically adjust log processor Lambda request's body size based on AOS instance type When you create an application log pipeline and select Nginx as log format, the default sample dashboard option is set to \"Yes\" Monitoring page cannot show metrics when there is only one dot The time of the data point of the monitoring metrics does not match the time of the abscissa Nov 2023 Released version 2.1.0 Added Light Engine to provide an Athena-based serverless and cost-effective log analytics engine to analyze infrequent access logs Added OpenSearch Ingestion to provide more log processing capabilities, with which OSI can provision compute resource (OCU)and pay per ingestion capacity Supported parsing logs in nested JSON format Supported CloudTrail logs ingestion from the specified bucket manually Fix can not list instances when creating instance group issue Fix the EC2 instance launch by the Auto Scaling group will fail to pass the health check issue","title":"Revisions"},{"location":"implementation-guide/source/","text":"Visit our GitHub repository to download the source code for this solution. The solution template is generated using the AWS Cloud Development Kit (CDK) . Refer to the README.md file for additional information.","title":"Developer guide"},{"location":"implementation-guide/trouble-shooting/","text":"Troubleshooting The following help you to fix errors or problems that you might encounter when using Centralized Logging with OpenSearch. Error: Failed to assume service-linked role arn:x:x:x:/AWSServiceRoleForAppSync The reason for this error is that the account has never used the AWS AppSync service. You can deploy the solution's CloudFormation template again. AWS has already created the role automatically when you encountered the error. You can also go to AWS CloudShell or the local terminal and run the following AWS CLI command to Link AppSync Role aws iam create-service-linked-role --aws-service-name appsync.amazonaws.com Error: Unable to add backend role Centralized Logging with OpenSearch only supports Amazon OpenSearch Service domain with Fine-grained access control enabled. You need to go to Amazon OpenSearch Service console, and edit the Access policy for the Amazon OpenSearch Service domain. Error\uff1aUser xxx is not authorized to perform sts:AssumeRole on resource If you see this error, please make sure you have entered the correct information during cross account setup , and then please wait for several minutes. Centralized Logging with OpenSearch uses AssumeRole for cross-account access. This is the best practice to temporary access the AWS resources in your member account. However, these roles created during cross account setup take seconds or minutes to be affective. Error: PutRecords API responded with error='InvalidSignatureException' Fluent-bit agent reports PutRecords API responded with error='InvalidSignatureException', message='The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.' Please restart the fluent-bit agent. For example, on EC2 with Amazon Linux2, run command: sudo service fluent-bit restart Error: PutRecords API responded with error='AccessDeniedException' Fluent-bit agent deployed on EKS Cluster reports \"AccessDeniedException\" when sending records to Kinesis. Verify that the IAM role trust relations are correctly set. With the Centralized Logging with OpenSearch console: Open the Centralized Logging with OpenSearch console. In the left sidebar, under Log Source , choose EKS Clusters . Choose the EKS Cluster that you want to check. Click the IAM Role ARN which will open the IAM Role in AWS Console. Choose the Trust relationships to verify that the OIDC Provider, the service account namespace and conditions are correctly set. You can get more information from Amazon EKS IAM role configuration My CloudFormation stack is stuck on deleting an AWS::Lambda::Function resource when I update the stack. How to resolve it? The Lambda function resides in a VPC, and you need to wait for the associated ENI resource to be deleted. The agent status is offline after I restart the EC2 instance, how can I make it auto start on instance restart? This usually happens if you have installed the log agent, but restart the instance before you create any Log Ingestion. The log agent will auto restart if there is at least one Log Ingestion. If you have a log ingestion, but the problem still exists, you can use systemctl status fluent-bit to check its status inside the instance. I have switched to Global tenant. However, I still cannot find the dashboard in OpenSearch. This is usually because Centralized Logging with OpenSearch received 403 error from OpenSearch when creating the index template and dashboard. This can be fixed by re-run the Lambda function manually by following the steps below: With the Centralized Logging with OpenSearch console: Open the Centralized Logging with OpenSearch console, and find the AWS Service Log pipeline which has this issue. Copy the first 5 characters from the ID section. E.g. you should copy c169c from ID c169cb23-88f3-4a7e-90d7-4ab4bc18982c Go to AWS Console > Lambda. Paste in function filters. This will filter in all the lambda function created for this AWS Service Log ingestion. Click the Lambda function whose name contains \"OpenSearchHelperFn\". In the Test tab, create a new event with any Event name. Click the Test button to trigger the Lambda, and wait the lambda function to complete. The dashboard should be available in OpenSearch. Error from Fluent-bit agent: version `GLIBC_2.25' not found This error is caused by old version of glibc . Centralized Logging with OpenSearch with version later than 1.2 requires glibc-2.25 or above. So you must upgrade the existing version in EC2 first. The upgrade command for different kinds of OS is shown as follows: Important We strongly recommend you run the commands with environments first. Any upgrade failure may cause severe loss. Redhat 7.9 For Redhat 7.9, the whole process will take about 2 hours,and at least 10 GB storage is needed. # install library yum install -y gcc gcc-c++ m4 python3 bison fontconfig-devel libXpm-devel texinfo bzip2 wget echo /usr/local/lib >> /etc/ld.so.conf # create tmp directory mkdir -p /tmp/library cd /tmp/library # install gmp-6.1.0 wget https://ftp.gnu.org/gnu/gmp/gmp-6.1.0.tar.bz2 tar xjvf gmp-6.1.0.tar.bz2 cd gmp-6.1.0 ./configure --prefix=/usr/local make && make install ldconfig cd .. # install mpfr-3.1.4 wget https://gcc.gnu.org/pub/gcc/infrastructure/mpfr-3.1.4.tar.bz2 tar xjvf mpfr-3.1.4.tar.bz2 cd mpfr-3.1.4 ./configure --with-gmp=/usr/local --prefix=/usr/local make && make install ldconfig cd .. # install mpc-1.0.3 wget https://gcc.gnu.org/pub/gcc/infrastructure/mpc-1.0.3.tar.gz tar xzvf mpc-1.0.3.tar.gz cd mpc-1.0.3 ./configure --prefix=/usr/local make && make install ldconfig cd .. # install gcc-9.3.0 wget https://ftp.gnu.org/gnu/gcc/gcc-9.3.0/gcc-9.3.0.tar.gz tar xzvf gcc-9.3.0.tar.gz cd gcc-9.3.0 mkdir build cd build/ ../configure --enable-checking=release --enable-language=c,c++ --disable-multilib --prefix=/usr make -j4 && make install ldconfig cd ../.. # install make-4.3 wget https://ftp.gnu.org/gnu/make/make-4.3.tar.gz tar xzvf make-4.3.tar.gz cd make-4.3 mkdir build cd build ../configure --prefix=/usr make && make install cd ../.. # install glibc-2.31 wget https://ftp.gnu.org/gnu/glibc/glibc-2.31.tar.gz tar xzvf glibc-2.31.tar.gz cd glibc-2.31 mkdir build cd build/ ../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin --disable-sanity-checks --disable-werror make all && make install make localedata/install-locales # clean tmp directory cd /tmp rm -rf /tmp/library Ubuntu 22 sudo ln -s /snap/core20/1623/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 sudo ln -s /snap/core20/1623/usr/lib/x86_64-linux-gnu/libssl.so.1.1 /usr/lib/x86_64-linux-gnu/libssl.so.1.1 sudo ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3 Amazon Linux 2023 x86-64: wget https://europe.mirror.pkgbuild.com/core/os/x86_64/openssl-1.1-1.1.1.u-1-x86_64.pkg.tar.zst unzstd openssl-1.1-1.1.1.u-1-x86_64.pkg.tar.zst tar -xvf openssl-1.1-1.1.1.u-1-x86_64.pkg.tar sudo cp usr/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1 sudo cp usr/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1 aarch64: wget https://eu.mirror.archlinuxarm.org/aarch64/core/openssl-1.1-1.1.1.t-1-aarch64.pkg.tar.xz xz --decompress openssl-1.1-1.1.1.t-1-aarch64.pkg.tar.xz tar -xvf openssl-1.1-1.1.1.t-1-aarch64.pkg.tar sudo cp usr/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1 sudo cp usr/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1","title":"Troubleshooting"},{"location":"implementation-guide/trouble-shooting/#troubleshooting","text":"The following help you to fix errors or problems that you might encounter when using Centralized Logging with OpenSearch.","title":"Troubleshooting"},{"location":"implementation-guide/trouble-shooting/#error-failed-to-assume-service-linked-role-arnxxxawsserviceroleforappsync","text":"The reason for this error is that the account has never used the AWS AppSync service. You can deploy the solution's CloudFormation template again. AWS has already created the role automatically when you encountered the error. You can also go to AWS CloudShell or the local terminal and run the following AWS CLI command to Link AppSync Role aws iam create-service-linked-role --aws-service-name appsync.amazonaws.com","title":"Error: Failed to assume service-linked role arn:x:x:x:/AWSServiceRoleForAppSync"},{"location":"implementation-guide/trouble-shooting/#error-unable-to-add-backend-role","text":"Centralized Logging with OpenSearch only supports Amazon OpenSearch Service domain with Fine-grained access control enabled. You need to go to Amazon OpenSearch Service console, and edit the Access policy for the Amazon OpenSearch Service domain.","title":"Error: Unable to add backend role"},{"location":"implementation-guide/trouble-shooting/#erroruser-xxx-is-not-authorized-to-perform-stsassumerole-on-resource","text":"If you see this error, please make sure you have entered the correct information during cross account setup , and then please wait for several minutes. Centralized Logging with OpenSearch uses AssumeRole for cross-account access. This is the best practice to temporary access the AWS resources in your member account. However, these roles created during cross account setup take seconds or minutes to be affective.","title":"Error\uff1aUser xxx is not authorized to perform sts:AssumeRole on resource"},{"location":"implementation-guide/trouble-shooting/#error-putrecords-api-responded-with-errorinvalidsignatureexception","text":"Fluent-bit agent reports PutRecords API responded with error='InvalidSignatureException', message='The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.' Please restart the fluent-bit agent. For example, on EC2 with Amazon Linux2, run command: sudo service fluent-bit restart","title":"Error: PutRecords API responded with error='InvalidSignatureException'"},{"location":"implementation-guide/trouble-shooting/#error-putrecords-api-responded-with-erroraccessdeniedexception","text":"Fluent-bit agent deployed on EKS Cluster reports \"AccessDeniedException\" when sending records to Kinesis. Verify that the IAM role trust relations are correctly set. With the Centralized Logging with OpenSearch console: Open the Centralized Logging with OpenSearch console. In the left sidebar, under Log Source , choose EKS Clusters . Choose the EKS Cluster that you want to check. Click the IAM Role ARN which will open the IAM Role in AWS Console. Choose the Trust relationships to verify that the OIDC Provider, the service account namespace and conditions are correctly set. You can get more information from Amazon EKS IAM role configuration","title":"Error: PutRecords API responded with error='AccessDeniedException'"},{"location":"implementation-guide/trouble-shooting/#my-cloudformation-stack-is-stuck-on-deleting-an-awslambdafunction-resource-when-i-update-the-stack-how-to-resolve-it","text":"The Lambda function resides in a VPC, and you need to wait for the associated ENI resource to be deleted.","title":"My CloudFormation stack is stuck on deleting an AWS::Lambda::Function resource when I update the stack. How to resolve it?"},{"location":"implementation-guide/trouble-shooting/#the-agent-status-is-offline-after-i-restart-the-ec2-instance-how-can-i-make-it-auto-start-on-instance-restart","text":"This usually happens if you have installed the log agent, but restart the instance before you create any Log Ingestion. The log agent will auto restart if there is at least one Log Ingestion. If you have a log ingestion, but the problem still exists, you can use systemctl status fluent-bit to check its status inside the instance.","title":"The agent status is offline after I restart the EC2 instance, how can I make it auto start on instance restart?"},{"location":"implementation-guide/trouble-shooting/#i-have-switched-to-global-tenant-however-i-still-cannot-find-the-dashboard-in-opensearch","text":"This is usually because Centralized Logging with OpenSearch received 403 error from OpenSearch when creating the index template and dashboard. This can be fixed by re-run the Lambda function manually by following the steps below: With the Centralized Logging with OpenSearch console: Open the Centralized Logging with OpenSearch console, and find the AWS Service Log pipeline which has this issue. Copy the first 5 characters from the ID section. E.g. you should copy c169c from ID c169cb23-88f3-4a7e-90d7-4ab4bc18982c Go to AWS Console > Lambda. Paste in function filters. This will filter in all the lambda function created for this AWS Service Log ingestion. Click the Lambda function whose name contains \"OpenSearchHelperFn\". In the Test tab, create a new event with any Event name. Click the Test button to trigger the Lambda, and wait the lambda function to complete. The dashboard should be available in OpenSearch.","title":"I have switched to Global tenant. However, I still cannot find the dashboard in OpenSearch."},{"location":"implementation-guide/trouble-shooting/#error-from-fluent-bit-agent-version-glibc_225-not-found","text":"This error is caused by old version of glibc . Centralized Logging with OpenSearch with version later than 1.2 requires glibc-2.25 or above. So you must upgrade the existing version in EC2 first. The upgrade command for different kinds of OS is shown as follows: Important We strongly recommend you run the commands with environments first. Any upgrade failure may cause severe loss.","title":"Error from Fluent-bit agent: version `GLIBC_2.25' not found"},{"location":"implementation-guide/trouble-shooting/#redhat-79","text":"For Redhat 7.9, the whole process will take about 2 hours,and at least 10 GB storage is needed. # install library yum install -y gcc gcc-c++ m4 python3 bison fontconfig-devel libXpm-devel texinfo bzip2 wget echo /usr/local/lib >> /etc/ld.so.conf # create tmp directory mkdir -p /tmp/library cd /tmp/library # install gmp-6.1.0 wget https://ftp.gnu.org/gnu/gmp/gmp-6.1.0.tar.bz2 tar xjvf gmp-6.1.0.tar.bz2 cd gmp-6.1.0 ./configure --prefix=/usr/local make && make install ldconfig cd .. # install mpfr-3.1.4 wget https://gcc.gnu.org/pub/gcc/infrastructure/mpfr-3.1.4.tar.bz2 tar xjvf mpfr-3.1.4.tar.bz2 cd mpfr-3.1.4 ./configure --with-gmp=/usr/local --prefix=/usr/local make && make install ldconfig cd .. # install mpc-1.0.3 wget https://gcc.gnu.org/pub/gcc/infrastructure/mpc-1.0.3.tar.gz tar xzvf mpc-1.0.3.tar.gz cd mpc-1.0.3 ./configure --prefix=/usr/local make && make install ldconfig cd .. # install gcc-9.3.0 wget https://ftp.gnu.org/gnu/gcc/gcc-9.3.0/gcc-9.3.0.tar.gz tar xzvf gcc-9.3.0.tar.gz cd gcc-9.3.0 mkdir build cd build/ ../configure --enable-checking=release --enable-language=c,c++ --disable-multilib --prefix=/usr make -j4 && make install ldconfig cd ../.. # install make-4.3 wget https://ftp.gnu.org/gnu/make/make-4.3.tar.gz tar xzvf make-4.3.tar.gz cd make-4.3 mkdir build cd build ../configure --prefix=/usr make && make install cd ../.. # install glibc-2.31 wget https://ftp.gnu.org/gnu/glibc/glibc-2.31.tar.gz tar xzvf glibc-2.31.tar.gz cd glibc-2.31 mkdir build cd build/ ../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin --disable-sanity-checks --disable-werror make all && make install make localedata/install-locales # clean tmp directory cd /tmp rm -rf /tmp/library","title":"Redhat 7.9"},{"location":"implementation-guide/trouble-shooting/#ubuntu-22","text":"sudo ln -s /snap/core20/1623/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 sudo ln -s /snap/core20/1623/usr/lib/x86_64-linux-gnu/libssl.so.1.1 /usr/lib/x86_64-linux-gnu/libssl.so.1.1 sudo ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3","title":"Ubuntu 22"},{"location":"implementation-guide/trouble-shooting/#amazon-linux-2023","text":"","title":"Amazon Linux 2023"},{"location":"implementation-guide/trouble-shooting/#x86-64","text":"wget https://europe.mirror.pkgbuild.com/core/os/x86_64/openssl-1.1-1.1.1.u-1-x86_64.pkg.tar.zst unzstd openssl-1.1-1.1.1.u-1-x86_64.pkg.tar.zst tar -xvf openssl-1.1-1.1.1.u-1-x86_64.pkg.tar sudo cp usr/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1 sudo cp usr/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1","title":"x86-64:"},{"location":"implementation-guide/trouble-shooting/#aarch64","text":"wget https://eu.mirror.archlinuxarm.org/aarch64/core/openssl-1.1-1.1.1.t-1-aarch64.pkg.tar.xz xz --decompress openssl-1.1-1.1.1.t-1-aarch64.pkg.tar.xz tar -xvf openssl-1.1-1.1.1.t-1-aarch64.pkg.tar sudo cp usr/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1 sudo cp usr/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1","title":"aarch64:"},{"location":"implementation-guide/uninstall/","text":"Uninstall the Centralized Logging with OpenSearch Warning You will encounter IAM role missing error if you delete the Centralized Logging with OpenSearch main stack before you delete the log pipelines. Centralized Logging with OpenSearch console launches additional CloudFormation stacks to ingest logs. If you want to uninstall the Centralized Logging with OpenSearch solution. We recommend you to delete log pipelines (incl. AWS Service log pipelines and application log pipelines) before uninstall the solution. Step 1. Delete Application Log Pipelines Important Please delete all the log ingestion before deleting an application log pipeline. Go to the Centralized Logging with OpenSearch console, in the left sidebar, choose Application Log . Click the application log pipeline to view details. In the ingestion tab, delete all the application log ingestion in the pipeline. Uninstall/Disable the Fluent Bit agent. EC2 (Optional): after removing the log ingestion from EC2 instance group. Fluent Bit will automatically stop ship logs, it is optional for you to stop the Fluent Bit in your instances. Here are the command for stopping Fluent Bit agent. sudo service fluent-bit stop sudo systemctl disable fluent-bit.service EKS DaemonSet (Mandatory): if you have chosen to deploy the Fluent Bit agent using DaemonSet, you need to delete your Fluent Bit agent. Otherwise, the agent will continue ship logs to Centralized Logging with OpenSearch pipelines. kubectl delete -f ~/fluent-bit-logging.yaml EKS SideCar (Mandatory): please remove the fluent-bit agent in your .yaml file, and restart your pod. Delete the Application Log pipeline. Repeat step 2 to Step 5 to delete all your application log pipelines. Step 2. Delete AWS Service Log Pipelines Go to the Centralized Logging with OpenSearch console, in the left sidebar, choose AWS Service Log . Select and delete the AWS Service Log Pipeline one by one. Step 3. Clean up imported OpenSearch domains Delete Access Proxy , if you have created the proxy using Centralized Logging with OpenSearch console. Delete Alarms , if you have created alarms using Centralized Logging with OpenSearch console. Delete VPC peering Connection between Centralized Logging with OpenSearch's VPC and OpenSearch's VPC. Go to AWS VPC Console . Choose Peering connections in left sidebar. Find and delete the VPC peering connection between the Centralized Logging with OpenSearch's VPC and OpenSearch's VPC. You may not have Peering Connections if you did not use the \"Automatic\" mode when importing OpenSearch domains. (Optional) Remove imported OpenSearch Domains. (This will not delete the Amazon OpenSearch domain in the AWS account.) Step 4. Delete Centralized Logging with OpenSearch stack Go to the CloudFormation console . Find CloudFormation Stack of the Centralized Logging with OpenSearch solution. (Optional) Delete S3 buckets created by Centralized Logging with OpenSearch. Important The S3 bucket whose name contains LoggingBucket is the centralized bucket for your AWS service log. You might have enabled AWS Services to send logs to this S3 bucket. Deleting this bucket will cause AWS Services failed to send logs. Choose the CloudFormation stack of the Centralized Logging with OpenSearch solution, and select the Resources tab. In search bar, enter AWS::S3::Bucket . This will show all the S3 buckets created by Centralized Logging with OpenSearch solution, and the Physical ID field is the S3 bucket name. Go to S3 console, and find the S3 bucket using the bucket name. Empty and Delete the S3 bucket. Delete the CloudFormation Stack of the Centralized Logging with OpenSearch solution","title":"Uninstall the solution"},{"location":"implementation-guide/uninstall/#uninstall-the-centralized-logging-with-opensearch","text":"Warning You will encounter IAM role missing error if you delete the Centralized Logging with OpenSearch main stack before you delete the log pipelines. Centralized Logging with OpenSearch console launches additional CloudFormation stacks to ingest logs. If you want to uninstall the Centralized Logging with OpenSearch solution. We recommend you to delete log pipelines (incl. AWS Service log pipelines and application log pipelines) before uninstall the solution.","title":"Uninstall the Centralized Logging with OpenSearch"},{"location":"implementation-guide/uninstall/#step-1-delete-application-log-pipelines","text":"Important Please delete all the log ingestion before deleting an application log pipeline. Go to the Centralized Logging with OpenSearch console, in the left sidebar, choose Application Log . Click the application log pipeline to view details. In the ingestion tab, delete all the application log ingestion in the pipeline. Uninstall/Disable the Fluent Bit agent. EC2 (Optional): after removing the log ingestion from EC2 instance group. Fluent Bit will automatically stop ship logs, it is optional for you to stop the Fluent Bit in your instances. Here are the command for stopping Fluent Bit agent. sudo service fluent-bit stop sudo systemctl disable fluent-bit.service EKS DaemonSet (Mandatory): if you have chosen to deploy the Fluent Bit agent using DaemonSet, you need to delete your Fluent Bit agent. Otherwise, the agent will continue ship logs to Centralized Logging with OpenSearch pipelines. kubectl delete -f ~/fluent-bit-logging.yaml EKS SideCar (Mandatory): please remove the fluent-bit agent in your .yaml file, and restart your pod. Delete the Application Log pipeline. Repeat step 2 to Step 5 to delete all your application log pipelines.","title":"Step 1. Delete Application Log Pipelines"},{"location":"implementation-guide/uninstall/#step-2-delete-aws-service-log-pipelines","text":"Go to the Centralized Logging with OpenSearch console, in the left sidebar, choose AWS Service Log . Select and delete the AWS Service Log Pipeline one by one.","title":"Step 2. Delete AWS Service Log Pipelines"},{"location":"implementation-guide/uninstall/#step-3-clean-up-imported-opensearch-domains","text":"Delete Access Proxy , if you have created the proxy using Centralized Logging with OpenSearch console. Delete Alarms , if you have created alarms using Centralized Logging with OpenSearch console. Delete VPC peering Connection between Centralized Logging with OpenSearch's VPC and OpenSearch's VPC. Go to AWS VPC Console . Choose Peering connections in left sidebar. Find and delete the VPC peering connection between the Centralized Logging with OpenSearch's VPC and OpenSearch's VPC. You may not have Peering Connections if you did not use the \"Automatic\" mode when importing OpenSearch domains. (Optional) Remove imported OpenSearch Domains. (This will not delete the Amazon OpenSearch domain in the AWS account.)","title":"Step 3. Clean up imported OpenSearch domains"},{"location":"implementation-guide/uninstall/#step-4-delete-centralized-logging-with-opensearch-stack","text":"Go to the CloudFormation console . Find CloudFormation Stack of the Centralized Logging with OpenSearch solution. (Optional) Delete S3 buckets created by Centralized Logging with OpenSearch. Important The S3 bucket whose name contains LoggingBucket is the centralized bucket for your AWS service log. You might have enabled AWS Services to send logs to this S3 bucket. Deleting this bucket will cause AWS Services failed to send logs. Choose the CloudFormation stack of the Centralized Logging with OpenSearch solution, and select the Resources tab. In search bar, enter AWS::S3::Bucket . This will show all the S3 buckets created by Centralized Logging with OpenSearch solution, and the Physical ID field is the S3 bucket name. Go to S3 console, and find the S3 bucket using the bucket name. Empty and Delete the S3 bucket. Delete the CloudFormation Stack of the Centralized Logging with OpenSearch solution","title":"Step 4. Delete Centralized Logging with OpenSearch stack"},{"location":"implementation-guide/applications/","text":"Application Log Analytics Pipelines Centralized Logging with OpenSearch supports ingesting application logs from the following log sources: Amazon EC2 instance group : the solution automatically installs log agent (Fluent Bit 1.9), collects application logs on EC2 instances and then sends logs into Amazon OpenSearch. Amazon EKS cluster : the solution generates all-in-one configuration file for customers to deploy the log agent (Fluent Bit 1.9) as a DaemonSet or Sidecar. After log agent is deployed, the solution starts collecting pod logs and sends them to Amazon OpenSearch Service. Amazon S3 : the solution either ingests logs in the specified Amazon S3 location continuously or performs one-time ingestion. You can also filter logs based on Amazon S3 prefix or parse logs with custom Log Config. Syslog : the solution collects syslog logs through UDP or TCP protocol. Amazon OpenSearch Service is suitable for real-time log analytics and frequent queries and has full-text search capability. As of release 2.1.0, the solution starts to support log ingestion into Light Engine, which is suitable for non real-time log analytics and infrequent queries and has SQL-like search capability. The feature is supported when you choose Amazon EC2 instance group or Amazon EKS cluster as log source. After creating a log analytics pipeline, you can add more log sources to the log analytics pipeline. For more information, see add a new log source . Important If you are using Centralized Logging with OpenSearch to create an application log pipeline for the first time, you are recommended to learn the concepts and supported log formats and log sources . Supported Log Formats and Log Sources The table lists the log formats supported by each log source. For more information about how to create log ingestion for each log format, refer to Log Config . Log Format EC2 Instance Group EKS Cluster Amazon S3 Syslog Nginx Yes Yes Yes No Apache HTTP Server Yes Yes Yes No JSON Yes Yes Yes Yes Single-line Text Yes Yes Yes Yes Multi-line Text Yes Yes Yes No Multi-line Text (Spring Boot) Yes Yes Yes No Syslog RFC5424/RFC3164 No No No Yes Syslog Custom No No No Yes Concepts The following introduce concepts that help you to understand how the application log ingestion works. Application Log Analytics Pipeline To collect application logs, a data pipeline is needed. The pipeline not only buffers the data in transmit but also cleans or pre-processes data. For example, transforming IP to Geo location. Currently, Kinesis Data Stream is used as data buffering for EC2 log source. Log Ingestion A log ingestion configures the Log Source, Log Config and the Application Log Analytics Pipeline for the log agent used by Centralized Logging with OpenSearch. After that, Centralized Logging with OpenSearch will start collecting certain type of logs from the log source and sending them to Amazon OpenSearch. Log Agent A log agent is a program that reads logs from one location and sends them to another location (for example, OpenSearch). Currently, Centralized Logging with OpenSearch only supports Fluent Bit 1.9 log agent which is installed automatically. The Fluent Bit agent has a dependency of OpenSSL 1.1 . To learn how to install OpenSSL on Linux instances, refer to OpenSSL installation . To find the supported platforms by Fluent Bit, refer to this link . Log Buffer Log Buffer is a buffer layer between the Log Agent and OpenSearch clusters. The agent uploads logs into the buffer layer before being processed and delivered into the OpenSearch clusters. A buffer layer is a way to protect OpenSearch clusters from overwhelming. This solution provides the following types of buffer layers. Amazon S3 . Use this option if you can bear minutes-level latency for log ingestion. The log agent periodically uploads logs to an Amazon S3 bucket. The frequency of data delivery to Amazon S3 is determined by Buffer size (default value is 50 MiB) and Buffer interval (default value is 60 seconds) value that you configured when creating the application log analytics pipelines. The condition satisfied first triggers data delivery to Amazon S3. Amazon Kinesis Data Streams . Use this option if you need real-time log ingestion. The log agent uploads logs to Amazon Kinesis Data Stream in seconds. The frequency of data delivery to Kinesis Data Streams is determined by Buffer size (10 MiB) and Buffer interval (5 seconds). The condition satisfied first triggers data delivery to Kinesis Data Streams. Log Buffer is optional when creating an application log analytics pipeline. For all types of application logs, this solution allows you to ingest logs without any buffer layers. However, we only recommend this option when you have small log volume, and you are confident that the logs will not exceed the thresholds at the OpenSearch side. Log Source A Log Source refers to a location where you want Centralized Logging with OpenSearch to collect application logs from. Supported log sources includes: Amazon EC2 Instance Group Amazon EKS Cluster Amazon S3 Syslog Instance Group An instance group is a collection of EC2 instances from which you want to collect application logs. Centralized Logging with OpenSearch can help you install the log agent in each instance within a group. You can select arbitrary instances through the user interface, or choose an EC2 Auto Scaling Group . EKS Cluster The EKS Cluster in Centralized Logging with OpenSearch refers to the Amazon EKS from which you want to collect pod logs. Centralized Logging with OpenSearch will guide you to deploy the log agent as a DaemonSet or Sidecar in the EKS Cluster. Amazon S3 Centralized Logging with OpenSearch supports collectings logs stored in an Amazon S3 bucket. Syslog Centralized Logging with OpenSearch supports collecting syslog logs through UDP or TCP protocol. Log Config A Log Config is a configuration that defines the format of logs (that is, what fields each log line includes, and the data type of each field), based on which the Log Analytics Pipeline parses the logs before ingesting them into log storage. Log Config also allows you to define filters of the logs based on the fields in the logs.","title":"Overview"},{"location":"implementation-guide/applications/#application-log-analytics-pipelines","text":"Centralized Logging with OpenSearch supports ingesting application logs from the following log sources: Amazon EC2 instance group : the solution automatically installs log agent (Fluent Bit 1.9), collects application logs on EC2 instances and then sends logs into Amazon OpenSearch. Amazon EKS cluster : the solution generates all-in-one configuration file for customers to deploy the log agent (Fluent Bit 1.9) as a DaemonSet or Sidecar. After log agent is deployed, the solution starts collecting pod logs and sends them to Amazon OpenSearch Service. Amazon S3 : the solution either ingests logs in the specified Amazon S3 location continuously or performs one-time ingestion. You can also filter logs based on Amazon S3 prefix or parse logs with custom Log Config. Syslog : the solution collects syslog logs through UDP or TCP protocol. Amazon OpenSearch Service is suitable for real-time log analytics and frequent queries and has full-text search capability. As of release 2.1.0, the solution starts to support log ingestion into Light Engine, which is suitable for non real-time log analytics and infrequent queries and has SQL-like search capability. The feature is supported when you choose Amazon EC2 instance group or Amazon EKS cluster as log source. After creating a log analytics pipeline, you can add more log sources to the log analytics pipeline. For more information, see add a new log source . Important If you are using Centralized Logging with OpenSearch to create an application log pipeline for the first time, you are recommended to learn the concepts and supported log formats and log sources .","title":"Application Log Analytics Pipelines"},{"location":"implementation-guide/applications/#supported-log-formats-and-log-sources","text":"The table lists the log formats supported by each log source. For more information about how to create log ingestion for each log format, refer to Log Config . Log Format EC2 Instance Group EKS Cluster Amazon S3 Syslog Nginx Yes Yes Yes No Apache HTTP Server Yes Yes Yes No JSON Yes Yes Yes Yes Single-line Text Yes Yes Yes Yes Multi-line Text Yes Yes Yes No Multi-line Text (Spring Boot) Yes Yes Yes No Syslog RFC5424/RFC3164 No No No Yes Syslog Custom No No No Yes","title":"Supported Log Formats and Log Sources"},{"location":"implementation-guide/applications/#concepts","text":"The following introduce concepts that help you to understand how the application log ingestion works.","title":"Concepts"},{"location":"implementation-guide/applications/#application-log-analytics-pipeline","text":"To collect application logs, a data pipeline is needed. The pipeline not only buffers the data in transmit but also cleans or pre-processes data. For example, transforming IP to Geo location. Currently, Kinesis Data Stream is used as data buffering for EC2 log source.","title":"Application Log Analytics Pipeline"},{"location":"implementation-guide/applications/#log-ingestion","text":"A log ingestion configures the Log Source, Log Config and the Application Log Analytics Pipeline for the log agent used by Centralized Logging with OpenSearch. After that, Centralized Logging with OpenSearch will start collecting certain type of logs from the log source and sending them to Amazon OpenSearch.","title":"Log Ingestion"},{"location":"implementation-guide/applications/#log-agent","text":"A log agent is a program that reads logs from one location and sends them to another location (for example, OpenSearch). Currently, Centralized Logging with OpenSearch only supports Fluent Bit 1.9 log agent which is installed automatically. The Fluent Bit agent has a dependency of OpenSSL 1.1 . To learn how to install OpenSSL on Linux instances, refer to OpenSSL installation . To find the supported platforms by Fluent Bit, refer to this link .","title":"Log Agent"},{"location":"implementation-guide/applications/#log-buffer","text":"Log Buffer is a buffer layer between the Log Agent and OpenSearch clusters. The agent uploads logs into the buffer layer before being processed and delivered into the OpenSearch clusters. A buffer layer is a way to protect OpenSearch clusters from overwhelming. This solution provides the following types of buffer layers. Amazon S3 . Use this option if you can bear minutes-level latency for log ingestion. The log agent periodically uploads logs to an Amazon S3 bucket. The frequency of data delivery to Amazon S3 is determined by Buffer size (default value is 50 MiB) and Buffer interval (default value is 60 seconds) value that you configured when creating the application log analytics pipelines. The condition satisfied first triggers data delivery to Amazon S3. Amazon Kinesis Data Streams . Use this option if you need real-time log ingestion. The log agent uploads logs to Amazon Kinesis Data Stream in seconds. The frequency of data delivery to Kinesis Data Streams is determined by Buffer size (10 MiB) and Buffer interval (5 seconds). The condition satisfied first triggers data delivery to Kinesis Data Streams. Log Buffer is optional when creating an application log analytics pipeline. For all types of application logs, this solution allows you to ingest logs without any buffer layers. However, we only recommend this option when you have small log volume, and you are confident that the logs will not exceed the thresholds at the OpenSearch side.","title":"Log Buffer"},{"location":"implementation-guide/applications/#log-source","text":"A Log Source refers to a location where you want Centralized Logging with OpenSearch to collect application logs from. Supported log sources includes: Amazon EC2 Instance Group Amazon EKS Cluster Amazon S3 Syslog","title":"Log Source"},{"location":"implementation-guide/applications/#instance-group","text":"An instance group is a collection of EC2 instances from which you want to collect application logs. Centralized Logging with OpenSearch can help you install the log agent in each instance within a group. You can select arbitrary instances through the user interface, or choose an EC2 Auto Scaling Group .","title":"Instance Group"},{"location":"implementation-guide/applications/#eks-cluster","text":"The EKS Cluster in Centralized Logging with OpenSearch refers to the Amazon EKS from which you want to collect pod logs. Centralized Logging with OpenSearch will guide you to deploy the log agent as a DaemonSet or Sidecar in the EKS Cluster.","title":"EKS Cluster"},{"location":"implementation-guide/applications/#amazon-s3","text":"Centralized Logging with OpenSearch supports collectings logs stored in an Amazon S3 bucket.","title":"Amazon S3"},{"location":"implementation-guide/applications/#syslog","text":"Centralized Logging with OpenSearch supports collecting syslog logs through UDP or TCP protocol.","title":"Syslog"},{"location":"implementation-guide/applications/#log-config","text":"A Log Config is a configuration that defines the format of logs (that is, what fields each log line includes, and the data type of each field), based on which the Log Analytics Pipeline parses the logs before ingesting them into log storage. Log Config also allows you to define filters of the logs based on the fields in the logs.","title":"Log Config"},{"location":"implementation-guide/applications/create-log-config/","text":"Log Config Centralized Logging with OpenSearch solution supports creating log configs for the following log formats: JSON Apache Nginx Syslog Single-ine text Multi-line text For more information, refer to supported log formats and log sources . The following describes how to create log config for each log format. Create a JSON config Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Choose Create a log config . Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose JSON in the log type dropdown list. In the Sample log parsing section, paste a sample JSON log and click Parse log to verify if the log parsing is successful.JSON type support nested Json with a maximum nesting depth of X. If your JSON log sample is nested JSON, choose Pase Log and it displays a list of field type options for each layer. If needed, you can set the corresponding field type for each layer of fields. If you choose Remove to delete a field. The field type will be automatically inferred by OpenSearch. For Example: {\"timestamp\": \"2023-11-06T08:29:55.266Z\", \"correlationId\": \"566829027325526589\", \"processInfo\": { \"startTime\": \"2023-11-06T08:29:55.266Z\", \"hostname\": \"ltvtix0apidev01\", \"domainId\": \"e6826d97-a60f-45cb-93e1-b4bb5a7add29\", \"groupId\": \"group-2\", \"groupName\": \"grp_dev_bba\", \"serviceId\": \"instance-1\", \"serviceName\": \"ins_dev_bba\", \"version\": \"7.7.20210130\" }, \"transactionSummary\": { \"path\": \"https://www.leadmission-critical.info/relationships\", \"protocol\": \"https\", \"protocolSrc\": \"97\", \"status\": \"exception\", \"serviceContexts\": [ { \"service\": \"NSC_APP-117127_DCTM_Get Documentum Token\", \"monitor\": true, \"client\": \"Pass Through\", \"org\": null, \"app\": null, \"method\": \"getTokenUsingPOST\", \"status\": \"exception\", \"duration\": 25270 } ] } } Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. For nested JSON, the Time Key must be on the first level. Specify the Time format . The format syntax follows strptime . Check this for details. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create . Create an Apache HTTP server log config Apache HTTP Server (httpd) is capable of writing error and access log files to a local directory. You can configure Centralized Logging with OpenSearch to ingest Apache HTTP server logs. Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose Apache HTTP server in the log type dropdown menu. In the Apache Log Format section, paste your Apache HTTP server log format configuration. It is in the format of /etc/httpd/conf/httpd.conf and starts with LogFormat . For example: LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined (Optional) In the Sample log parsing section, paste a sample Apache HTTP server log to verify if the log parsing is successful. For example: 127.0.0.1 - - [22/Dec/2021:06:48:57 +0000] \"GET /xxx HTTP/1.1\" 404 196 \"-\" \"curl/7.79.1\" Choose Create . Create an Nginx log config Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose Nginx in the log type dropdown menu. In the Nginx Log Format section, paste your Nginx log format configuration. It is in the format of /etc/nginx/nginx.conf and starts with log_format . For example: log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; (Optional) In the Sample log parsing section, paste a sample Nginx log to verify if the log parsing is successful. For example: 127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\" (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create . Create a Syslog config Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Choose Syslog in the log type dropdown menu. Note that Centralized Logging with OpenSearch also supports Syslog with JSON format and single-line text format. RFC5424 Paste a sample RFC5424 log. For example: <35>1 2013-10-11T22:14:15Z client_machine su - - - 'su root' failed for joe on /dev/pts/2 Choose Parse Log . Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Specify the Time format . The format syntax follows strptime . Check this manual for details. For example: %Y-%m-%dT%H:%M:%SZ (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create . RFC3164 Paste a sample RFC3164 log. For example: <35>Oct 12 22:14:15 client_machine su: 'su root' failed for joe on /dev/pts/2 Choose Parse Log . Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Since there is no year in the timestamp of RFC3164, it cannot be displayed as a time histogram in the Discover interface of Amazon OpenSearch. Specify the Time format . The format syntax follows strptime . Check this for details. For example: %b %m %H:%M:%S (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create . Custom In the Syslog Format section, paste your Syslog log format configuration. It is in the format of /etc/rsyslog.conf and starts with template or $template . The format syntax follows Syslog Message Format . For example: <%pri%>1 %timestamp:::date-rfc3339% %HOSTNAME% %app-name% %procid% %msgid% %msg%\\n In the Sample log parsing section, paste a sample Nginx log to verify if the log parsing is successful. For example: <35>1 2013-10-11T22:14:15.003Z client_machine su - - 'su root' failed for joe on /dev/pts/2 Check if each fields type mapping is correct. Change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Specify the Time format . The format syntax follows strptime . Check this manual for details. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create . Create a single-line text config Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose Single-line Text in the log type dropdown menu. Write the regular expression in Rubular to validate first and enter the value. For example: (?<remote_addr>\\S+)\\s*-\\s*(?<remote_user>\\S+)\\s*\\[(?<time_local>\\d+/\\S+/\\d+:\\d+:\\d+:\\d+)\\s+\\S+\\]\\s*\"(?<request_method>\\S+)\\s+(?<request_uri>\\S+)\\s+\\S+\"\\s*(?<status>\\S+)\\s*(?<body_bytes_sent>\\S+)\\s*\"(?<http_referer>[^\"]*)\"\\s*\"(?<http_user_agent>[^\"]*)\"\\s*\"(?<http_x_forwarded_for>[^\"]*)\".* In the Sample log parsing section, paste a sample Single-line text log and click Parse log to verify if the log parsing is successful. For example: 127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\" Check if each fields type mapping is correct. Change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Specify the Time format . The format syntax follows strptime . Check this manual for details. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create . Create a multi-line text config Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose Multi-line Text in the log type dropdown menu. Java - Spring Boot For Java Spring Boot logs, you could provide a simple log format. For example: %d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%thread] %logger : %msg%n Paste a sample multi-line log. For example: 2022-02-18 10:32:26.400 ERROR [http-nio-8080-exec-1] org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.ArithmeticException: / by zero] with root cause java.lang.ArithmeticException: / by zero at com.springexamples.demo.web.LoggerController.logs(LoggerController.java:22) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke Choose Parse Log . Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Specify the Time format . The format syntax follows strptime . Check this for details. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create . Custom For other kinds of logs, you could specify the first line regex pattern. For example: (?<time>\\d{4}-\\d{2}-\\d{2}\\s*\\d{2}:\\d{2}:\\d{2}.\\d{3})\\s*(?<message>goroutine\\s*\\d\\s*\\[.+\\]:) Paste a sample multi-line log. For example: 2023-07-12 10:32:26.400 goroutine 1 [chan receive]: runtime.gopark(0x4739b8, 0xc420024178, 0x46fcd7, 0xc, 0xc420028e17, 0x3) /usr/local/go/src/runtime/proc.go:280 +0x12c fp=0xc420053e30 sp=0xc420053e00 pc=0x42503c runtime.goparkunlock(0xc420024178, 0x46fcd7, 0xc, 0x1000f010040c217, 0x3) /usr/local/go/src/runtime/proc.go:286 +0x5e fp=0xc420053e70 sp=0xc420053e30 pc=0x42512e runtime.chanrecv(0xc420024120, 0x0, 0xc420053f01, 0x4512d8) /usr/local/go/src/runtime/chan.go:506 +0x304 fp=0xc420053f20 sp=0xc420053e70 pc=0x4046b4 runtime.chanrecv1(0xc420024120, 0x0) /usr/local/go/src/runtime/chan.go:388 +0x2b fp=0xc420053f50 sp=0xc420053f20 pc=0x40439b main.main() foo.go:9 +0x6f fp=0xc420053f80 sp=0xc420053f50 pc=0x4512ef runtime.main() /usr/local/go/src/runtime/proc.go:185 +0x20d fp=0xc420053fe0 sp=0xc420053f80 pc=0x424bad runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:2337 +0x1 fp=0xc420053fe8 sp=0xc420053fe0 pc=0x44b4d1 Choose Parse Log . Check if each field type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"Log Config"},{"location":"implementation-guide/applications/create-log-config/#log-config","text":"Centralized Logging with OpenSearch solution supports creating log configs for the following log formats: JSON Apache Nginx Syslog Single-ine text Multi-line text For more information, refer to supported log formats and log sources . The following describes how to create log config for each log format.","title":"Log Config"},{"location":"implementation-guide/applications/create-log-config/#create-a-json-config","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Choose Create a log config . Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose JSON in the log type dropdown list. In the Sample log parsing section, paste a sample JSON log and click Parse log to verify if the log parsing is successful.JSON type support nested Json with a maximum nesting depth of X. If your JSON log sample is nested JSON, choose Pase Log and it displays a list of field type options for each layer. If needed, you can set the corresponding field type for each layer of fields. If you choose Remove to delete a field. The field type will be automatically inferred by OpenSearch. For Example: {\"timestamp\": \"2023-11-06T08:29:55.266Z\", \"correlationId\": \"566829027325526589\", \"processInfo\": { \"startTime\": \"2023-11-06T08:29:55.266Z\", \"hostname\": \"ltvtix0apidev01\", \"domainId\": \"e6826d97-a60f-45cb-93e1-b4bb5a7add29\", \"groupId\": \"group-2\", \"groupName\": \"grp_dev_bba\", \"serviceId\": \"instance-1\", \"serviceName\": \"ins_dev_bba\", \"version\": \"7.7.20210130\" }, \"transactionSummary\": { \"path\": \"https://www.leadmission-critical.info/relationships\", \"protocol\": \"https\", \"protocolSrc\": \"97\", \"status\": \"exception\", \"serviceContexts\": [ { \"service\": \"NSC_APP-117127_DCTM_Get Documentum Token\", \"monitor\": true, \"client\": \"Pass Through\", \"org\": null, \"app\": null, \"method\": \"getTokenUsingPOST\", \"status\": \"exception\", \"duration\": 25270 } ] } } Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. For nested JSON, the Time Key must be on the first level. Specify the Time format . The format syntax follows strptime . Check this for details. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"Create a JSON config"},{"location":"implementation-guide/applications/create-log-config/#create-an-apache-http-server-log-config","text":"Apache HTTP Server (httpd) is capable of writing error and access log files to a local directory. You can configure Centralized Logging with OpenSearch to ingest Apache HTTP server logs. Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose Apache HTTP server in the log type dropdown menu. In the Apache Log Format section, paste your Apache HTTP server log format configuration. It is in the format of /etc/httpd/conf/httpd.conf and starts with LogFormat . For example: LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined (Optional) In the Sample log parsing section, paste a sample Apache HTTP server log to verify if the log parsing is successful. For example: 127.0.0.1 - - [22/Dec/2021:06:48:57 +0000] \"GET /xxx HTTP/1.1\" 404 196 \"-\" \"curl/7.79.1\" Choose Create .","title":"Create an Apache HTTP server log config"},{"location":"implementation-guide/applications/create-log-config/#create-an-nginx-log-config","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose Nginx in the log type dropdown menu. In the Nginx Log Format section, paste your Nginx log format configuration. It is in the format of /etc/nginx/nginx.conf and starts with log_format . For example: log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; (Optional) In the Sample log parsing section, paste a sample Nginx log to verify if the log parsing is successful. For example: 127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\" (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"Create an Nginx log config"},{"location":"implementation-guide/applications/create-log-config/#create-a-syslog-config","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Choose Syslog in the log type dropdown menu. Note that Centralized Logging with OpenSearch also supports Syslog with JSON format and single-line text format.","title":"Create a Syslog config"},{"location":"implementation-guide/applications/create-log-config/#rfc5424","text":"Paste a sample RFC5424 log. For example: <35>1 2013-10-11T22:14:15Z client_machine su - - - 'su root' failed for joe on /dev/pts/2 Choose Parse Log . Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Specify the Time format . The format syntax follows strptime . Check this manual for details. For example: %Y-%m-%dT%H:%M:%SZ (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"RFC5424"},{"location":"implementation-guide/applications/create-log-config/#rfc3164","text":"Paste a sample RFC3164 log. For example: <35>Oct 12 22:14:15 client_machine su: 'su root' failed for joe on /dev/pts/2 Choose Parse Log . Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Since there is no year in the timestamp of RFC3164, it cannot be displayed as a time histogram in the Discover interface of Amazon OpenSearch. Specify the Time format . The format syntax follows strptime . Check this for details. For example: %b %m %H:%M:%S (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"RFC3164"},{"location":"implementation-guide/applications/create-log-config/#custom","text":"In the Syslog Format section, paste your Syslog log format configuration. It is in the format of /etc/rsyslog.conf and starts with template or $template . The format syntax follows Syslog Message Format . For example: <%pri%>1 %timestamp:::date-rfc3339% %HOSTNAME% %app-name% %procid% %msgid% %msg%\\n In the Sample log parsing section, paste a sample Nginx log to verify if the log parsing is successful. For example: <35>1 2013-10-11T22:14:15.003Z client_machine su - - 'su root' failed for joe on /dev/pts/2 Check if each fields type mapping is correct. Change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Specify the Time format . The format syntax follows strptime . Check this manual for details. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"Custom"},{"location":"implementation-guide/applications/create-log-config/#create-a-single-line-text-config","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose Single-line Text in the log type dropdown menu. Write the regular expression in Rubular to validate first and enter the value. For example: (?<remote_addr>\\S+)\\s*-\\s*(?<remote_user>\\S+)\\s*\\[(?<time_local>\\d+/\\S+/\\d+:\\d+:\\d+:\\d+)\\s+\\S+\\]\\s*\"(?<request_method>\\S+)\\s+(?<request_uri>\\S+)\\s+\\S+\"\\s*(?<status>\\S+)\\s*(?<body_bytes_sent>\\S+)\\s*\"(?<http_referer>[^\"]*)\"\\s*\"(?<http_user_agent>[^\"]*)\"\\s*\"(?<http_x_forwarded_for>[^\"]*)\".* In the Sample log parsing section, paste a sample Single-line text log and click Parse log to verify if the log parsing is successful. For example: 127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\" Check if each fields type mapping is correct. Change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Specify the Time format . The format syntax follows strptime . Check this manual for details. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"Create a single-line text config"},{"location":"implementation-guide/applications/create-log-config/#create-a-multi-line-text-config","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Resources , choose Log Config . Click the Create a log config button. Specify Config Name . Specify Log Path . You can use , to separate multiple paths. Choose Multi-line Text in the log type dropdown menu.","title":"Create a multi-line text config"},{"location":"implementation-guide/applications/create-log-config/#java-spring-boot","text":"For Java Spring Boot logs, you could provide a simple log format. For example: %d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%thread] %logger : %msg%n Paste a sample multi-line log. For example: 2022-02-18 10:32:26.400 ERROR [http-nio-8080-exec-1] org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.ArithmeticException: / by zero] with root cause java.lang.ArithmeticException: / by zero at com.springexamples.demo.web.LoggerController.logs(LoggerController.java:22) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke Choose Parse Log . Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. Specify the Time format . The format syntax follows strptime . Check this for details. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"Java - Spring Boot"},{"location":"implementation-guide/applications/create-log-config/#custom_1","text":"For other kinds of logs, you could specify the first line regex pattern. For example: (?<time>\\d{4}-\\d{2}-\\d{2}\\s*\\d{2}:\\d{2}:\\d{2}.\\d{3})\\s*(?<message>goroutine\\s*\\d\\s*\\[.+\\]:) Paste a sample multi-line log. For example: 2023-07-12 10:32:26.400 goroutine 1 [chan receive]: runtime.gopark(0x4739b8, 0xc420024178, 0x46fcd7, 0xc, 0xc420028e17, 0x3) /usr/local/go/src/runtime/proc.go:280 +0x12c fp=0xc420053e30 sp=0xc420053e00 pc=0x42503c runtime.goparkunlock(0xc420024178, 0x46fcd7, 0xc, 0x1000f010040c217, 0x3) /usr/local/go/src/runtime/proc.go:286 +0x5e fp=0xc420053e70 sp=0xc420053e30 pc=0x42512e runtime.chanrecv(0xc420024120, 0x0, 0xc420053f01, 0x4512d8) /usr/local/go/src/runtime/chan.go:506 +0x304 fp=0xc420053f20 sp=0xc420053e70 pc=0x4046b4 runtime.chanrecv1(0xc420024120, 0x0) /usr/local/go/src/runtime/chan.go:388 +0x2b fp=0xc420053f50 sp=0xc420053f20 pc=0x40439b main.main() foo.go:9 +0x6f fp=0xc420053f80 sp=0xc420053f50 pc=0x4512ef runtime.main() /usr/local/go/src/runtime/proc.go:185 +0x20d fp=0xc420053fe0 sp=0xc420053f80 pc=0x424bad runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:2337 +0x1 fp=0xc420053fe8 sp=0xc420053fe0 pc=0x44b4d1 Choose Parse Log . Check if each field type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types . Note You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added. (Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. Select Create .","title":"Custom"},{"location":"implementation-guide/applications/create-log-source/","text":"You need to create a log source first before collecting application logs. Centralized Logging with OpenSearch supports the following log sources: Amazon EC2 instance group Amazon EKS cluster Amazon S3 Syslog For more information, see concepts . Amazon EC2 Instance Group An instance group represents a group of EC2 Linux instances, which enables the solution to associate a Log Config with multiple EC2 instances quickly. Centralized Logging with OpenSearch uses Systems Manager Agent(SSM Agent) to install/configure Fluent Bit agent, and sends log data to Kinesis Data Streams . Prerequisites Make sure the instances meet the following requirements: SSM agent is installed on instances. Refer to install SSM agent on EC2 instances for Linux for more details. The AmazonSSMManagedInstanceCore policy is being associated with the instances. The OpenSSL 1.1 or later is installed. Refer to OpenSSL Installation for more details. The instances have network access to AWS Systems Manager. The instances have network access to Amazon Kinesis Data Streams, if you use it as the Log Buffer . The instances have network access to Amazon S3, if you use it as the Log Buffer . The operating system of the instances are supported by Fluent Bit. Refer to Supported Platform . (Option 1) Select instances to create an Instance Group Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Source , choose Instance Group . Choose Create an instance group . In the Instance Group Settings section, specify a group name. Select Instances . You can use up to 5 tags to filter the instances. Verify that all the selected instances \"Pending Status\" is Online . (Optional) If the selected instances \"Pending Status\" are empty, click the Install log agent button and wait for \"Pending Status\" to become Online . (Optional) If you want to ingest logs from another account, select a linked account in the Account Settings section to create an instance group log source from another account. Choose Create . Important An installation error may occur if you use the Centralized Logging with OpenSearch console to install Fluent Bit agent on Ubuntu instances in China (Beijing) Region Operated by Sinnet (cn-north-1) and China (Ningxia) Region Operated by NWCD (cn-northwest-1) Region. This is because the Fluent Bit assets cannot be downloaded successfully. You need to install the Fluent Bit agent by yourself. (Option 2) Select an Auto Scaling group to create an Instance Group When creating an Instance Group with Amazon EC2 Auto Scaling group, the solution will generate a shell script which you should include in the EC2 User Data . Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Source , choose Instance Group . Choose Create an instance group . In the Instance Group Settings section, specify a group name. Select Auto Scaling Groups . Select the autoscaling group from which you want to collect logs. (Optional) If you want to ingest logs from another account, select a linked account in the Account Settings section to create an instance group log source from another account. Choose Create . After you created a Log Ingestion using the Instance Group, you can find the generated Shell Script in the details page. Copy the shell script and update the User Data of the Auto Scaling Group's launch configurations or launch template . The shell script will automatically install Fluent Bit, SSM agent if needed, and download Fluent Bit configurations. Once you have updated the launch configurations or launch template, you need to start an instance refresh to update the instances within the Auto Scaling group. The newly launched instances will ingest logs to the OpenSearch cluster or the Log Buffer layer. Amazon EKS cluster The EKS Cluster in Centralized Logging with OpenSearch refers to the Amazon Elastic Kubernetes Service (Amazon EKS) from which you want to collect pod logs. Centralized Logging with OpenSearch will guide you to deploy the log agent as a DaemonSet or Sidecar in the EKS Cluster. Important Centralized Logging with OpenSearch does not support sending logs in one EKS cluster to more than one Amazon OpenSearch domain at the same time. Make sure your EKS cluster's VPC is connected to Amazon OpenSearch Service cluster's VPC so that log can be ingested. Refer to VPC Connectivity for more details regarding approaches to connect VPCs. Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Source , choose EKS Cluster . Choose Import a Cluster . Choose the EKS Cluster where Centralized Logging with OpenSearch collects logs from. (Optional) If you want to ingest logs from another account, select a linked account from the Account dropdown to import an EKS log source from another account. Select DaemonSet or Sidecar as log agent's deployment pattern. Choose Next . Specify the Amazon OpenSearch where Centralized Logging with OpenSearch sends the logs to. Follow the guidance to establish a VPC peering connection between EKS's VPC and OpenSearch's VPC. Create and accept VPC peering connections Update your route tables for a VPC peering connection Update your security groups to reference peer VPC groups Choose Next . Add tags if needed. Choose Create . Amazon S3 The S3 in Centralized Logging with OpenSearch refers to the Amazon S3 from which you want to collect application logs stored in your bucket. You can choose On-going or One-time to create your ingestion job. Important On-going means that the ingestion job will run when a new file is delivered to the specified S3 location. One-time means that the ingestion job will run at creation and only will run once to load all files in the specified location. Syslog Important To ingest logs, make sure your Syslog generator/sender\u2019s subnet is connected to Centralized Logging with OpenSearch\u2019s two private subnets. Refer to VPC Connectivity for more details about how to connect VPCs. You can use UDP or TCP custom port number to collect syslog in Centralized Logging with OpenSearch. Syslog refers to logs generated by Linux instance, routers or network equipment. For more information, see Syslog in Wikipedia. Add a new log source After log analytics pipeline is created, it has one log source. You can choose to add more log sources into the log pipeline by following below steps: Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose the log pipeline by clicking its ID \u3002 Choose Create a source . Follow the instructions in Amazon EC2 instance group , Amazon EKS cluster , Amazon S3 , or Syslog to create a log source according to your need.","title":"Log source"},{"location":"implementation-guide/applications/create-log-source/#amazon-ec2-instance-group","text":"An instance group represents a group of EC2 Linux instances, which enables the solution to associate a Log Config with multiple EC2 instances quickly. Centralized Logging with OpenSearch uses Systems Manager Agent(SSM Agent) to install/configure Fluent Bit agent, and sends log data to Kinesis Data Streams .","title":"Amazon EC2 Instance Group"},{"location":"implementation-guide/applications/create-log-source/#prerequisites","text":"Make sure the instances meet the following requirements: SSM agent is installed on instances. Refer to install SSM agent on EC2 instances for Linux for more details. The AmazonSSMManagedInstanceCore policy is being associated with the instances. The OpenSSL 1.1 or later is installed. Refer to OpenSSL Installation for more details. The instances have network access to AWS Systems Manager. The instances have network access to Amazon Kinesis Data Streams, if you use it as the Log Buffer . The instances have network access to Amazon S3, if you use it as the Log Buffer . The operating system of the instances are supported by Fluent Bit. Refer to Supported Platform .","title":"Prerequisites"},{"location":"implementation-guide/applications/create-log-source/#option-1-select-instances-to-create-an-instance-group","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Source , choose Instance Group . Choose Create an instance group . In the Instance Group Settings section, specify a group name. Select Instances . You can use up to 5 tags to filter the instances. Verify that all the selected instances \"Pending Status\" is Online . (Optional) If the selected instances \"Pending Status\" are empty, click the Install log agent button and wait for \"Pending Status\" to become Online . (Optional) If you want to ingest logs from another account, select a linked account in the Account Settings section to create an instance group log source from another account. Choose Create . Important An installation error may occur if you use the Centralized Logging with OpenSearch console to install Fluent Bit agent on Ubuntu instances in China (Beijing) Region Operated by Sinnet (cn-north-1) and China (Ningxia) Region Operated by NWCD (cn-northwest-1) Region. This is because the Fluent Bit assets cannot be downloaded successfully. You need to install the Fluent Bit agent by yourself.","title":"(Option 1) Select instances to create an Instance Group"},{"location":"implementation-guide/applications/create-log-source/#option-2-select-an-auto-scaling-group-to-create-an-instance-group","text":"When creating an Instance Group with Amazon EC2 Auto Scaling group, the solution will generate a shell script which you should include in the EC2 User Data . Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Source , choose Instance Group . Choose Create an instance group . In the Instance Group Settings section, specify a group name. Select Auto Scaling Groups . Select the autoscaling group from which you want to collect logs. (Optional) If you want to ingest logs from another account, select a linked account in the Account Settings section to create an instance group log source from another account. Choose Create . After you created a Log Ingestion using the Instance Group, you can find the generated Shell Script in the details page. Copy the shell script and update the User Data of the Auto Scaling Group's launch configurations or launch template . The shell script will automatically install Fluent Bit, SSM agent if needed, and download Fluent Bit configurations. Once you have updated the launch configurations or launch template, you need to start an instance refresh to update the instances within the Auto Scaling group. The newly launched instances will ingest logs to the OpenSearch cluster or the Log Buffer layer.","title":"(Option 2) Select an Auto Scaling group to create an Instance Group"},{"location":"implementation-guide/applications/create-log-source/#amazon-eks-cluster","text":"The EKS Cluster in Centralized Logging with OpenSearch refers to the Amazon Elastic Kubernetes Service (Amazon EKS) from which you want to collect pod logs. Centralized Logging with OpenSearch will guide you to deploy the log agent as a DaemonSet or Sidecar in the EKS Cluster. Important Centralized Logging with OpenSearch does not support sending logs in one EKS cluster to more than one Amazon OpenSearch domain at the same time. Make sure your EKS cluster's VPC is connected to Amazon OpenSearch Service cluster's VPC so that log can be ingested. Refer to VPC Connectivity for more details regarding approaches to connect VPCs. Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Source , choose EKS Cluster . Choose Import a Cluster . Choose the EKS Cluster where Centralized Logging with OpenSearch collects logs from. (Optional) If you want to ingest logs from another account, select a linked account from the Account dropdown to import an EKS log source from another account. Select DaemonSet or Sidecar as log agent's deployment pattern. Choose Next . Specify the Amazon OpenSearch where Centralized Logging with OpenSearch sends the logs to. Follow the guidance to establish a VPC peering connection between EKS's VPC and OpenSearch's VPC. Create and accept VPC peering connections Update your route tables for a VPC peering connection Update your security groups to reference peer VPC groups Choose Next . Add tags if needed. Choose Create .","title":"Amazon EKS cluster"},{"location":"implementation-guide/applications/create-log-source/#amazon-s3","text":"The S3 in Centralized Logging with OpenSearch refers to the Amazon S3 from which you want to collect application logs stored in your bucket. You can choose On-going or One-time to create your ingestion job. Important On-going means that the ingestion job will run when a new file is delivered to the specified S3 location. One-time means that the ingestion job will run at creation and only will run once to load all files in the specified location.","title":"Amazon S3"},{"location":"implementation-guide/applications/create-log-source/#syslog","text":"Important To ingest logs, make sure your Syslog generator/sender\u2019s subnet is connected to Centralized Logging with OpenSearch\u2019s two private subnets. Refer to VPC Connectivity for more details about how to connect VPCs. You can use UDP or TCP custom port number to collect syslog in Centralized Logging with OpenSearch. Syslog refers to logs generated by Linux instance, routers or network equipment. For more information, see Syslog in Wikipedia.","title":"Syslog"},{"location":"implementation-guide/applications/create-log-source/#add-a-new-log-source","text":"After log analytics pipeline is created, it has one log source. You can choose to add more log sources into the log pipeline by following below steps: Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose the log pipeline by clicking its ID \u3002 Choose Create a source . Follow the instructions in Amazon EC2 instance group , Amazon EKS cluster , Amazon S3 , or Syslog to create a log source according to your need.","title":"Add a new log source"},{"location":"implementation-guide/applications/ec2-pipeline/","text":"Amazon EC2 instance group as log source An instance group represents a group of EC2 Linux instances, which enables the solution to associate a Log Config with multiple EC2 instances quickly. Centralized Logging with OpenSearch uses Systems Manager Agent (SSM Agent) to install/configure Fluent Bit agent, and sends log data to Kinesis Data Streams. This article guides you to create a log pipeline that ingests logs from an EC2 instance group. Create a log analytics pipeline (Amazon OpenSearch) Prerequisites Import an Amazon OpenSearch Service domain . Create a log analytics pipeline Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline Choose Instance Group as Log Source, choose Amazon OpenSearch , and choose Next . Select an instance group. If you have no instance group yet, choose Create Instance Group at the top right corner, and follow the Instance Group guide to create an instance group. After that, choose Refresh and then select the newly created instance group. (Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template . Keep the default Permission grant method . (Optional) If you choose I will manually add the below required permissions after pipeline creation , continue to do the following: Choose Expand to view required permissions and copy the provided JSON policy. Go to AWS Management Console . On the left navigation pane, choose IAM , and select Policies under Access management . Choose Create Policy , choose JSON and replace all the content inside the text block. Make sure to substitute <YOUR ACCOUNT ID> with your account id. Choose Next , and then enter a name for this policy. Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your [launch template][launch-template] or [launch configuration][launch-configuration]. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon EC2 instance group as log source. Enter a Log Path to specify the location of logs you want to collect. Select a log config and then choose Next . If you do not find desired log config from the drop-down list, choose Create New , and follow instructions in Log Cong . Specify Index name in lowercase. In the Buffer section, choose S3 or Kinesis Data Streams . If you don't want the buffer layer, choose None . Refer to the Log Buffer for more information about choosing the appropriate buffer layer. S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. S3 Bucket Prefix AppLogs/<index-prefix>/year=%Y/month=%m/day=%d The log agent appends the prefix when delivering the log files to the S3 bucket. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Kinesis Data Streams buffer parameters Parameter Default Description Shard number <Requires input> The number of shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. Enable auto scaling No This solution monitors the utilization of Kinesis Data Streams every 5 minutes, and scale in/out the number of shards automatically. The solution will scale in/out for a maximum of 8 times within 24 hours. Maximum Shard number <Requires input> Required if auto scaling is enabled. The maximum number of shards. Important You may observe duplicate logs in OpenSearch if threshold error occurs in Kinesis Data Streams (KDS). This is because the Fluent Bit log agent uploads logs in chunk (contains multiple records), and will retry the chunk if upload failed. Each KDS shard can support up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. Please estimate your log volume and choose an appropriate shard number. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. Add tags if needed. Choose Create . Wait for the application pipeline turning to \"Active\" state. Create a log analytics pipeline (Light Engine) Create a log analytics pipeline Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline Choose Instance Group as Log Source, Choose Light Engine , and choose Next . Select an instance group. If you have no instance group yet, choose Create Instance Group at the top right corner, and follow the Instance Group guide to create an instance group. After that, choose Refresh and then select the newly created instance group. (Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template . Keep the default Permission grant method . (Optional) If you choose I will manually add the below required permissions after pipeline creation , continue to do the following: Choose Expand to view required permissions and copy the provided JSON policy. Go to AWS Management Console . On the left navigation pane, choose IAM , and select Policies under Access management . Choose Create Policy , choose JSON and replace all the content inside the text block. Make sure to substitute <YOUR ACCOUNT ID> with your account id. Choose Next , and then enter a name for this policy. Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your [launch template][launch-template] or [launch configuration][launch-configuration]. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon EC2 instance group as log source. Enter a Log Path to specify the location of logs you want to collect. Select a log config and then choose Next . If you do not find desired log config from the drop-down list, choose Create New , and follow instructions in Log Cong . In the Buffer section, S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Choose Next . In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. If desired, add tags. Select Create . Wait for the application pipeline turning to \"Active\" state.","title":"EC2 instance group"},{"location":"implementation-guide/applications/ec2-pipeline/#amazon-ec2-instance-group-as-log-source","text":"An instance group represents a group of EC2 Linux instances, which enables the solution to associate a Log Config with multiple EC2 instances quickly. Centralized Logging with OpenSearch uses Systems Manager Agent (SSM Agent) to install/configure Fluent Bit agent, and sends log data to Kinesis Data Streams. This article guides you to create a log pipeline that ingests logs from an EC2 instance group.","title":"Amazon EC2 instance group as log source"},{"location":"implementation-guide/applications/ec2-pipeline/#create-a-log-analytics-pipeline-amazon-opensearch","text":"","title":"Create a log analytics pipeline (Amazon OpenSearch)"},{"location":"implementation-guide/applications/ec2-pipeline/#prerequisites","text":"Import an Amazon OpenSearch Service domain .","title":"Prerequisites"},{"location":"implementation-guide/applications/ec2-pipeline/#create-a-log-analytics-pipeline","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline Choose Instance Group as Log Source, choose Amazon OpenSearch , and choose Next . Select an instance group. If you have no instance group yet, choose Create Instance Group at the top right corner, and follow the Instance Group guide to create an instance group. After that, choose Refresh and then select the newly created instance group. (Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template . Keep the default Permission grant method . (Optional) If you choose I will manually add the below required permissions after pipeline creation , continue to do the following: Choose Expand to view required permissions and copy the provided JSON policy. Go to AWS Management Console . On the left navigation pane, choose IAM , and select Policies under Access management . Choose Create Policy , choose JSON and replace all the content inside the text block. Make sure to substitute <YOUR ACCOUNT ID> with your account id. Choose Next , and then enter a name for this policy. Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your [launch template][launch-template] or [launch configuration][launch-configuration]. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon EC2 instance group as log source. Enter a Log Path to specify the location of logs you want to collect. Select a log config and then choose Next . If you do not find desired log config from the drop-down list, choose Create New , and follow instructions in Log Cong . Specify Index name in lowercase. In the Buffer section, choose S3 or Kinesis Data Streams . If you don't want the buffer layer, choose None . Refer to the Log Buffer for more information about choosing the appropriate buffer layer. S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. S3 Bucket Prefix AppLogs/<index-prefix>/year=%Y/month=%m/day=%d The log agent appends the prefix when delivering the log files to the S3 bucket. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Kinesis Data Streams buffer parameters Parameter Default Description Shard number <Requires input> The number of shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. Enable auto scaling No This solution monitors the utilization of Kinesis Data Streams every 5 minutes, and scale in/out the number of shards automatically. The solution will scale in/out for a maximum of 8 times within 24 hours. Maximum Shard number <Requires input> Required if auto scaling is enabled. The maximum number of shards. Important You may observe duplicate logs in OpenSearch if threshold error occurs in Kinesis Data Streams (KDS). This is because the Fluent Bit log agent uploads logs in chunk (contains multiple records), and will retry the chunk if upload failed. Each KDS shard can support up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. Please estimate your log volume and choose an appropriate shard number. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. Add tags if needed. Choose Create . Wait for the application pipeline turning to \"Active\" state.","title":"Create a log analytics pipeline"},{"location":"implementation-guide/applications/ec2-pipeline/#create-a-log-analytics-pipeline-light-engine","text":"","title":"Create a log analytics pipeline (Light Engine)"},{"location":"implementation-guide/applications/ec2-pipeline/#create-a-log-analytics-pipeline_1","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline Choose Instance Group as Log Source, Choose Light Engine , and choose Next . Select an instance group. If you have no instance group yet, choose Create Instance Group at the top right corner, and follow the Instance Group guide to create an instance group. After that, choose Refresh and then select the newly created instance group. (Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template . Keep the default Permission grant method . (Optional) If you choose I will manually add the below required permissions after pipeline creation , continue to do the following: Choose Expand to view required permissions and copy the provided JSON policy. Go to AWS Management Console . On the left navigation pane, choose IAM , and select Policies under Access management . Choose Create Policy , choose JSON and replace all the content inside the text block. Make sure to substitute <YOUR ACCOUNT ID> with your account id. Choose Next , and then enter a name for this policy. Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your [launch template][launch-template] or [launch configuration][launch-configuration]. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon EC2 instance group as log source. Enter a Log Path to specify the location of logs you want to collect. Select a log config and then choose Next . If you do not find desired log config from the drop-down list, choose Create New , and follow instructions in Log Cong . In the Buffer section, S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Choose Next . In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. If desired, add tags. Select Create . Wait for the application pipeline turning to \"Active\" state.","title":"Create a log analytics pipeline"},{"location":"implementation-guide/applications/eks-pipeline/","text":"Amazon EKS cluster as log source For Amazon Elastic Kubernetes Service (Amazon EKS) clusters, Centralized Logging with OpenSearch will generate all-in-one configuration file for customers to deploy the log agent ( Fluent Bit 1.9 ) as a DaemonSet or Sidecar. After log agent is deployed, Centralized Logging with OpenSearch will start collecting pod logs and send to Amazon OpenSearch. This article guides you to create a log pipeline that ingests logs from an EKS cluster. Create log analytics pipeline (Amazon OpenSearch) Prerequisites Import an Amazon OpenSearch Service domain . Create log analytics pipeline Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline . Choose EKS Clusters as Log Source, and choose Next . Choose the AWS account in which the logs are stored. Choose an EKS Cluster. If you have no imported cluster yet, choose Import an EKS Cluster , and follow the EKS cluster guide to import an EKS cluster. After that, select the newly imported EKS cluster from the drop-down list. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon EKS cluster as log source. Enter a Log Path to specify the location of logs you want to collect. Select a log config and then choose Next . If you do not find desired log config from the drop-down list, choose Create New and follow instructions in Log Cong . Specify Index name in lowercase. In the Buffer section, choose S3 or Kinesis Data Streams . If you don't want the buffer layer, choose None . Refer to the Log Buffer for more information about choosing the appropriate buffer layer. S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. S3 Bucket Prefix AppLogs/<index-prefix>/year=%Y/month=%m/day=%d The log agent appends the prefix when delivering the log files to the S3 bucket. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Kinesis Data Streams buffer parameters Parameter Default Description Shard number <Requires input> The number of shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. Enable auto scaling No This solution monitors the utilization of Kinesis Data Streams every 5 minutes, and scale in/out the number of shards automatically. The solution will scale in/out for a maximum of 8 times within 24 hours. Maximum Shard number <Requires input> Required if auto scaling is enabled. The maximum number of shards. Important You may observe duplicate logs in OpenSearch if threshold error occurs in Kinesis Data Streams (KDS). This is because the Fluent Bit log agent uploads logs in chunk (contains multiple records), and will retry the chunk if upload failed. Each KDS shard can support up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. Please estimate your log volume and choose an appropriate shard number. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. Add tags if needed. Choose Create . Wait for the application pipeline turning to \"Active\" state. Create a log analytics pipeline (Light Engine) Create a log analytics pipeline Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline . Choose Instance Group as Log Source, choose Light Engine , and choose Next . Select an instance group. If you have no instance group yet, choose Create Instance Group at the top right corner, and follow the Instance Group guide to create an instance group. After that, choose Refresh and then select the newly created instance group. (Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template . Keep the default Permission grant method . (Optional) If you choose I will manually add the below required permissions after pipeline creation , continue to do the following: Choose Expand to view required permissions and copy the provided JSON policy. Go to AWS Management Console . On the left navigation pane, choose IAM , and select Policies under Access management . Choose Create Policy , choose JSON and replace all the content inside the text block. Make sure to substitute <YOUR ACCOUNT ID> with your account id. Choose Next , and then enter a name for this policy. Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your [launch template][launch-template] or [launch configuration][launch-configuration]. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon EC2 instance group as log source. Enter a Log Path to specify the location of logs you want to collect. Select a log config and then choose Next . If you do not find desired log config from the drop-down list, choose Create New , and follow instructions in Log Cong . In the Buffer section, S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Choose Next . In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. If desired, add tags. Select Create . Wait for the application pipeline turning to \"Active\" state.","title":"EKS cluster"},{"location":"implementation-guide/applications/eks-pipeline/#amazon-eks-cluster-as-log-source","text":"For Amazon Elastic Kubernetes Service (Amazon EKS) clusters, Centralized Logging with OpenSearch will generate all-in-one configuration file for customers to deploy the log agent ( Fluent Bit 1.9 ) as a DaemonSet or Sidecar. After log agent is deployed, Centralized Logging with OpenSearch will start collecting pod logs and send to Amazon OpenSearch. This article guides you to create a log pipeline that ingests logs from an EKS cluster.","title":"Amazon EKS cluster as log source"},{"location":"implementation-guide/applications/eks-pipeline/#create-log-analytics-pipeline-amazon-opensearch","text":"","title":"Create log analytics pipeline (Amazon OpenSearch)"},{"location":"implementation-guide/applications/eks-pipeline/#prerequisites","text":"Import an Amazon OpenSearch Service domain .","title":"Prerequisites"},{"location":"implementation-guide/applications/eks-pipeline/#create-log-analytics-pipeline","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline . Choose EKS Clusters as Log Source, and choose Next . Choose the AWS account in which the logs are stored. Choose an EKS Cluster. If you have no imported cluster yet, choose Import an EKS Cluster , and follow the EKS cluster guide to import an EKS cluster. After that, select the newly imported EKS cluster from the drop-down list. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon EKS cluster as log source. Enter a Log Path to specify the location of logs you want to collect. Select a log config and then choose Next . If you do not find desired log config from the drop-down list, choose Create New and follow instructions in Log Cong . Specify Index name in lowercase. In the Buffer section, choose S3 or Kinesis Data Streams . If you don't want the buffer layer, choose None . Refer to the Log Buffer for more information about choosing the appropriate buffer layer. S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. S3 Bucket Prefix AppLogs/<index-prefix>/year=%Y/month=%m/day=%d The log agent appends the prefix when delivering the log files to the S3 bucket. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Kinesis Data Streams buffer parameters Parameter Default Description Shard number <Requires input> The number of shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. Enable auto scaling No This solution monitors the utilization of Kinesis Data Streams every 5 minutes, and scale in/out the number of shards automatically. The solution will scale in/out for a maximum of 8 times within 24 hours. Maximum Shard number <Requires input> Required if auto scaling is enabled. The maximum number of shards. Important You may observe duplicate logs in OpenSearch if threshold error occurs in Kinesis Data Streams (KDS). This is because the Fluent Bit log agent uploads logs in chunk (contains multiple records), and will retry the chunk if upload failed. Each KDS shard can support up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. Please estimate your log volume and choose an appropriate shard number. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. Add tags if needed. Choose Create . Wait for the application pipeline turning to \"Active\" state.","title":"Create log analytics pipeline"},{"location":"implementation-guide/applications/eks-pipeline/#create-a-log-analytics-pipeline-light-engine","text":"","title":"Create a log analytics pipeline (Light Engine)"},{"location":"implementation-guide/applications/eks-pipeline/#create-a-log-analytics-pipeline","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline . Choose Instance Group as Log Source, choose Light Engine , and choose Next . Select an instance group. If you have no instance group yet, choose Create Instance Group at the top right corner, and follow the Instance Group guide to create an instance group. After that, choose Refresh and then select the newly created instance group. (Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template . Keep the default Permission grant method . (Optional) If you choose I will manually add the below required permissions after pipeline creation , continue to do the following: Choose Expand to view required permissions and copy the provided JSON policy. Go to AWS Management Console . On the left navigation pane, choose IAM , and select Policies under Access management . Choose Create Policy , choose JSON and replace all the content inside the text block. Make sure to substitute <YOUR ACCOUNT ID> with your account id. Choose Next , and then enter a name for this policy. Attach the policy to your EC2 instance profile to grant the log agent permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group. If needed, you can follow the documentation to update your [launch template][launch-template] or [launch configuration][launch-configuration]. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon EC2 instance group as log source. Enter a Log Path to specify the location of logs you want to collect. Select a log config and then choose Next . If you do not find desired log config from the drop-down list, choose Create New , and follow instructions in Log Cong . In the Buffer section, S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Choose Next . In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. If desired, add tags. Select Create . Wait for the application pipeline turning to \"Active\" state.","title":"Create a log analytics pipeline"},{"location":"implementation-guide/applications/include-supported-app-logs/","text":"The table lists the log formats supported by each log source. For more information about how to create log ingestion for each log format, refer to Log Config . Log Format EC2 Instance Group EKS Cluster Amazon S3 Syslog Nginx Yes Yes Yes No Apache HTTP Server Yes Yes Yes No JSON Yes Yes Yes Yes Single-line Text Yes Yes Yes Yes Multi-line Text Yes Yes Yes No Multi-line Text (Spring Boot) Yes Yes Yes No Syslog RFC5424/RFC3164 No No No Yes Syslog Custom No No No Yes","title":"Include supported app logs"},{"location":"implementation-guide/applications/s3-pipeline/","text":"Amazon S3 as log source For Amazon S3, Centralized Logging with OpenSearch will ingest logs in a specified S3 location continuously or perform one-time ingestion. You can also filter logs based on S3 prefix or parse logs with custom Log Config. This article guides you to create a log pipeline that ingests logs from an S3 bucket. Prerequisites Import an Amazon OpenSearch Service domain . Create log analytics pipeline Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline . Choose Amazon S3 as Log Source, and choose Next . Choose the Amazon S3 bucket where your logs are stored. If needed,enter Prefix filter , which is optional. Choose Ingestion mode based on your need. If you want to ingest the log continuously, select On-going ; if you only need to ingest the log once, select One-time . Specify Compression format if your log files are compressed, and choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon S3 as log source. Select a log config. If you do not find the desired log config from the drop-down list, choose Create New . Refer to Log Config for more information. Choose Next . Specify Index name in lowercase. In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. Add tags if needed. Choose Create . Wait for the application pipeline turning to \"Active\" state.","title":"S3"},{"location":"implementation-guide/applications/s3-pipeline/#amazon-s3-as-log-source","text":"For Amazon S3, Centralized Logging with OpenSearch will ingest logs in a specified S3 location continuously or perform one-time ingestion. You can also filter logs based on S3 prefix or parse logs with custom Log Config. This article guides you to create a log pipeline that ingests logs from an S3 bucket.","title":"Amazon S3 as log source"},{"location":"implementation-guide/applications/s3-pipeline/#prerequisites","text":"Import an Amazon OpenSearch Service domain .","title":"Prerequisites"},{"location":"implementation-guide/applications/s3-pipeline/#create-log-analytics-pipeline","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline . Choose Amazon S3 as Log Source, and choose Next . Choose the Amazon S3 bucket where your logs are stored. If needed,enter Prefix filter , which is optional. Choose Ingestion mode based on your need. If you want to ingest the log continuously, select On-going ; if you only need to ingest the log once, select One-time . Specify Compression format if your log files are compressed, and choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with Amazon S3 as log source. Select a log config. If you do not find the desired log config from the drop-down list, choose Create New . Refer to Log Config for more information. Choose Next . Specify Index name in lowercase. In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. Add tags if needed. Choose Create . Wait for the application pipeline turning to \"Active\" state.","title":"Create log analytics pipeline"},{"location":"implementation-guide/applications/syslog-pipeline/","text":"Syslog as log source Centralized Logging with OpenSearch will collect syslog logs through UDP or TCP protocol. This article guides you to create a log pipeline that ingests logs from a syslog endpoint. Prerequisites Import an Amazon OpenSearch Service domain . Create log analytics pipeline Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline . Choose Syslog Endpoint as Log Source. Select UDP or TCP with custom port number. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with syslog as log source. Select a log config. If you do not find the desired log config from the drop-down list, choose Create New . Refer to Log Config for more information. Choose Next . Specify Index name in lowercase. In the Buffer section, choose S3 or Kinesis Data Streams . If you don't want the buffer layer, choose None . Refer to the Log Buffer for more information about choosing the appropriate buffer layer. S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. S3 Bucket Prefix AppLogs/<index-prefix>/year=%Y/month=%m/day=%d The log agent appends the prefix when delivering the log files to the S3 bucket. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Kinesis Data Streams buffer parameters Parameter Default Description Shard number <Requires input> The number of shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. Enable auto scaling No This solution monitors the utilization of Kinesis Data Streams every 5 minutes, and scale in/out the number of shards automatically. The solution will scale in/out for a maximum of 8 times within 24 hours. Maximum Shard number <Requires input> Required if auto scaling is enabled. The maximum number of shards. Important You may observe duplicate logs in OpenSearch if threshold error occurs in Kinesis Data Streams (KDS). This is because the Fluent Bit log agent uploads logs in chunk (contains multiple records), and will retry the chunk if upload failed. Each KDS shard can support up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. Please estimate your log volume and choose an appropriate shard number. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. Add tags if needed. Choose Create . Wait for the application pipeline turning to \"Active\" state.","title":"Syslog"},{"location":"implementation-guide/applications/syslog-pipeline/#syslog-as-log-source","text":"Centralized Logging with OpenSearch will collect syslog logs through UDP or TCP protocol. This article guides you to create a log pipeline that ingests logs from a syslog endpoint.","title":"Syslog as log source"},{"location":"implementation-guide/applications/syslog-pipeline/#prerequisites","text":"Import an Amazon OpenSearch Service domain .","title":"Prerequisites"},{"location":"implementation-guide/applications/syslog-pipeline/#create-log-analytics-pipeline","text":"Sign in to the Centralized Logging with OpenSearch Console. In the left sidebar, under Log Analytics Pipelines , choose Application Log . Choose Create a pipeline . Choose Syslog Endpoint as Log Source. Select UDP or TCP with custom port number. Choose Next . You have created a log source for the log analytics pipeline. Now you are ready to make further configurations for the log analytics pipeline with syslog as log source. Select a log config. If you do not find the desired log config from the drop-down list, choose Create New . Refer to Log Config for more information. Choose Next . Specify Index name in lowercase. In the Buffer section, choose S3 or Kinesis Data Streams . If you don't want the buffer layer, choose None . Refer to the Log Buffer for more information about choosing the appropriate buffer layer. S3 buffer parameters Parameter Default Description S3 Bucket A log bucket will be created by the solution. You can also select a bucket to store the log data. S3 Bucket Prefix AppLogs/<index-prefix>/year=%Y/month=%m/day=%d The log agent appends the prefix when delivering the log files to the S3 bucket. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. For more information, see Data Delivery Frequency . Buffer interval 60 seconds The maximum interval of the log agent to deliver logs to S3. For more information, see Data Delivery Frequency . Compression for data records Gzip The log agent compresses records before delivering them to the S3 bucket. Kinesis Data Streams buffer parameters Parameter Default Description Shard number <Requires input> The number of shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. Enable auto scaling No This solution monitors the utilization of Kinesis Data Streams every 5 minutes, and scale in/out the number of shards automatically. The solution will scale in/out for a maximum of 8 times within 24 hours. Maximum Shard number <Requires input> Required if auto scaling is enabled. The maximum number of shards. Important You may observe duplicate logs in OpenSearch if threshold error occurs in Kinesis Data Streams (KDS). This is because the Fluent Bit log agent uploads logs in chunk (contains multiple records), and will retry the chunk if upload failed. Each KDS shard can support up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. Please estimate your log volume and choose an appropriate shard number. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Enable Alarms if needed and select an exiting SNS topic. If you choose Create a new SNS topic , please provide a name and an email address for the new SNS topic. Add tags if needed. Choose Create . Wait for the application pipeline turning to \"Active\" state.","title":"Create log analytics pipeline"},{"location":"implementation-guide/architecture-details/services-in-this-solution/","text":"The following AWS services are included in this solution: AWS service Description Amazon CloudFront To distribute the frontend web UI assets. Amazon S3 To store the static web assets (frontend user interface), and also uses it as a data buffer for log shipping. Amazon Cognito To authenticate users (in AWS Regions). AWS AppSync To provide the backend GraphQL APIs. Amazon DynamoDB To store the solution related information as backend database. AWS Lambda To interact with other AWS Services to process core logic of managing log pipelines or log agents, and obtain information updated in DynamoDB tables. AWS Step Functions To orchestrate on-demand AWS CloudFormation deployment of a set of predefined stacks for log pipeline management. AWS CloudFormation To provision the AWS resources for the modules of pipelines and the solution web console. AWS Systems Manager To manage log agents for collecting logs from application servers, such as installing log agents (Fluent Bit) for application servers. Amazon Kinesis Data Streams To subscribe to logs from a CloudWatch Log Group or as a data buffer for log shipping, and then initiate the Log Processor Lambda to run. Amazon Kinesis Data Firehose To subscribe the logs from CloudWatch Log Group and then put logs into Amazon S3. Amazon SQS To receive Amazon S3 Event Notifications and then initiate the Log Processor Lambda to run.","title":"AWS services in this solution"},{"location":"implementation-guide/architecture-details/solution-components/","text":"The solution consists of the following components: Domain Management This solution uses Amazon OpenSearch Service as the underlying engine to store and analyze logs. You can import an existing Amazon OpenSearch Service domain for log ingestion, and provide an access proxy to the Amazon OpenSearch Service dashboards within VPC. Moreover, you can set up recommended Amazon CloudWatch alarms for Amazon OpenSearch Service. Analytics Pipelines A log pipeline includes a series of log processing steps, including collecting logs from sources, processing and sending them to Amazon OpenSearch Service for further analysis. Centralized Logging with OpenSearch supports AWS Service log ingestion and server-side application log ingestion. Service Log Pipeline This solution supports out of the box log analysis for AWS service logs, such as Amazon S3 access logs, and ELB access logs. The component is designed to reduce the complexities of building log analytics pipelines for different AWS services with different formats. Application Log Pipeline This solution supports out of the box log analysis for application logs, such as Nginx/Apache logs or general application logs via regex parser. The component uses Fluent Bit as the underlying log agent to collect logs from application servers, and allows you to easily install log agent and monitor the agent health via System Manager.","title":"Solution components"},{"location":"implementation-guide/architecture-details/solution-components/#domain-management","text":"This solution uses Amazon OpenSearch Service as the underlying engine to store and analyze logs. You can import an existing Amazon OpenSearch Service domain for log ingestion, and provide an access proxy to the Amazon OpenSearch Service dashboards within VPC. Moreover, you can set up recommended Amazon CloudWatch alarms for Amazon OpenSearch Service.","title":"Domain Management"},{"location":"implementation-guide/architecture-details/solution-components/#analytics-pipelines","text":"A log pipeline includes a series of log processing steps, including collecting logs from sources, processing and sending them to Amazon OpenSearch Service for further analysis. Centralized Logging with OpenSearch supports AWS Service log ingestion and server-side application log ingestion.","title":"Analytics Pipelines"},{"location":"implementation-guide/architecture-details/solution-components/#service-log-pipeline","text":"This solution supports out of the box log analysis for AWS service logs, such as Amazon S3 access logs, and ELB access logs. The component is designed to reduce the complexities of building log analytics pipelines for different AWS services with different formats.","title":"Service Log Pipeline"},{"location":"implementation-guide/architecture-details/solution-components/#application-log-pipeline","text":"This solution supports out of the box log analysis for application logs, such as Nginx/Apache logs or general application logs via regex parser. The component uses Fluent Bit as the underlying log agent to collect logs from application servers, and allows you to easily install log agent and monitor the agent health via System Manager.","title":"Application Log Pipeline"},{"location":"implementation-guide/architecture-overview/well-architected-pillars/","text":"This solution was designed with best practices from the AWS Well-Architected Framework which helps customers design and operate reliable, secure, efficient, and cost-effective workloads in the cloud. This section describes how the design principles and best practices of the Well-Architected Framework were applied when building this solution. Operational excellence This section describes how the principles and best practices of the operational excellence pillar were applied when designing this solution. The solution pushes metrics, logs and traces to Amazon CloudWatch at various stages to provide observability into the infrastructure, Elastic load balancer, Amazon ECS cluster, Lambda functions, Step Function workflow and the rest of the solution components. This solution also creates the CloudWatch dashboards for each pipeline monitoring. Security This section describes how the principles and best practices of the security pillar were applied when designing this solution. The web console users are authenticated and authorized with Amazon Cognito or OpenID Connect. All inter-service communications use AWS IAM roles. All roles used by the solution follows least-privilege access. That is, it only contains minimum permissions required so the service can function properly. Reliability This section describes how the principles and best practices of the reliability pillar were applied when designing this solution. Using AWS serverless services wherever possible (for example, AWS AppSync, Amazon DynamoDB, AWS Lambda, AWS Step Functions, Amazon S3, and Amazon SQS) to ensure high availability and recovery from service failure. Configuration management content of the solution is stored in Amazon DynamoDB, all of your data is stored on solid-state disks (SSDs) and is automatically replicated across multiple Availability Zones (AZs) in an AWS Region, providing built-in high availability and data durability. Performance efficiency This section describes how the principles and best practices of the performance efficiency pillar were applied when designing this solution. The ability to launch this solution in any Region that supports AWS services in this solution such as: Amazon S3, Amazon ECS, Elastic load balancer. Using serverless architecture removes the need for you to run and maintain physical servers for traditional compute activities. Automatically testing and deploying this solution daily. Reviewing this solution by solution architects and subject matter experts for areas to experiment and improve. Cost optimization This section describes how the principles and best practices of the cost optimization pillar were applied when designing this solution. Use Autoscaling Group so that the compute costs are only related to how much data is ingested and processed. Using serverless services such as Amazon S3, Amazon DynamoDB, AWS Lambda, etc, so that customers only get charged for what they use. Sustainability This section describes how the principles and best practices of the sustainability pillar were applied when designing this solution. The solution\u2018s serverless design (using Amazon Kinesis Data Streams, Amazon S3, AWS Lambda) and the use of managed services (such as Amazon ECS) are aimed at reducing carbon footprint compared to the footprint of continually operating on-premises servers.","title":"AWS Well-Architected pillars"},{"location":"implementation-guide/architecture-overview/well-architected-pillars/#operational-excellence","text":"This section describes how the principles and best practices of the operational excellence pillar were applied when designing this solution. The solution pushes metrics, logs and traces to Amazon CloudWatch at various stages to provide observability into the infrastructure, Elastic load balancer, Amazon ECS cluster, Lambda functions, Step Function workflow and the rest of the solution components. This solution also creates the CloudWatch dashboards for each pipeline monitoring.","title":"Operational excellence"},{"location":"implementation-guide/architecture-overview/well-architected-pillars/#security","text":"This section describes how the principles and best practices of the security pillar were applied when designing this solution. The web console users are authenticated and authorized with Amazon Cognito or OpenID Connect. All inter-service communications use AWS IAM roles. All roles used by the solution follows least-privilege access. That is, it only contains minimum permissions required so the service can function properly.","title":"Security"},{"location":"implementation-guide/architecture-overview/well-architected-pillars/#reliability","text":"This section describes how the principles and best practices of the reliability pillar were applied when designing this solution. Using AWS serverless services wherever possible (for example, AWS AppSync, Amazon DynamoDB, AWS Lambda, AWS Step Functions, Amazon S3, and Amazon SQS) to ensure high availability and recovery from service failure. Configuration management content of the solution is stored in Amazon DynamoDB, all of your data is stored on solid-state disks (SSDs) and is automatically replicated across multiple Availability Zones (AZs) in an AWS Region, providing built-in high availability and data durability.","title":"Reliability"},{"location":"implementation-guide/architecture-overview/well-architected-pillars/#performance-efficiency","text":"This section describes how the principles and best practices of the performance efficiency pillar were applied when designing this solution. The ability to launch this solution in any Region that supports AWS services in this solution such as: Amazon S3, Amazon ECS, Elastic load balancer. Using serverless architecture removes the need for you to run and maintain physical servers for traditional compute activities. Automatically testing and deploying this solution daily. Reviewing this solution by solution architects and subject matter experts for areas to experiment and improve.","title":"Performance efficiency"},{"location":"implementation-guide/architecture-overview/well-architected-pillars/#cost-optimization","text":"This section describes how the principles and best practices of the cost optimization pillar were applied when designing this solution. Use Autoscaling Group so that the compute costs are only related to how much data is ingested and processed. Using serverless services such as Amazon S3, Amazon DynamoDB, AWS Lambda, etc, so that customers only get charged for what they use.","title":"Cost optimization"},{"location":"implementation-guide/architecture-overview/well-architected-pillars/#sustainability","text":"This section describes how the principles and best practices of the sustainability pillar were applied when designing this solution. The solution\u2018s serverless design (using Amazon Kinesis Data Streams, Amazon S3, AWS Lambda) and the use of managed services (such as Amazon ECS) are aimed at reducing carbon footprint compared to the footprint of continually operating on-premises servers.","title":"Sustainability"},{"location":"implementation-guide/aws-services/","text":"Build AWS Service Log Analytics Pipelines Centralized Logging with OpenSearch supports ingesting AWS service logs into Amazon OpenSearch Service through log analytics pipelines, which you can build using the Centralized Logging with OpenSearch web console or via a standalone CloudFormation template . Centralized Logging with OpenSearch reads the data source, parse, cleanup/enrich and ingest logs into Amazon OpenSearch Service domains for analysis. Moreover, the solution provides templated dashboards to facilitate log visualization. Amazon OpenSearch Service is suitable for real-time log analytics and frequent queries and has full-text search capability. As of release 2.1.0, the solution starts to support log ingestion into Light Engine, which is suitable for non real-time log analytics and infrequent queries and has SQL-like search capability. The feature is supported by Amazon CloudFront logs, Application Load Balancing logs, and AWS WAF logs. Important AWS managed services must be in the same region as Centralized Logging with OpenSearch. To ingest logs from different AWS regions, we recommend using S3 cross-region replication . The solution will rotate the index on a daily basis, and cannot be adjusted. Supported AWS Services Most of AWS managed services output logs to Amazon CloudWatch Logs, Amazon S3, Amazon Kinesis Data Streams or Amazon Kinesis Firehose. The following table lists the supported AWS services and the corresponding features. AWS Service Log Type Log Location Automatic Ingestion Built-in Dashboard Amazon CloudTrail N/A S3 Yes Yes Amazon S3 Access logs S3 Yes Yes Amazon RDS/Aurora MySQL Logs CloudWatch Logs Yes Yes Amazon CloudFront Standard access logs S3 Yes Yes Application Load Balancer Access logs S3 Yes Yes AWS WAF Web ACL logs S3 Yes Yes AWS Lambda N/A CloudWatch Logs Yes Yes Amazon VPC Flow logs S3 Yes Yes AWS Config N/A S3 Yes Yes Automatic Ingestion : The solution detects the log location of the resource automatically and then reads the logs. Built-in Dashboard : An out-of-box dashboard for the specified AWS service. The solution will automatically ingest a dashboard into the Amazon OpenSearch Service. Most of supported AWS services in Centralized Logging with OpenSearch offers built-in dashboard when creating the log analytics pipelines. You go to the OpenSearch Dashboards to view the dashboards after the pipeline being provisioned. In this chapter, you will learn how to create log ingestion and dashboards for the following AWS services: Amazon CloudTrail Amazon S3 Amazon RDS/Aurora Amazon CloudFront AWS Lambda Elastic Load Balancing AWS WAF Amazon VPC AWS Config Cross-region log ingestion When you deploy Centralized Logging with OpenSearch in one Region, the solution allows you to process service logs from another Region. Note For Amazon RDS/Aurora and AWS Lambda service logs, this feature is not supported. Important The Region where the service resides is referred to as \u201cSource Region\u201d, while the Region where the Centralized Logging with OpenSearch console is deployed as \u201cLogging Region\u201d. For Amazon CloudTrail, you can create a new trail which send logs into a S3 bucket in the Logging Region, and you can find the CloudTrail in the list. To learn how to create a new trail, please refer to Creating a trail . For other services with logs located in S3 buckets, you can manually transfer logs (for example, using S3 Cross-Region Replication feature) to the Logging Region S3 bucket. You can follow the steps below to implement Cross-Region Logging: Set the service log location in another Region to be the Logging Region (such as Amazon WAF), or automatically copy logs from the Source Region to the Logging Region using Cross-Region Replication (CRR) . In the solution console, choose AWS Service Log in the left navigation pane, and choose Create a pipeline . In the Select an AWS Service area, choose a service in the list, and choose Next . In Creation Method , choose Manual , then enter the resource name and S3 log location parameter, and choose Next . Set OpenSearch domain and Log Lifecycle as needed, and choose Next . Add tags if you need, and choose Next to create the pipeline. Then you can use the OpenSearch dashboard to discover logs and view dashboards.","title":"Overview"},{"location":"implementation-guide/aws-services/#build-aws-service-log-analytics-pipelines","text":"Centralized Logging with OpenSearch supports ingesting AWS service logs into Amazon OpenSearch Service through log analytics pipelines, which you can build using the Centralized Logging with OpenSearch web console or via a standalone CloudFormation template . Centralized Logging with OpenSearch reads the data source, parse, cleanup/enrich and ingest logs into Amazon OpenSearch Service domains for analysis. Moreover, the solution provides templated dashboards to facilitate log visualization. Amazon OpenSearch Service is suitable for real-time log analytics and frequent queries and has full-text search capability. As of release 2.1.0, the solution starts to support log ingestion into Light Engine, which is suitable for non real-time log analytics and infrequent queries and has SQL-like search capability. The feature is supported by Amazon CloudFront logs, Application Load Balancing logs, and AWS WAF logs. Important AWS managed services must be in the same region as Centralized Logging with OpenSearch. To ingest logs from different AWS regions, we recommend using S3 cross-region replication . The solution will rotate the index on a daily basis, and cannot be adjusted.","title":"Build AWS Service Log Analytics Pipelines"},{"location":"implementation-guide/aws-services/#supported-aws-services","text":"Most of AWS managed services output logs to Amazon CloudWatch Logs, Amazon S3, Amazon Kinesis Data Streams or Amazon Kinesis Firehose. The following table lists the supported AWS services and the corresponding features. AWS Service Log Type Log Location Automatic Ingestion Built-in Dashboard Amazon CloudTrail N/A S3 Yes Yes Amazon S3 Access logs S3 Yes Yes Amazon RDS/Aurora MySQL Logs CloudWatch Logs Yes Yes Amazon CloudFront Standard access logs S3 Yes Yes Application Load Balancer Access logs S3 Yes Yes AWS WAF Web ACL logs S3 Yes Yes AWS Lambda N/A CloudWatch Logs Yes Yes Amazon VPC Flow logs S3 Yes Yes AWS Config N/A S3 Yes Yes Automatic Ingestion : The solution detects the log location of the resource automatically and then reads the logs. Built-in Dashboard : An out-of-box dashboard for the specified AWS service. The solution will automatically ingest a dashboard into the Amazon OpenSearch Service. Most of supported AWS services in Centralized Logging with OpenSearch offers built-in dashboard when creating the log analytics pipelines. You go to the OpenSearch Dashboards to view the dashboards after the pipeline being provisioned. In this chapter, you will learn how to create log ingestion and dashboards for the following AWS services: Amazon CloudTrail Amazon S3 Amazon RDS/Aurora Amazon CloudFront AWS Lambda Elastic Load Balancing AWS WAF Amazon VPC AWS Config","title":"Supported AWS Services"},{"location":"implementation-guide/aws-services/#cross-region-log-ingestion","text":"When you deploy Centralized Logging with OpenSearch in one Region, the solution allows you to process service logs from another Region. Note For Amazon RDS/Aurora and AWS Lambda service logs, this feature is not supported. Important The Region where the service resides is referred to as \u201cSource Region\u201d, while the Region where the Centralized Logging with OpenSearch console is deployed as \u201cLogging Region\u201d. For Amazon CloudTrail, you can create a new trail which send logs into a S3 bucket in the Logging Region, and you can find the CloudTrail in the list. To learn how to create a new trail, please refer to Creating a trail . For other services with logs located in S3 buckets, you can manually transfer logs (for example, using S3 Cross-Region Replication feature) to the Logging Region S3 bucket. You can follow the steps below to implement Cross-Region Logging: Set the service log location in another Region to be the Logging Region (such as Amazon WAF), or automatically copy logs from the Source Region to the Logging Region using Cross-Region Replication (CRR) . In the solution console, choose AWS Service Log in the left navigation pane, and choose Create a pipeline . In the Select an AWS Service area, choose a service in the list, and choose Next . In Creation Method , choose Manual , then enter the resource name and S3 log location parameter, and choose Next . Set OpenSearch domain and Log Lifecycle as needed, and choose Next . Add tags if you need, and choose Next to create the pipeline. Then you can use the OpenSearch dashboard to discover logs and view dashboards.","title":"Cross-region log ingestion"},{"location":"implementation-guide/aws-services/cloudfront/","text":"Amazon CloudFront Logs CloudFront standard logs provide detailed records about every request made to a distribution. You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The CloudFront logging bucket must be the same region as the Centralized Logging with OpenSearch solution. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Create log ingestion (Amazon OpenSearch for log analytics) Using the Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Amazon CloudFront . Choose Amazon OpenSearch , and choose Next . Under Specify settings , choose Automatic or Manual for CloudFront logs enabling . The automatic mode will detect the CloudFront log location automatically. For Automatic mode , choose the CloudFront distribution and Log Type from the dropdown lists. For Standard Log, the solution will automatically detect the log location if logging is enabled. For Real-time log, the solution will prompt you for confirmation to create or replace CloudFront real-time log configuration. For Manual mode , enter the CloudFront Distribution ID and CloudFront Standard Log location . (Note that CloudFront real-time log is not supported in Manual mode) (Optional) If you are ingesting CloudFront logs from another account, select a linked account from the Account dropdown list first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the CloudFront distribution ID. In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. In the Log processor settings section, choose Log processor type , and then Next . Add tags if needed. Choose Create . Using the CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - CloudFront Standard Log Ingestion template in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional input> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Plugins <Optional> List of plugins delimited by comma. Leave it blank if there are no available plugins to use. Valid inputs are user_agent , geo_ip . Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Total Requests log event Displays the total number of viewer requests received by the Amazon CloudFront, for all HTTP methods and for both HTTP and HTTPS requests. Edge Locations x-edge-location Shows a pie chart representing the proportion of the locations of CloudFront edge servers. Request History log event Presents a bar chart that displays the distribution of events over time. Unique Visitors c-ip Displays unique visitors identified by client IP address. Cache Hit Rate sc-bytes Shows the proportion of your viewer requests that are served directly from the CloudFront cache instead of going to your origin servers for content. Result Type x-edge-response-result-type Shows the percentage of hits, misses, and errors to the total viewer requests for the selected CloudFront distribution: Hit \u2013 A viewer request for which the object is served from a CloudFront edge cache. In access logs, these are requests for which the value of x-edge-response-result-type is Hit Miss \u2013 A viewer request for which the object isn't currently in an edge cache, so CloudFront must get the object from your origin. In access logs, these are requests for which the value of x-edge-response-result-type is Miss. Error \u2013 A viewer request that resulted in an error, so CloudFront didn't serve the object. In access logs, these are requests for which the value of x-edge-response-result-type is Error, LimitExceeded, or CapacityExceeded. The chart does not include refresh hits\u2014requests for objects that are in the edge cache but that have expired. In access logs, refresh hits are requests for which the value of x-edge-response-result-type is RefreshHit. Top Miss URI cs-uri-stem cs-method Shows top 10 of the requested objects that are not in the cache. Bandwidth cs-bytes sc-bytes Provides insights into data transfer activities from the locations of CloudFront edge. Bandwidth History cs-bytes sc-bytes Shows the historical trend of the data transfer activities from the locations of CloudFront edge. Top Client IPs c-ip Provides the top 10 IP address accessing your Amazon CloudFront. Status Code Count sc-status Displays the count of requests made to the Amazon CloudFront, grouped by HTTP status codes(e.g., 200, 404, 403, etc.). Status History @timestamp sc-status Shows the historical trend of HTTP status codes returned by the Amazon CloudFront over a specific period of time. Status Code sc-status Identifies the users or IAM roles responsible for changes to EC2 resources, assisting in accountability and tracking of modifications. Average Time Taken time-taken This visualization calculates and presents the average time taken for various operations in the Amazon CloudFront (e.g., average time for GET, PUT requests, etc.). Average Time History time-taken time-to-first-byte @timestamp Shows the historical trend of the average time taken for various operations in the Amazon CloudFront. Http Method cs-method Displays the count of requests made to the Amazon CloudFront using a pie chart, grouped by http request method names (e.g., POST, GET, HEAD, etc.). Average Time To First Byte time-to-first-byte Provides the average time taken in seconds by the origin server to respond back with the first byte of the response. Top Request URIs cs-uri-stem cs-method Provides the top 10 request URIs accessing your CloudFront. Top User Agents cs-user-agent Provides the top 10 user agents accessing your CloudFront. Edge Location Heatmap x-edge-location x-edge-result-type Shows a heatmap representing the result type of each edge locations. Top Referers cs-referer Top 10 referers with the Amazon CloudFront access. Top Countries or Regions c_country Top 10 countries with the Amazon CloudFront access. Sample dashboard You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard. Create log ingestion (Light Engine for log analytics) Using the Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Amazon CloudFront . Choose Light Engine , Choose Next . Under Specify settings , choose Automatic or Manual for CloudFront logs enabling . The automatic mode will detect the CloudFront log location automatically. For Automatic mode , choose the CloudFront distribution and Log Type from the dropdown lists. For Standard Log, the solution will automatically detect the log location if logging is enabled. For Manual mode , enter the CloudFront Distribution ID and CloudFront Standard Log location . (Note that CloudFront real-time log is not supported in Manual mode) (Optional) If you are ingesting CloudFront logs from another account, select a linked account from the Account dropdown list first. Choose Next . Choose Log Processing Enriched fields if needed. The available plugins are location and OS/User Agent . Enabling rich fields increases data processing latency and processing costs. By default, it is not selected. In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . If desired, add tags. Select Create . Using the CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - CloudFront Standard Log Ingestion template in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameters for Pipeline settings Parameter Default Description Pipeline Id <Requires input> The unique identifier for the pipeline is essential if you need to create multiple ALB pipelines and write different ALB logs into separate tables. To ensure uniqueness, you can generate a unique pipeline identifier using uuidgenerator . Staging Bucket Prefix AWSLogs/CloudFrontLogs The storage directory for logs in the temporary storage area should ensure the uniqueness and non-overlapping of the Prefix for different pipelines. Parameters for Destination settings Parameters Default Description Centralized Bucket Name <Requires input> Centralized s3 bucket name. For example, centralized-logging-bucket. Centralized Bucket Prefix datalake Centralized bucket prefix. By default, the data base location is s3://{Centralized Bucket Name}/{Centralized Bucket Prefix}/amazon_cl_centralized. Centralized Table Name CloudFront Table name for writing data to the centralized database. You can modify it if needed. Enrichment Plugins <Optional input> The available plugins to choose from are location and OS/User Agent . Enabling rich fields will increase data processing latency and processing costs, it is not selected by default. Parameters for Scheduler settings Parameters Default Description LogProcessor Schedule Expression rate(5 minutes) Task scheduling expression for performing log processing, with a default value of executing the LogProcessor every 5 minutes. Configuration for reference \u3002 LogMerger Schedule Expression cron(0 1 * ? ) \u6267ask scheduling expression for performing log merging, with a default value of executing the LogMerger at 1 AM every day. Configuration for reference \u3002 LogArchive Schedule Expression cron(0 2 * ? ) Task scheduling expression for performing log archiving, with a default value of executing the LogArchive at 2 AM every day. Configuration for reference \u3002 Age to Merge 7 Small file retention days, with a default value of 7, indicates that logs older than 7 days will be merged into small files. It can be adjusted as needed. Age to Archive 30 Log retention days, with a default value of 30, indicates that data older than 30 days will be archived and deleted. It can be adjusted as needed. Parameters for Notification settings Parameters Default Description Notification Service SNS Notification method for alerts. If your main stack is using China, you can only choose the SNS method. If your main stack is using Global, you can choose either the SNS or SES method. Recipients <Requires Input> Alert notification: If the Notification Service is SNS, enter the SNS Topic ARN here, ensuring that you have the necessary permissions. If the Notification Service is SES, enter the email addresses separated by commas here, ensuring that the email addresses are already Verified Identities in SES. The adminEmail provided during the creation of the main stack will receive a verification email by default. Parameters for Dashboard settings Parameters Default Description Import Dashboards FALSE Whether to import the Dashboard into Grafana, with a default value of false. If set to true, you must provide the Grafana URL and Grafana Service Account Token.\u3002 Grafana URL <Requires Input> Grafana access URL\uff0cfor example: https://alb-72277319.us-west-2.elb.amazonaws.com\u3002 Grafana Service Account Token <Requires Input> Grafana Service Account Token\uff1aService Account Token created in Grafana\u3002 Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View Dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Filters Filters The following data can be filtered by query filter conditions. Total Requests log event Displays the total number of viewer requests received by the Amazon CloudFront, for all HTTP methods and for both HTTP and HTTPS requests. Unique Visitors c-ip Displays unique visitors identified by client IP address. Requests History log event Presents a bar chart that displays the distribution of events over time. Request By Edge Location x-edge-location Shows a pie chart representing the proportion of the locations of CloudFront edge servers. HTTP Status Code sc-status Displays the count of requests made to the Amazon CloudFront, grouped by HTTP status codes (e.g., 200, 404, 403, etc.). Status Code History sc-status Shows the historical trend of HTTP status codes returned by the Amazon CloudFront over a specific period of time. Status Code Pie sc-status Represents the distribution of requests based on different HTTP status codes using a pie chart. Average Processing Time time-taken time-to-first-byte This visualization calculates and presents the average time taken for various operations in the Amazon CloudFront (e.g., average time for GET, PUT requests, etc.). Avg. Processing Time History time-taken time-to-first-byte Shows the historical trend of the average time taken for various operations in the Amazon CloudFront. Avg. Processing Time History time-taken time-to-first-byte Shows the historical trend of the average time taken for various operations in the Amazon CloudFront. HTTP Method cs-method Displays the count of requests made to the Amazon CloudFront using a pie chart, grouped by HTTP request method names (e.g., POST, GET, HEAD, etc.). Total Bytes cs-bytes sc-bytes Provides insights into data transfer activities, including the total bytes transferred. Response Bytes History cs-bytes sc-bytes Displays the historical trend of the received bytes, send bytes. Edge Response Type x-edge-response-result-type Shows the percentage of hits, misses, and errors to the total viewer requests for the selected CloudFront distribution: - Hit \u2013 A viewer request for which the object is served from a CloudFront edge cache. In access logs, these are requests for which the value of x-edge-response-result-type is Hit. - Miss \u2013 A viewer request for which the object isn't currently in an edge cache, so CloudFront must get the object from your origin. In access logs, these are requests for which the value of x-edge-response-result-type is Miss. - Error \u2013 A viewer request that resulted in an error, so CloudFront didn't serve the object. In access logs, these are requests for which the value of x-edge-response-result-type is Error, LimitExceeded, or CapacityExceeded. The chart does not include refresh hits\u2014requests for objects that are in the edge cache but that have expired. In access logs, refresh hits are requests for which the value of x-edge-response-result-type is RefreshHit. Requests / Origin Requests log event Displays the number of requests made to CloudFront and the number of requests back to the origin. Requests / Origin Requests Latency log event time-taken Displays the request latency from the client to CloudFront and the request latency back to the origin. Top 20 URLs with most requests log event Top 20 URLs based on the number of requests. Requests 3xx / 4xx / 5xx error rate log event sc-status Displays the ratio of 3xx/4xx/5xx status codes from the client to CloudFront. Origin Requests 3xx / 4xx / 5xx error rate log event sc-status x-edge-detailed-result-type Display the proportion of 3xx/4xx/5xx status codes returned to the origin. Requests 3xx / 4xx / 5xx error latency log event sc-status time-taken Displays the latency from the client to CloudFront for 3xx/4xx/5xx status codes. Origin Requests 3xx / 4xx / 5xx error latency log event sc-status x-edge-detailed-result-type time-taken Displays the delay in returning to the source 3xx/4xx/5xx status code. Response Latency (>= 1sec) rate log event time-taken Display the proportion of delay above 1s. Bandwidth sc-bytes Displays the bandwidth from the client to CloudFront and the bandwidth back to the origin. Data transfer sc-bytes Display the response traffic. Top 20 URLs with most traffic cs-uri-stem sc-bytes Top 20 URLs calculated by traffic. Cache hit rate (calculated using requests) log event x-edge-result-type Displays the cache hit ratio calculated by the number of requests. Cache hit rate (calculated using bandwidth) log event sc-bytes x-edge-result-type Displays the cache hit ratio calculated by bandwidth. Cache Result log event x-edge-result-type Displays the number of requests of various x-edge-result-types, such as the number of requests that hit the cache and the number of requests that missed the cache. Cache Result Latency log event sc-bytes x-edge-result-type Displays the request latency of various x-edge-result-types, such as the request latency that hits the cache and the request latency that misses the cache. Requests by OS ua_os Displays the count of requests made to the ALB, grouped by user agent OS. Requests by Device ua_device Displays the count of requests made to the ALB, grouped by user agent device. Requests by Browser ua_browser Displays the count of requests made to the ALB, grouped by user agent browser. Requests by Category ua_category Displays the count of category made to the ALB, grouped by user agent category (e.g., PC, Mobile, Tablet, etc.). Requests by Countries or Regions geo_iso_code Displays the count of requests made to the ALB (grouped by the corresponding country or region resolved by the client IP). Top Countries or Regions geo_country Top 10 countries with the ALB Access. Top Cities geo_city Top 10 cities with ALB Access.","title":"Amazon CloudFront"},{"location":"implementation-guide/aws-services/cloudfront/#amazon-cloudfront-logs","text":"CloudFront standard logs provide detailed records about every request made to a distribution. You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The CloudFront logging bucket must be the same region as the Centralized Logging with OpenSearch solution. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"Amazon CloudFront Logs"},{"location":"implementation-guide/aws-services/cloudfront/#create-log-ingestion-amazon-opensearch-for-log-analytics","text":"","title":"Create log ingestion (Amazon OpenSearch for log analytics)"},{"location":"implementation-guide/aws-services/cloudfront/#using-the-console","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Amazon CloudFront . Choose Amazon OpenSearch , and choose Next . Under Specify settings , choose Automatic or Manual for CloudFront logs enabling . The automatic mode will detect the CloudFront log location automatically. For Automatic mode , choose the CloudFront distribution and Log Type from the dropdown lists. For Standard Log, the solution will automatically detect the log location if logging is enabled. For Real-time log, the solution will prompt you for confirmation to create or replace CloudFront real-time log configuration. For Manual mode , enter the CloudFront Distribution ID and CloudFront Standard Log location . (Note that CloudFront real-time log is not supported in Manual mode) (Optional) If you are ingesting CloudFront logs from another account, select a linked account from the Account dropdown list first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the CloudFront distribution ID. In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. In the Log processor settings section, choose Log processor type , and then Next . Add tags if needed. Choose Create .","title":"Using the Console"},{"location":"implementation-guide/aws-services/cloudfront/#using-the-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - CloudFront Standard Log Ingestion template in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional input> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Plugins <Optional> List of plugins delimited by comma. Leave it blank if there are no available plugins to use. Valid inputs are user_agent , geo_ip . Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the CloudFormation Stack"},{"location":"implementation-guide/aws-services/cloudfront/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Total Requests log event Displays the total number of viewer requests received by the Amazon CloudFront, for all HTTP methods and for both HTTP and HTTPS requests. Edge Locations x-edge-location Shows a pie chart representing the proportion of the locations of CloudFront edge servers. Request History log event Presents a bar chart that displays the distribution of events over time. Unique Visitors c-ip Displays unique visitors identified by client IP address. Cache Hit Rate sc-bytes Shows the proportion of your viewer requests that are served directly from the CloudFront cache instead of going to your origin servers for content. Result Type x-edge-response-result-type Shows the percentage of hits, misses, and errors to the total viewer requests for the selected CloudFront distribution: Hit \u2013 A viewer request for which the object is served from a CloudFront edge cache. In access logs, these are requests for which the value of x-edge-response-result-type is Hit Miss \u2013 A viewer request for which the object isn't currently in an edge cache, so CloudFront must get the object from your origin. In access logs, these are requests for which the value of x-edge-response-result-type is Miss. Error \u2013 A viewer request that resulted in an error, so CloudFront didn't serve the object. In access logs, these are requests for which the value of x-edge-response-result-type is Error, LimitExceeded, or CapacityExceeded. The chart does not include refresh hits\u2014requests for objects that are in the edge cache but that have expired. In access logs, refresh hits are requests for which the value of x-edge-response-result-type is RefreshHit. Top Miss URI cs-uri-stem cs-method Shows top 10 of the requested objects that are not in the cache. Bandwidth cs-bytes sc-bytes Provides insights into data transfer activities from the locations of CloudFront edge. Bandwidth History cs-bytes sc-bytes Shows the historical trend of the data transfer activities from the locations of CloudFront edge. Top Client IPs c-ip Provides the top 10 IP address accessing your Amazon CloudFront. Status Code Count sc-status Displays the count of requests made to the Amazon CloudFront, grouped by HTTP status codes(e.g., 200, 404, 403, etc.). Status History @timestamp sc-status Shows the historical trend of HTTP status codes returned by the Amazon CloudFront over a specific period of time. Status Code sc-status Identifies the users or IAM roles responsible for changes to EC2 resources, assisting in accountability and tracking of modifications. Average Time Taken time-taken This visualization calculates and presents the average time taken for various operations in the Amazon CloudFront (e.g., average time for GET, PUT requests, etc.). Average Time History time-taken time-to-first-byte @timestamp Shows the historical trend of the average time taken for various operations in the Amazon CloudFront. Http Method cs-method Displays the count of requests made to the Amazon CloudFront using a pie chart, grouped by http request method names (e.g., POST, GET, HEAD, etc.). Average Time To First Byte time-to-first-byte Provides the average time taken in seconds by the origin server to respond back with the first byte of the response. Top Request URIs cs-uri-stem cs-method Provides the top 10 request URIs accessing your CloudFront. Top User Agents cs-user-agent Provides the top 10 user agents accessing your CloudFront. Edge Location Heatmap x-edge-location x-edge-result-type Shows a heatmap representing the result type of each edge locations. Top Referers cs-referer Top 10 referers with the Amazon CloudFront access. Top Countries or Regions c_country Top 10 countries with the Amazon CloudFront access.","title":"View dashboard"},{"location":"implementation-guide/aws-services/cloudfront/#sample-dashboard","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Sample dashboard"},{"location":"implementation-guide/aws-services/cloudfront/#create-log-ingestion-light-engine-for-log-analytics","text":"","title":"Create log ingestion (Light Engine for log analytics)"},{"location":"implementation-guide/aws-services/cloudfront/#using-the-console_1","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Amazon CloudFront . Choose Light Engine , Choose Next . Under Specify settings , choose Automatic or Manual for CloudFront logs enabling . The automatic mode will detect the CloudFront log location automatically. For Automatic mode , choose the CloudFront distribution and Log Type from the dropdown lists. For Standard Log, the solution will automatically detect the log location if logging is enabled. For Manual mode , enter the CloudFront Distribution ID and CloudFront Standard Log location . (Note that CloudFront real-time log is not supported in Manual mode) (Optional) If you are ingesting CloudFront logs from another account, select a linked account from the Account dropdown list first. Choose Next . Choose Log Processing Enriched fields if needed. The available plugins are location and OS/User Agent . Enabling rich fields increases data processing latency and processing costs. By default, it is not selected. In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . If desired, add tags. Select Create .","title":"Using the Console"},{"location":"implementation-guide/aws-services/cloudfront/#using-the-cloudformation-stack_1","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - CloudFront Standard Log Ingestion template in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameters for Pipeline settings Parameter Default Description Pipeline Id <Requires input> The unique identifier for the pipeline is essential if you need to create multiple ALB pipelines and write different ALB logs into separate tables. To ensure uniqueness, you can generate a unique pipeline identifier using uuidgenerator . Staging Bucket Prefix AWSLogs/CloudFrontLogs The storage directory for logs in the temporary storage area should ensure the uniqueness and non-overlapping of the Prefix for different pipelines. Parameters for Destination settings Parameters Default Description Centralized Bucket Name <Requires input> Centralized s3 bucket name. For example, centralized-logging-bucket. Centralized Bucket Prefix datalake Centralized bucket prefix. By default, the data base location is s3://{Centralized Bucket Name}/{Centralized Bucket Prefix}/amazon_cl_centralized. Centralized Table Name CloudFront Table name for writing data to the centralized database. You can modify it if needed. Enrichment Plugins <Optional input> The available plugins to choose from are location and OS/User Agent . Enabling rich fields will increase data processing latency and processing costs, it is not selected by default. Parameters for Scheduler settings Parameters Default Description LogProcessor Schedule Expression rate(5 minutes) Task scheduling expression for performing log processing, with a default value of executing the LogProcessor every 5 minutes. Configuration for reference \u3002 LogMerger Schedule Expression cron(0 1 * ? ) \u6267ask scheduling expression for performing log merging, with a default value of executing the LogMerger at 1 AM every day. Configuration for reference \u3002 LogArchive Schedule Expression cron(0 2 * ? ) Task scheduling expression for performing log archiving, with a default value of executing the LogArchive at 2 AM every day. Configuration for reference \u3002 Age to Merge 7 Small file retention days, with a default value of 7, indicates that logs older than 7 days will be merged into small files. It can be adjusted as needed. Age to Archive 30 Log retention days, with a default value of 30, indicates that data older than 30 days will be archived and deleted. It can be adjusted as needed. Parameters for Notification settings Parameters Default Description Notification Service SNS Notification method for alerts. If your main stack is using China, you can only choose the SNS method. If your main stack is using Global, you can choose either the SNS or SES method. Recipients <Requires Input> Alert notification: If the Notification Service is SNS, enter the SNS Topic ARN here, ensuring that you have the necessary permissions. If the Notification Service is SES, enter the email addresses separated by commas here, ensuring that the email addresses are already Verified Identities in SES. The adminEmail provided during the creation of the main stack will receive a verification email by default. Parameters for Dashboard settings Parameters Default Description Import Dashboards FALSE Whether to import the Dashboard into Grafana, with a default value of false. If set to true, you must provide the Grafana URL and Grafana Service Account Token.\u3002 Grafana URL <Requires Input> Grafana access URL\uff0cfor example: https://alb-72277319.us-west-2.elb.amazonaws.com\u3002 Grafana Service Account Token <Requires Input> Grafana Service Account Token\uff1aService Account Token created in Grafana\u3002 Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the CloudFormation Stack"},{"location":"implementation-guide/aws-services/cloudfront/#view-dashboard_1","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Filters Filters The following data can be filtered by query filter conditions. Total Requests log event Displays the total number of viewer requests received by the Amazon CloudFront, for all HTTP methods and for both HTTP and HTTPS requests. Unique Visitors c-ip Displays unique visitors identified by client IP address. Requests History log event Presents a bar chart that displays the distribution of events over time. Request By Edge Location x-edge-location Shows a pie chart representing the proportion of the locations of CloudFront edge servers. HTTP Status Code sc-status Displays the count of requests made to the Amazon CloudFront, grouped by HTTP status codes (e.g., 200, 404, 403, etc.). Status Code History sc-status Shows the historical trend of HTTP status codes returned by the Amazon CloudFront over a specific period of time. Status Code Pie sc-status Represents the distribution of requests based on different HTTP status codes using a pie chart. Average Processing Time time-taken time-to-first-byte This visualization calculates and presents the average time taken for various operations in the Amazon CloudFront (e.g., average time for GET, PUT requests, etc.). Avg. Processing Time History time-taken time-to-first-byte Shows the historical trend of the average time taken for various operations in the Amazon CloudFront. Avg. Processing Time History time-taken time-to-first-byte Shows the historical trend of the average time taken for various operations in the Amazon CloudFront. HTTP Method cs-method Displays the count of requests made to the Amazon CloudFront using a pie chart, grouped by HTTP request method names (e.g., POST, GET, HEAD, etc.). Total Bytes cs-bytes sc-bytes Provides insights into data transfer activities, including the total bytes transferred. Response Bytes History cs-bytes sc-bytes Displays the historical trend of the received bytes, send bytes. Edge Response Type x-edge-response-result-type Shows the percentage of hits, misses, and errors to the total viewer requests for the selected CloudFront distribution: - Hit \u2013 A viewer request for which the object is served from a CloudFront edge cache. In access logs, these are requests for which the value of x-edge-response-result-type is Hit. - Miss \u2013 A viewer request for which the object isn't currently in an edge cache, so CloudFront must get the object from your origin. In access logs, these are requests for which the value of x-edge-response-result-type is Miss. - Error \u2013 A viewer request that resulted in an error, so CloudFront didn't serve the object. In access logs, these are requests for which the value of x-edge-response-result-type is Error, LimitExceeded, or CapacityExceeded. The chart does not include refresh hits\u2014requests for objects that are in the edge cache but that have expired. In access logs, refresh hits are requests for which the value of x-edge-response-result-type is RefreshHit. Requests / Origin Requests log event Displays the number of requests made to CloudFront and the number of requests back to the origin. Requests / Origin Requests Latency log event time-taken Displays the request latency from the client to CloudFront and the request latency back to the origin. Top 20 URLs with most requests log event Top 20 URLs based on the number of requests. Requests 3xx / 4xx / 5xx error rate log event sc-status Displays the ratio of 3xx/4xx/5xx status codes from the client to CloudFront. Origin Requests 3xx / 4xx / 5xx error rate log event sc-status x-edge-detailed-result-type Display the proportion of 3xx/4xx/5xx status codes returned to the origin. Requests 3xx / 4xx / 5xx error latency log event sc-status time-taken Displays the latency from the client to CloudFront for 3xx/4xx/5xx status codes. Origin Requests 3xx / 4xx / 5xx error latency log event sc-status x-edge-detailed-result-type time-taken Displays the delay in returning to the source 3xx/4xx/5xx status code. Response Latency (>= 1sec) rate log event time-taken Display the proportion of delay above 1s. Bandwidth sc-bytes Displays the bandwidth from the client to CloudFront and the bandwidth back to the origin. Data transfer sc-bytes Display the response traffic. Top 20 URLs with most traffic cs-uri-stem sc-bytes Top 20 URLs calculated by traffic. Cache hit rate (calculated using requests) log event x-edge-result-type Displays the cache hit ratio calculated by the number of requests. Cache hit rate (calculated using bandwidth) log event sc-bytes x-edge-result-type Displays the cache hit ratio calculated by bandwidth. Cache Result log event x-edge-result-type Displays the number of requests of various x-edge-result-types, such as the number of requests that hit the cache and the number of requests that missed the cache. Cache Result Latency log event sc-bytes x-edge-result-type Displays the request latency of various x-edge-result-types, such as the request latency that hits the cache and the request latency that misses the cache. Requests by OS ua_os Displays the count of requests made to the ALB, grouped by user agent OS. Requests by Device ua_device Displays the count of requests made to the ALB, grouped by user agent device. Requests by Browser ua_browser Displays the count of requests made to the ALB, grouped by user agent browser. Requests by Category ua_category Displays the count of category made to the ALB, grouped by user agent category (e.g., PC, Mobile, Tablet, etc.). Requests by Countries or Regions geo_iso_code Displays the count of requests made to the ALB (grouped by the corresponding country or region resolved by the client IP). Top Countries or Regions geo_country Top 10 countries with the ALB Access. Top Cities geo_city Top 10 cities with ALB Access.","title":"View Dashboard"},{"location":"implementation-guide/aws-services/cloudtrail/","text":"Amazon CloudTrail Logs Amazon CloudTrail monitors and records account activity across your AWS infrastructure. It outputs all the data to the specified S3 bucket or a CloudWatch log group. Create log ingestion You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The CloudTrail region must be the same as the solution region. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Using the Centralized Logging with OpenSearch console Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose Create a log ingestion . In the AWS Services section, choose Amazon CloudTrail . Choose Next . Under Specify settings , for Trail , select one from the dropdown list. (Optional) If you are ingesting CloudTrail logs from another account, select a linked account from the Account dropdown list first. Under Log Source , Select S3 or CloudWatch as the log source. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your trail name. In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create . Using the standalone CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - CloudTrail Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Global Control awsRegion Provides users with the ability to drill down data by Region. Event History log event Presents a bar chart that displays the distribution of events over time. Event by Account ID userIdentity.accountId Breaks down events based on the AWS account ID, enabling you to analyze activity patterns across different accounts within your organization. Top Event Names eventName Shows the most frequently occurring event names, helping you identify common activities or potential anomalies. Top Event Sources eventSource Highlights the top sources generating events, providing insights into the services or resources that are most active or experiencing the highest event volume. Event Category eventCategory Categorizes events into different types or classifications, facilitating analysis and understanding of event distribution across categories. Top Users userIdentity.sessionContext.sessionIssuer.userName userIdentity.sessionContext.sessionIssuer.arn userIdentity.accountId userIdentity.sessionContext.sessionIssuer.type Identifies the users or IAM roles associated with the highest number of events, aiding in user activity monitoring and access management. Top Source IPs sourceIPAddress Lists the source IP addresses associated with events, enabling you to identify and investigate potentially suspicious or unauthorized activities. S3 Access Denied eventSource: s3* errorCode: AccessDenied Displays events where access to Amazon S3 resources was denied, helping you identify and troubleshoot permission issues or potential security breaches. S3 Buckets requestParameters.bucketName Provides a summary of S3 bucket activity, including create, delete, and modify operations, allowing you to monitor changes and access patterns. Top S3 Change Events eventName requestParameters.bucketName Presents the most common types of changes made to S3 resources, such as object uploads, deletions, or modifications, aiding in change tracking and auditing. EC2 Change Event Count eventSource: ec2* eventName: (RunInstances or TerminateInstances or RunInstances or StopInstances) Shows the total count of EC2-related change events, giving an overview of the volume and frequency of changes made to EC2 instances and resources. EC2 Changed By userIdentity.sessionContext.sessionIssuer.userName Identifies the users or IAM roles responsible for changes to EC2 resources, assisting in accountability and tracking of modifications. Top EC2 Change Events eventName Highlights the most common types of changes made to EC2 instances or related resources, allowing you to focus on the most significant or frequent changes. Error Events awsRegion errorCode errorMessage eventName eventSource sourceIPAddress userAgent userIdentity.\u200baccountId userIdentity.\u200bsessionContext.\u200bsessionIssuer.\u200baccountId userIdentity.\u200bsessionContext.\u200bsessionIssuer.\u200barn userIdentity.\u200bsessionContext.\u200bsessionIssuer.\u200btype userIdentity.\u200bsessionContext.\u200bsessionIssuer.\u200buserName Displays events that resulted in errors or failures, helping you identify and troubleshoot issues related to API calls or resource operations. Sample dashboard You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Amazon CloudTrail"},{"location":"implementation-guide/aws-services/cloudtrail/#amazon-cloudtrail-logs","text":"Amazon CloudTrail monitors and records account activity across your AWS infrastructure. It outputs all the data to the specified S3 bucket or a CloudWatch log group.","title":"Amazon CloudTrail Logs"},{"location":"implementation-guide/aws-services/cloudtrail/#create-log-ingestion","text":"You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The CloudTrail region must be the same as the solution region. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"Create log ingestion"},{"location":"implementation-guide/aws-services/cloudtrail/#using-the-centralized-logging-with-opensearch-console","text":"Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose Create a log ingestion . In the AWS Services section, choose Amazon CloudTrail . Choose Next . Under Specify settings , for Trail , select one from the dropdown list. (Optional) If you are ingesting CloudTrail logs from another account, select a linked account from the Account dropdown list first. Under Log Source , Select S3 or CloudWatch as the log source. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your trail name. In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create .","title":"Using the Centralized Logging with OpenSearch console"},{"location":"implementation-guide/aws-services/cloudtrail/#using-the-standalone-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - CloudTrail Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the standalone CloudFormation Stack"},{"location":"implementation-guide/aws-services/cloudtrail/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Global Control awsRegion Provides users with the ability to drill down data by Region. Event History log event Presents a bar chart that displays the distribution of events over time. Event by Account ID userIdentity.accountId Breaks down events based on the AWS account ID, enabling you to analyze activity patterns across different accounts within your organization. Top Event Names eventName Shows the most frequently occurring event names, helping you identify common activities or potential anomalies. Top Event Sources eventSource Highlights the top sources generating events, providing insights into the services or resources that are most active or experiencing the highest event volume. Event Category eventCategory Categorizes events into different types or classifications, facilitating analysis and understanding of event distribution across categories. Top Users userIdentity.sessionContext.sessionIssuer.userName userIdentity.sessionContext.sessionIssuer.arn userIdentity.accountId userIdentity.sessionContext.sessionIssuer.type Identifies the users or IAM roles associated with the highest number of events, aiding in user activity monitoring and access management. Top Source IPs sourceIPAddress Lists the source IP addresses associated with events, enabling you to identify and investigate potentially suspicious or unauthorized activities. S3 Access Denied eventSource: s3* errorCode: AccessDenied Displays events where access to Amazon S3 resources was denied, helping you identify and troubleshoot permission issues or potential security breaches. S3 Buckets requestParameters.bucketName Provides a summary of S3 bucket activity, including create, delete, and modify operations, allowing you to monitor changes and access patterns. Top S3 Change Events eventName requestParameters.bucketName Presents the most common types of changes made to S3 resources, such as object uploads, deletions, or modifications, aiding in change tracking and auditing. EC2 Change Event Count eventSource: ec2* eventName: (RunInstances or TerminateInstances or RunInstances or StopInstances) Shows the total count of EC2-related change events, giving an overview of the volume and frequency of changes made to EC2 instances and resources. EC2 Changed By userIdentity.sessionContext.sessionIssuer.userName Identifies the users or IAM roles responsible for changes to EC2 resources, assisting in accountability and tracking of modifications. Top EC2 Change Events eventName Highlights the most common types of changes made to EC2 instances or related resources, allowing you to focus on the most significant or frequent changes. Error Events awsRegion errorCode errorMessage eventName eventSource sourceIPAddress userAgent userIdentity.\u200baccountId userIdentity.\u200bsessionContext.\u200bsessionIssuer.\u200baccountId userIdentity.\u200bsessionContext.\u200bsessionIssuer.\u200barn userIdentity.\u200bsessionContext.\u200bsessionIssuer.\u200btype userIdentity.\u200bsessionContext.\u200bsessionIssuer.\u200buserName Displays events that resulted in errors or failures, helping you identify and troubleshoot issues related to API calls or resource operations.","title":"View dashboard"},{"location":"implementation-guide/aws-services/cloudtrail/#sample-dashboard","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Sample dashboard"},{"location":"implementation-guide/aws-services/config/","text":"AWS Config Logs By default, AWS Config delivers configuration history and snapshot files to your Amazon S3 bucket. Create log ingestion You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important AWS Config must be enabled in the same region as the Centralized Logging with OpenSearch solution. The Amazon S3 bucket region must be the same as the Centralized Logging with OpenSearch solution. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Using the Centralized Logging with OpenSearch Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose AWS Config Logs . Choose Next . Under Specify settings , choose Automatic or Manual for Log creation . For Automatic mode , make sure the S3 bucket location is correct, and enter the AWS Config Name . For Manual mode , enter the AWS Config Name and Log location . (Optional) If you are ingesting AWS Config logs from another account, select a linked account from the Account dropdown list first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix the AWS Config Name you entered in previous steps. In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create . Using the standalone CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - AWS Config Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Global Filters awsAccountId awsRegion resourceType resourceId resourceName The charts are filtered according to Account ID, Region, Resource Type and other conditions. Total Change Events log event Shows the number of configuration changes detected across all AWS resources during a selected time period. Top Resource Types resourceType Displays the breakdown of configuration changes by the most frequently modified AWS resource types during a selected time period. Config History log event Presents a bar chart that displays the distribution of events over time. Total Delete Events log event Shows the number of AWS resource deletion events detected by AWS Config during a selected time period. Config Status configurationItemStatus Displays the operational state of the AWS Config service across monitored regions and accounts. Top S3 Changes resourceName Displays the Amazon S3 buckets undergoing the highest number of configuration changes during a selected time period. Top Changed Resources resourceName resourceId resourceType Displays the individual AWS resources undergoing the highest number of configuration changes during a selected time period. Top VPC Changes resourceId Presents a bar chart that Displays the Amazon VPCs undergoing the highest number of configuration changes during a selected time period. Top Subnet Changes resourceId Delivers targeted visibility into the subnets undergoing the most transformation for governance, security and stability. Top Network Interface Changes resourceId Spotlights the Amazon VPC network interfaces seeing the most configuration changes during a selected period. Top Security Group Changes resourceId Top 10 changed groups rank by total modification count. EC2 Config @timestamp awsAccountId awsRegion resourceId configurationItemStatus Allows reconstructing the incremental changes applied to EC2 configurations over time for auditing. RDS Config @timestamp awsAccountId awsRegion resourceId resourceName configurationItemStatus Shows the configuration history and changes detected by AWS Config for RDS database resources Latest Config Changes @timestamp awsAccountId awsRegion resourceType resourceId resourceName relationships configurationItemStatus Offers an at-a-glance overview of infrastructure modifications. Sample Dashboard You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"AWS Config"},{"location":"implementation-guide/aws-services/config/#aws-config-logs","text":"By default, AWS Config delivers configuration history and snapshot files to your Amazon S3 bucket.","title":"AWS Config Logs"},{"location":"implementation-guide/aws-services/config/#create-log-ingestion","text":"You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important AWS Config must be enabled in the same region as the Centralized Logging with OpenSearch solution. The Amazon S3 bucket region must be the same as the Centralized Logging with OpenSearch solution. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"Create log ingestion"},{"location":"implementation-guide/aws-services/config/#using-the-centralized-logging-with-opensearch-console","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose AWS Config Logs . Choose Next . Under Specify settings , choose Automatic or Manual for Log creation . For Automatic mode , make sure the S3 bucket location is correct, and enter the AWS Config Name . For Manual mode , enter the AWS Config Name and Log location . (Optional) If you are ingesting AWS Config logs from another account, select a linked account from the Account dropdown list first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix the AWS Config Name you entered in previous steps. In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create .","title":"Using the Centralized Logging with OpenSearch Console"},{"location":"implementation-guide/aws-services/config/#using-the-standalone-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - AWS Config Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the standalone CloudFormation Stack"},{"location":"implementation-guide/aws-services/config/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Global Filters awsAccountId awsRegion resourceType resourceId resourceName The charts are filtered according to Account ID, Region, Resource Type and other conditions. Total Change Events log event Shows the number of configuration changes detected across all AWS resources during a selected time period. Top Resource Types resourceType Displays the breakdown of configuration changes by the most frequently modified AWS resource types during a selected time period. Config History log event Presents a bar chart that displays the distribution of events over time. Total Delete Events log event Shows the number of AWS resource deletion events detected by AWS Config during a selected time period. Config Status configurationItemStatus Displays the operational state of the AWS Config service across monitored regions and accounts. Top S3 Changes resourceName Displays the Amazon S3 buckets undergoing the highest number of configuration changes during a selected time period. Top Changed Resources resourceName resourceId resourceType Displays the individual AWS resources undergoing the highest number of configuration changes during a selected time period. Top VPC Changes resourceId Presents a bar chart that Displays the Amazon VPCs undergoing the highest number of configuration changes during a selected time period. Top Subnet Changes resourceId Delivers targeted visibility into the subnets undergoing the most transformation for governance, security and stability. Top Network Interface Changes resourceId Spotlights the Amazon VPC network interfaces seeing the most configuration changes during a selected period. Top Security Group Changes resourceId Top 10 changed groups rank by total modification count. EC2 Config @timestamp awsAccountId awsRegion resourceId configurationItemStatus Allows reconstructing the incremental changes applied to EC2 configurations over time for auditing. RDS Config @timestamp awsAccountId awsRegion resourceId resourceName configurationItemStatus Shows the configuration history and changes detected by AWS Config for RDS database resources Latest Config Changes @timestamp awsAccountId awsRegion resourceType resourceId resourceName relationships configurationItemStatus Offers an at-a-glance overview of infrastructure modifications.","title":"View dashboard"},{"location":"implementation-guide/aws-services/config/#sample-dashboard","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Sample Dashboard"},{"location":"implementation-guide/aws-services/elb/","text":"Application Load Balancing (ALB) Logs ALB Access logs provide access logs that capture detailed information about requests sent to your load balancer. ALB publishes a log file for each load balancer node every 5 minutes. You can create a log ingestion into Amazon OpenSearch Service or Light Engine either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The ELB logging bucket's region must be the same as the Centralized Logging with OpenSearch solution. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Create log ingestion (Amazon OpenSearch for log analytics) Using the Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Elastic Load Balancer . Choose Amazon OpenSearch , Choose Next . Under Specify settings , choose Automatic or Manual . For Automatic mode, choose an application load balancer in the dropdown list. (If the selected ALB access log is not enabled, click Enable to enable the ALB access log.) For Manual mode, enter the Application Load Balancer identifier and Log location . (Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Load Balancer Name . In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create . Using the CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - ELB Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional input> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Plugins <Optional> List of plugins delimited by comma. Leave it blank if there are no available plugins to use. Valid inputs are user_agent , geo_ip . Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Total Requests log event Displays aggregated events based on a specified time interval. Request History log event Presents a bar chart that displays the distribution of events over time. Request By Target log event target_ip Presents a bar chart that displays the distribution of events over time and IP. Unique Visitors client_ip Displays unique visitors identified by client IP address. Status Code elb_status_code Displays the count of requests made to the ALB, grouped by HTTP status codes (e.g., 200, 404, 403, etc.). Status History elb_status_code Shows the historical trend of HTTP status codes returned by the ALB over a specific period of time. Status Code Pipe elb_status_code Represents the distribution of requests based on different HTTP status codes using a pie chart. Average Processing Time request_processing_time response_processing_time target_processing_time This visualization calculates and presents the average time taken for various operations in the ALB. Avg. Processing Time History request_processing_time response_processing_time target_processing_time Displays the historical trend of the average time-consuming of each operation returned by the ALB within a specific period of time. Request Verb request_verb Displays the count of requests made to the ALB using a pie chart, grouped by http request method names (e.g., POST, GET, HEAD, etc.). Total Bytes received_bytes sent_bytes Provides insights into data transfer activities, including the total bytes transferred. Sent and Received Bytes History received_bytes sent_bytes Displays the historical trend of the the received bytes, send bytes SSL Protocol ssl_protocol Displays the count of requests made to the ALB, grouped by SSL Protocol Top Request URLs request_url The web requests view enables you to analyze the top web requests. Top Client IPs client_ip Provides the top 10 IP address accessing your ALB. Top User Agents user_agent Provides the top 10 user agents accessing your ALB. Target Status target_ip target_status_code Displays the http status code request count for targets in ALB target group. Abnormal Requests @timestamp client_ip target_ip elb_status_code error_reason request_verb target_status_code target_status_code_list request_url request_proto trace_id Provides a detailed list of log events, including timestamps, client ip, target ip, etc. Requests by OS ua_os Displays the count of requests made to the ALB, grouped by user agent OS Request by Device ua_device Displays the count of requests made to the ALB, grouped by user agent device. Request by Browser ua_browser Displays the count of requests made to the ALB, grouped by user agent browser. Request by Category ua_category Displays the count of category made to the ALB, grouped by user agent category (e.g., PC, Mobile, Tablet, etc.). Requests by Countries or Regions geo_iso_code Displays the count of requests made to the ALB (grouped by the corresponding country or region resolved by the client IP). Top Countries or Regions geo_country Top 10 countries with the ALB Access. Top Cities geo_city Top 10 cities with ALB Access Sample Dashboard Create log ingestion (Light Engine for log analytics) Using the Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Elastic Load Balancer . Choose Light Engine , Choose Next . Under Specify settings , choose Automatic or Manual . For Automatic mode, choose an application load balancer in the dropdown list. (If the selected ALB access log is not enabled, click Enable to enable the ALB access log.) For Manual mode, enter the Application Load Balancer identifier and Log location . (Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first. Choose Next . Log Processing Enriched fields \uff0cThe available plugins to choose from are location and OS/User Agent . Enabling rich fields will increase data processing latency and processing costs, it is not selected by default. In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . If desired, add tags. Select Create . Using the CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - ELB Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameters for Pipeline settings Parameter Defaul Description Pipeline Id <Requires input> The unique identifier for the pipeline is essential if you need to create multiple ALB pipelines and write different ALB logs into separate tables. To ensure uniqueness, you can generate a unique pipeline identifier using uuidgenerator . Staging Bucket Prefix AWSLogs/ALBLogs The storage directory for logs in the temporary storage area should ensure the uniqueness and non-overlapping of the Prefix for different pipelines. Parameters for Destination settings Parameters Default Description Centralized Bucket Name <Requires input> Input centralized s3 bucket name\uff0cfor expample:centralized-logging-bucket\u3002 Centralized Bucket Prefix datalake Input centralized bucket prefix\uff0cdefault is datalake which means your data base's location is s3://{Centralized Bucket Name}/{Centralized Bucket Prefix}/amazon_cl_centralized\u3002 Centralized Table Name ALB Table name for writing data to the centralized database, can be defined as needed, default value is 'ALB'. Enrichment Plugins <Optional input> The available plugins to choose from are location and OS/User Agent . Enabling rich fields will increase data processing latency and processing costs, it is not selected by default. Parameters for Scheduler settings Parameters Default Description LogProcessor Schedule Expression rate(5 minutes) Task scheduling expression for performing log processing, with a default value of executing the LogProcessor every 5 minutes. Configuration for reference \u3002 LogMerger Schedule Expression cron(0 1 * ? ) \u6267ask scheduling expression for performing log merging, with a default value of executing the LogMerger at 1 AM every day. Configuration for reference \u3002 LogArchive Schedule Expression cron(0 2 * ? ) Task scheduling expression for performing log archiving, with a default value of executing the LogArchive at 2 AM every day. Configuration for reference \u3002 Age to Merge 7 Small file retention days, with a default value of 7, indicates that logs older than 7 days will be merged into small files. It can be adjusted as needed. Age to Archive 30 Log retention days, with a default value of 30, indicates that data older than 30 days will be archived and deleted. It can be adjusted as needed. Parameters for Notification settings Parameters Default Description Notification Service SNS Notification method for alerts. If your main stack is using China, you can only choose the SNS method. If your main stack is using Global, you can choose either the SNS or SES method. Recipients <Requires Input> Alert notification: If the Notification Service is SNS, enter the SNS Topic ARN here, ensuring that you have the necessary permissions. If the Notification Service is SES, enter the email addresses separated by commas here, ensuring that the email addresses are already Verified Identities in SES. The adminEmail provided during the creation of the main stack will receive a verification email by default. Parameters for Dashboard settings Parameters Default Description Import Dashboards FALSE Whether to import the Dashboard into Grafana, with a default value of false. If set to true, you must provide the Grafana URL and Grafana Service Account Token.\u3002 Grafana URL <Requires Input> Grafana access URL\uff0cfor example: https://alb-72277319.us-west-2.elb.amazonaws.com\u3002 Grafana Service Account Token <Requires Input> Grafana Service Account Token\uff1aService Account Token created in Grafana\u3002 Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View Dashboard Visualization Name Source Field Description Filters Filters The following data can be filtered by query filter conditions. Total Requests log event Displays aggregated events based on a specified time interval. Unique Visitors client_ip Displays unique visitors identified by client IP address. Requests History log event Presents a bar chart that displays the distribution of events over time. Request By Target log event target_ip Presents a bar chart that displays the distribution of events over time and IP. HTTP Status Code elb_status_code Displays the count of requests made to the ALB, grouped by HTTP status codes (e.g., 200, 404, 403, etc.). Status Code History elb_status_code Shows the historical trend of HTTP status codes returned by the ALB over a specific period of time. Status Code Pie elb_status_code Represents the distribution of requests based on different HTTP status codes using a pie chart. Average Processing Time request_processing_time response_processing_time target_processing_time This visualization calculates and presents the average time taken for various operations in the ALB. Avg. Processing Time History request_processing_time response_processing_time target_processing_time Displays the historical trend of the average time-consuming of each operation returned by the ALB within a specific period of time. HTTP Method request_verb Displays the count of requests made to the ALB using a pie chart, grouped by HTTP request method names (e.g., POST, GET, HEAD, etc.). Total Bytes received_bytes sent_bytes Provides insights into data transfer activities, including the total bytes transferred. Sent and Received Bytes History received_bytes sent_bytes Displays the historical trend of the received bytes, send bytes. SSL Protocol ssl_protocol Displays the count of requests made to the ALB, grouped by SSL Protocol. Top Request URLs request_url The web requests view enables you to analyze the top web requests. Top Client IPs client_ip Provides the top 10 IP addresses accessing your ALB. Bad Requests type client_ip target_group_arn target_ip elb_status_code request_verb request_url ssl_protocol received_bytes sent_bytes Provides a detailed list of log events, including timestamps, client IP, target IP, etc. Requests by OS ua_os Displays the count of requests made to the ALB, grouped by user agent OS. Requests by Device ua_device Displays the count of requests made to the ALB, grouped by user agent device. Requests by Browser ua_browser Displays the count of requests made to the ALB, grouped by user agent browser. Requests by Category ua_category Displays the count of category made to the ALB, grouped by user agent category (e.g., PC, Mobile, Tablet, etc.). Requests by Countries or Regions geo_iso_code Displays the count of requests made to the ALB (grouped by the corresponding country or region resolved by the client IP). Top Countries or Regions geo_country Top 10 countries with the ALB Access. Top Cities geo_city Top 10 cities with ALB Access.","title":"Elastic Load Balancing"},{"location":"implementation-guide/aws-services/elb/#application-load-balancing-alb-logs","text":"ALB Access logs provide access logs that capture detailed information about requests sent to your load balancer. ALB publishes a log file for each load balancer node every 5 minutes. You can create a log ingestion into Amazon OpenSearch Service or Light Engine either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The ELB logging bucket's region must be the same as the Centralized Logging with OpenSearch solution. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"Application Load Balancing (ALB) Logs"},{"location":"implementation-guide/aws-services/elb/#create-log-ingestion-amazon-opensearch-for-log-analytics","text":"","title":"Create log ingestion (Amazon OpenSearch for log analytics)"},{"location":"implementation-guide/aws-services/elb/#using-the-console","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Elastic Load Balancer . Choose Amazon OpenSearch , Choose Next . Under Specify settings , choose Automatic or Manual . For Automatic mode, choose an application load balancer in the dropdown list. (If the selected ALB access log is not enabled, click Enable to enable the ALB access log.) For Manual mode, enter the Application Load Balancer identifier and Log location . (Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Load Balancer Name . In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create .","title":"Using the Console"},{"location":"implementation-guide/aws-services/elb/#using-the-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - ELB Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional input> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Plugins <Optional> List of plugins delimited by comma. Leave it blank if there are no available plugins to use. Valid inputs are user_agent , geo_ip . Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the CloudFormation Stack"},{"location":"implementation-guide/aws-services/elb/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Total Requests log event Displays aggregated events based on a specified time interval. Request History log event Presents a bar chart that displays the distribution of events over time. Request By Target log event target_ip Presents a bar chart that displays the distribution of events over time and IP. Unique Visitors client_ip Displays unique visitors identified by client IP address. Status Code elb_status_code Displays the count of requests made to the ALB, grouped by HTTP status codes (e.g., 200, 404, 403, etc.). Status History elb_status_code Shows the historical trend of HTTP status codes returned by the ALB over a specific period of time. Status Code Pipe elb_status_code Represents the distribution of requests based on different HTTP status codes using a pie chart. Average Processing Time request_processing_time response_processing_time target_processing_time This visualization calculates and presents the average time taken for various operations in the ALB. Avg. Processing Time History request_processing_time response_processing_time target_processing_time Displays the historical trend of the average time-consuming of each operation returned by the ALB within a specific period of time. Request Verb request_verb Displays the count of requests made to the ALB using a pie chart, grouped by http request method names (e.g., POST, GET, HEAD, etc.). Total Bytes received_bytes sent_bytes Provides insights into data transfer activities, including the total bytes transferred. Sent and Received Bytes History received_bytes sent_bytes Displays the historical trend of the the received bytes, send bytes SSL Protocol ssl_protocol Displays the count of requests made to the ALB, grouped by SSL Protocol Top Request URLs request_url The web requests view enables you to analyze the top web requests. Top Client IPs client_ip Provides the top 10 IP address accessing your ALB. Top User Agents user_agent Provides the top 10 user agents accessing your ALB. Target Status target_ip target_status_code Displays the http status code request count for targets in ALB target group. Abnormal Requests @timestamp client_ip target_ip elb_status_code error_reason request_verb target_status_code target_status_code_list request_url request_proto trace_id Provides a detailed list of log events, including timestamps, client ip, target ip, etc. Requests by OS ua_os Displays the count of requests made to the ALB, grouped by user agent OS Request by Device ua_device Displays the count of requests made to the ALB, grouped by user agent device. Request by Browser ua_browser Displays the count of requests made to the ALB, grouped by user agent browser. Request by Category ua_category Displays the count of category made to the ALB, grouped by user agent category (e.g., PC, Mobile, Tablet, etc.). Requests by Countries or Regions geo_iso_code Displays the count of requests made to the ALB (grouped by the corresponding country or region resolved by the client IP). Top Countries or Regions geo_country Top 10 countries with the ALB Access. Top Cities geo_city Top 10 cities with ALB Access","title":"View dashboard"},{"location":"implementation-guide/aws-services/elb/#sample-dashboard","text":"","title":"Sample Dashboard"},{"location":"implementation-guide/aws-services/elb/#create-log-ingestion-light-engine-for-log-analytics","text":"","title":"Create log ingestion (Light Engine for log analytics)"},{"location":"implementation-guide/aws-services/elb/#using-the-console_1","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Elastic Load Balancer . Choose Light Engine , Choose Next . Under Specify settings , choose Automatic or Manual . For Automatic mode, choose an application load balancer in the dropdown list. (If the selected ALB access log is not enabled, click Enable to enable the ALB access log.) For Manual mode, enter the Application Load Balancer identifier and Log location . (Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first. Choose Next . Log Processing Enriched fields \uff0cThe available plugins to choose from are location and OS/User Agent . Enabling rich fields will increase data processing latency and processing costs, it is not selected by default. In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . If desired, add tags. Select Create .","title":"Using the Console"},{"location":"implementation-guide/aws-services/elb/#using-the-cloudformation-stack_1","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - ELB Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameters for Pipeline settings Parameter Defaul Description Pipeline Id <Requires input> The unique identifier for the pipeline is essential if you need to create multiple ALB pipelines and write different ALB logs into separate tables. To ensure uniqueness, you can generate a unique pipeline identifier using uuidgenerator . Staging Bucket Prefix AWSLogs/ALBLogs The storage directory for logs in the temporary storage area should ensure the uniqueness and non-overlapping of the Prefix for different pipelines. Parameters for Destination settings Parameters Default Description Centralized Bucket Name <Requires input> Input centralized s3 bucket name\uff0cfor expample:centralized-logging-bucket\u3002 Centralized Bucket Prefix datalake Input centralized bucket prefix\uff0cdefault is datalake which means your data base's location is s3://{Centralized Bucket Name}/{Centralized Bucket Prefix}/amazon_cl_centralized\u3002 Centralized Table Name ALB Table name for writing data to the centralized database, can be defined as needed, default value is 'ALB'. Enrichment Plugins <Optional input> The available plugins to choose from are location and OS/User Agent . Enabling rich fields will increase data processing latency and processing costs, it is not selected by default. Parameters for Scheduler settings Parameters Default Description LogProcessor Schedule Expression rate(5 minutes) Task scheduling expression for performing log processing, with a default value of executing the LogProcessor every 5 minutes. Configuration for reference \u3002 LogMerger Schedule Expression cron(0 1 * ? ) \u6267ask scheduling expression for performing log merging, with a default value of executing the LogMerger at 1 AM every day. Configuration for reference \u3002 LogArchive Schedule Expression cron(0 2 * ? ) Task scheduling expression for performing log archiving, with a default value of executing the LogArchive at 2 AM every day. Configuration for reference \u3002 Age to Merge 7 Small file retention days, with a default value of 7, indicates that logs older than 7 days will be merged into small files. It can be adjusted as needed. Age to Archive 30 Log retention days, with a default value of 30, indicates that data older than 30 days will be archived and deleted. It can be adjusted as needed. Parameters for Notification settings Parameters Default Description Notification Service SNS Notification method for alerts. If your main stack is using China, you can only choose the SNS method. If your main stack is using Global, you can choose either the SNS or SES method. Recipients <Requires Input> Alert notification: If the Notification Service is SNS, enter the SNS Topic ARN here, ensuring that you have the necessary permissions. If the Notification Service is SES, enter the email addresses separated by commas here, ensuring that the email addresses are already Verified Identities in SES. The adminEmail provided during the creation of the main stack will receive a verification email by default. Parameters for Dashboard settings Parameters Default Description Import Dashboards FALSE Whether to import the Dashboard into Grafana, with a default value of false. If set to true, you must provide the Grafana URL and Grafana Service Account Token.\u3002 Grafana URL <Requires Input> Grafana access URL\uff0cfor example: https://alb-72277319.us-west-2.elb.amazonaws.com\u3002 Grafana Service Account Token <Requires Input> Grafana Service Account Token\uff1aService Account Token created in Grafana\u3002 Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the CloudFormation Stack"},{"location":"implementation-guide/aws-services/elb/#view-dashboard_1","text":"Visualization Name Source Field Description Filters Filters The following data can be filtered by query filter conditions. Total Requests log event Displays aggregated events based on a specified time interval. Unique Visitors client_ip Displays unique visitors identified by client IP address. Requests History log event Presents a bar chart that displays the distribution of events over time. Request By Target log event target_ip Presents a bar chart that displays the distribution of events over time and IP. HTTP Status Code elb_status_code Displays the count of requests made to the ALB, grouped by HTTP status codes (e.g., 200, 404, 403, etc.). Status Code History elb_status_code Shows the historical trend of HTTP status codes returned by the ALB over a specific period of time. Status Code Pie elb_status_code Represents the distribution of requests based on different HTTP status codes using a pie chart. Average Processing Time request_processing_time response_processing_time target_processing_time This visualization calculates and presents the average time taken for various operations in the ALB. Avg. Processing Time History request_processing_time response_processing_time target_processing_time Displays the historical trend of the average time-consuming of each operation returned by the ALB within a specific period of time. HTTP Method request_verb Displays the count of requests made to the ALB using a pie chart, grouped by HTTP request method names (e.g., POST, GET, HEAD, etc.). Total Bytes received_bytes sent_bytes Provides insights into data transfer activities, including the total bytes transferred. Sent and Received Bytes History received_bytes sent_bytes Displays the historical trend of the received bytes, send bytes. SSL Protocol ssl_protocol Displays the count of requests made to the ALB, grouped by SSL Protocol. Top Request URLs request_url The web requests view enables you to analyze the top web requests. Top Client IPs client_ip Provides the top 10 IP addresses accessing your ALB. Bad Requests type client_ip target_group_arn target_ip elb_status_code request_verb request_url ssl_protocol received_bytes sent_bytes Provides a detailed list of log events, including timestamps, client IP, target IP, etc. Requests by OS ua_os Displays the count of requests made to the ALB, grouped by user agent OS. Requests by Device ua_device Displays the count of requests made to the ALB, grouped by user agent device. Requests by Browser ua_browser Displays the count of requests made to the ALB, grouped by user agent browser. Requests by Category ua_category Displays the count of category made to the ALB, grouped by user agent category (e.g., PC, Mobile, Tablet, etc.). Requests by Countries or Regions geo_iso_code Displays the count of requests made to the ALB (grouped by the corresponding country or region resolved by the client IP). Top Countries or Regions geo_country Top 10 countries with the ALB Access. Top Cities geo_city Top 10 cities with ALB Access.","title":"View Dashboard"},{"location":"implementation-guide/aws-services/include-cfn-common/","text":"Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Include cfn common"},{"location":"implementation-guide/aws-services/include-cfn-plugins-common/","text":"Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional input> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Plugins <Optional> List of plugins delimited by comma. Leave it blank if there are no available plugins to use. Valid inputs are user_agent , geo_ip . Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Include cfn plugins common"},{"location":"implementation-guide/aws-services/include-cw-cfn-common/","text":"Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the Centralized Logging with OpenSearch in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name to export the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the the logs. Log Source Account ID <Optional> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional input> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional input> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Log Group Names <Requires input> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <requires input> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <requires input> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional input> The KMS-CMK ARN for SQS encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Include cw cfn common"},{"location":"implementation-guide/aws-services/include-index-pattern/","text":"Choose the Stack Management in the left sidebar, and select Index Patterns . Choose Create index pattern , and enter the index pattern name. Choose Next step . Specify time field, and choose Create index pattern .","title":"Include index pattern"},{"location":"implementation-guide/aws-services/include-supported-service-logs/","text":"The following table lists the supported AWS services and the corresponding features. AWS Service Log Type Log Location Automatic Ingestion Built-in Dashboard Amazon CloudTrail N/A S3 Yes Yes Amazon S3 Access logs S3 Yes Yes Amazon RDS/Aurora MySQL Logs CloudWatch Logs Yes Yes Amazon CloudFront Standard access logs S3 Yes Yes Application Load Balancer Access logs S3 Yes Yes AWS WAF Web ACL logs S3 Yes Yes AWS Lambda N/A CloudWatch Logs Yes Yes Amazon VPC Flow logs S3 Yes Yes AWS Config N/A S3 Yes Yes Automatic Ingestion : The solution detects the log location of the resource automatically and then reads the logs. Built-in Dashboard : An out-of-box dashboard for the specified AWS service. The solution will automatically ingest a dashboard into the Amazon OpenSearch Service.","title":"Include supported service logs"},{"location":"implementation-guide/aws-services/lambda/","text":"AWS Lambda Logs AWS Lambda automatically monitors Lambda functions on your behalf and sends function metrics to Amazon CloudWatch. Create log ingestion You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The Lambda region must be the same as the Centralized Logging with OpenSearch solution. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Using the Centralized Logging with OpenSearch Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose AWS Lambda . Choose Next . Under Specify settings , choose the Lambda function from the dropdown list. (Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Lambda function name. In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create . Using the CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Lambda Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the Centralized Logging with OpenSearch in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name to export the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the the logs. Log Source Account ID <Optional> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional input> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional input> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Log Group Names <Requires input> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <requires input> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <requires input> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional input> The KMS-CMK ARN for SQS encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Lambda Events log event Presents a chart that displays the distribution of events over time. Log Accounts owner Shows a pie chart representing the proportion of log events from different AWS accounts (owners). Log Groups log_group Displays a pie chart depicting the distribution of log events among various log groups in the Lambda environment. Log-List time log_group log_stream log_detail Provides a detailed list of log events, including timestamps, log groups, log streams, and log details. Sample Dashboard You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"AWS Lambda"},{"location":"implementation-guide/aws-services/lambda/#aws-lambda-logs","text":"AWS Lambda automatically monitors Lambda functions on your behalf and sends function metrics to Amazon CloudWatch.","title":"AWS Lambda Logs"},{"location":"implementation-guide/aws-services/lambda/#create-log-ingestion","text":"You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The Lambda region must be the same as the Centralized Logging with OpenSearch solution. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"Create log ingestion"},{"location":"implementation-guide/aws-services/lambda/#using-the-centralized-logging-with-opensearch-console","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose AWS Lambda . Choose Next . Under Specify settings , choose the Lambda function from the dropdown list. (Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Lambda function name. In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create .","title":"Using the Centralized Logging with OpenSearch Console"},{"location":"implementation-guide/aws-services/lambda/#using-the-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Lambda Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the Centralized Logging with OpenSearch in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name to export the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the the logs. Log Source Account ID <Optional> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional input> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional input> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Log Group Names <Requires input> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <requires input> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <requires input> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional input> The KMS-CMK ARN for SQS encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Using the CloudFormation Stack"},{"location":"implementation-guide/aws-services/lambda/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Lambda Events log event Presents a chart that displays the distribution of events over time. Log Accounts owner Shows a pie chart representing the proportion of log events from different AWS accounts (owners). Log Groups log_group Displays a pie chart depicting the distribution of log events among various log groups in the Lambda environment. Log-List time log_group log_stream log_detail Provides a detailed list of log events, including timestamps, log groups, log streams, and log details.","title":"View dashboard"},{"location":"implementation-guide/aws-services/lambda/#sample-dashboard","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Sample Dashboard"},{"location":"implementation-guide/aws-services/rds/","text":"Amazon RDS/Aurora Logs You can publish database instance logs to Amazon CloudWatch Logs . Then, you can perform real-time analysis of the log data, store the data in highly durable storage, and manage the data with the CloudWatch Logs Agent. Prerequisites Make sure your database logs are enabled. Some databases logs are not enabled by default, and you need to update your database parameters to enable the logs. Refer to How do I enable and monitor logs for an Amazon RDS MySQL DB instance? to learn how to output logs to CloudWatch Logs. The table below lists the requirements for RDS/Aurora MySQL parameters. Parameter Requirement Audit Log The database instance must use a custom option group with the MARIADB_AUDIT_PLUGIN option. General log The database instance must use a custom parameter group with the parameter setting general_log = 1 to enable the general log. Slow query log The database instance must use a custom parameter group with the parameter setting slow_query_log = 1 to enable the slow query log. Log output The database instance must use a custom parameter group with the parameter setting log_output = FILE to write logs to the file system and publish them to CloudWatch Logs. Create log ingestion You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The RDS and CloudWatch region must be the same as the Centralized Logging with OpenSearch solution region. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Using the Centralized Logging with OpenSearch Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Amazon RDS . Choose Next . Under Specify settings , choose Automatic or Manual for RDS log enabling . The automatic mode will detect your RDS log configurations and ingest logs from CloudWatch. For Automatic mode , choose the RDS cluster from the dropdown list. For Manual mode , enter the DB identifier , select the Database type and input the CloudWatch log location in Log type and location . (Optional) If you are ingesting RDS/Aurora logs from another account, select a linked account from the Account dropdown first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Database identifier . In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create . Using the CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - RDS Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the Centralized Logging with OpenSearch in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name to export the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the the logs. Log Source Account ID <Optional> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional input> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional input> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Log Group Names <Requires input> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <requires input> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <requires input> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional input> The KMS-CMK ARN for SQS encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Controller db-identifier sq-table-name This visualization allows users to filter data based on the db-identifier and sq-table-name fields. Total Log Events Overview db-identifier log event This visualization presents an overview of the total log events for the specified database ('db-identifier'). It helps monitor the frequency of various log events. Slow Query History log event This visualization shows the historical data of slow query log events. It allows you to track the occurrences of slow queries and identify potential performance issues. Average Slow Query Time History Average sq-duration This visualization depicts the historical trend of the average duration of slow queries ('sq-duration'). It helps in understanding the database's performance over time and identifying trends related to slow query durations. Total Slow Queries log event This visualization provides the total count of slow queries in the log events. It gives an immediate view of how many slow queries have occurred during a specific time period, which is useful for assessing the database's performance and potential bottlenecks. Average Slow Query Duration Average sq-duration This visualization shows the average duration of slow queries ('sq-duration') over time. It is valuable for understanding the typical performance of slow queries in the database. Top Slow Query IP sq-ip sq-duration This visualization highlights the IP addresses ('sq-ip') associated with the slowest queries and their respective durations ('sq-duration'). It helps identify sources of slow queries and potential areas for optimization. Slow Query Scatter Plot sq-duration sq-ip sq-query This scatter plot visualization represents the relationship between the duration of slow queries ('sq-duration'), the IP addresses ('sq-ip') from which they originated, and the actual query content ('sq-query'). It helps in understanding query performance patterns and identifying potential issues related to specific queries and their sources. Slow Query Pie sq-query This pie chart visualization shows the distribution of slow queries based on their content ('sq-query'). It provides an overview of the types of queries causing performance issues, allowing you to focus on optimizing specific query patterns. Slow Query Table Name Pie sq-table-name This pie chart visualization displays the distribution of slow queries based on the table names ('sq-table-name') they access. It helps identify which tables are affected by slow queries, enabling targeted optimization efforts for specific tables. Top Slow Query sq-query This visualization presents the slowest individual queries based on their content ('sq-query'). It is helpful in pinpointing specific queries that have the most significant impact on performance, allowing developers and administrators to focus on optimizing these critical queries. Slow Query Logs db-identifier sq-db-name sq-table-name sq-query sq-ip sq-host-name sq-rows-examined sq-rows-sent sq-id sq-duration sq-lock-wait This visualization provides detailed logs of slow queries, including database ('sq-db-name'), table ('sq-table-name'), query content ('sq-query'), IP address ('sq-ip'), host name ('sq-host-name'), rows examined ('sq-rows-examined'), rows sent ('sq-rows-sent'), query ID ('sq-id'), query duration ('sq-duration'), and lock wait time ('sq-lock-wait'). It is beneficial for in-depth analysis and troubleshooting of slow query performance. Total Deadlock Queries log event This visualization shows the total number of deadlock occurrences based on the log events. Deadlocks are critical issues that can cause database transactions to fail, and monitoring their frequency is essential for ensuring database stability. Deadlock History log event This visualization displays the historical data of deadlock occurrences based on the log events. Understanding the pattern of deadlocks over time can help identify recurring issues and take preventive measures to reduce their impact on the database. Deadlock Query Logs db-identifier log-detail deadlock-ip-1 deadlock-action-1 deadlock-os-thread-handle-1 deadlock-query-1 deadlock-query-id-1 deadlock-thread-id-1 deadlock-user-1 deadlock-action-2 deadlock-ip-2 deadlock-os-thread-handle-2 deadlock-query-2 deadlock-query-id-2 deadlock-thread-id-2 deadlock-user-2 This visualization provides detailed logs of deadlock occurrences Total Error Logs log event This visualization presents the total count of error log events. Monitoring error logs helps identify database issues and potential errors that need attention and resolution. Error History log event This visualization shows the historical data of error log events. Understanding the error patterns over time can aid in identifying recurring issues and taking corrective actions to improve the database's overall health and stability. Error Logs db-identifier err-label err-code err-detail err-sub-system err-thread This visualization displays the error logs generated by the AWS RDS instance. It provides valuable insights into any errors, warnings, or issues encountered within the database system, helping to identify and troubleshoot problems effectively. Monitoring error logs is essential for maintaining the health and reliability of the database. Audit History log event This visualization presents the audit history of the AWS RDS instance. It tracks the various log events and activities related to database access, modifications, and security-related events. Monitoring the audit logs is crucial for ensuring compliance, detecting unauthorized access, and keeping track of changes made to the database. Audit Logs db-identifier audit-operation audit-ip audit-query audit-retcode audit-connection-id audit-host-name audit-query-id audit-user This visualization provides an overview of the audit logs generated by the AWS RDS instance. It shows the operations performed on the database, including queries executed, connection details, IP addresses, and associated users. Monitoring audit logs enhances the security and governance of the database, helping to detect suspicious activities and track user actions. Sample Dashboard You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Amazon RDS/Aurora"},{"location":"implementation-guide/aws-services/rds/#amazon-rdsaurora-logs","text":"You can publish database instance logs to Amazon CloudWatch Logs . Then, you can perform real-time analysis of the log data, store the data in highly durable storage, and manage the data with the CloudWatch Logs Agent.","title":"Amazon RDS/Aurora Logs"},{"location":"implementation-guide/aws-services/rds/#prerequisites","text":"Make sure your database logs are enabled. Some databases logs are not enabled by default, and you need to update your database parameters to enable the logs. Refer to How do I enable and monitor logs for an Amazon RDS MySQL DB instance? to learn how to output logs to CloudWatch Logs. The table below lists the requirements for RDS/Aurora MySQL parameters. Parameter Requirement Audit Log The database instance must use a custom option group with the MARIADB_AUDIT_PLUGIN option. General log The database instance must use a custom parameter group with the parameter setting general_log = 1 to enable the general log. Slow query log The database instance must use a custom parameter group with the parameter setting slow_query_log = 1 to enable the slow query log. Log output The database instance must use a custom parameter group with the parameter setting log_output = FILE to write logs to the file system and publish them to CloudWatch Logs.","title":"Prerequisites"},{"location":"implementation-guide/aws-services/rds/#create-log-ingestion","text":"You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The RDS and CloudWatch region must be the same as the Centralized Logging with OpenSearch solution region. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"Create log ingestion"},{"location":"implementation-guide/aws-services/rds/#using-the-centralized-logging-with-opensearch-console","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Amazon RDS . Choose Next . Under Specify settings , choose Automatic or Manual for RDS log enabling . The automatic mode will detect your RDS log configurations and ingest logs from CloudWatch. For Automatic mode , choose the RDS cluster from the dropdown list. For Manual mode , enter the DB identifier , select the Database type and input the CloudWatch log location in Log type and location . (Optional) If you are ingesting RDS/Aurora logs from another account, select a linked account from the Account dropdown first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Database identifier . In the Log Lifecycle section, input the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create .","title":"Using the Centralized Logging with OpenSearch Console"},{"location":"implementation-guide/aws-services/rds/#using-the-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - RDS Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the Centralized Logging with OpenSearch in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name to export the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the the logs. Log Source Account ID <Optional> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional input> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional input> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Log Group Names <Requires input> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <requires input> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <requires input> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional input> The KMS-CMK ARN for SQS encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Using the CloudFormation Stack"},{"location":"implementation-guide/aws-services/rds/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Controller db-identifier sq-table-name This visualization allows users to filter data based on the db-identifier and sq-table-name fields. Total Log Events Overview db-identifier log event This visualization presents an overview of the total log events for the specified database ('db-identifier'). It helps monitor the frequency of various log events. Slow Query History log event This visualization shows the historical data of slow query log events. It allows you to track the occurrences of slow queries and identify potential performance issues. Average Slow Query Time History Average sq-duration This visualization depicts the historical trend of the average duration of slow queries ('sq-duration'). It helps in understanding the database's performance over time and identifying trends related to slow query durations. Total Slow Queries log event This visualization provides the total count of slow queries in the log events. It gives an immediate view of how many slow queries have occurred during a specific time period, which is useful for assessing the database's performance and potential bottlenecks. Average Slow Query Duration Average sq-duration This visualization shows the average duration of slow queries ('sq-duration') over time. It is valuable for understanding the typical performance of slow queries in the database. Top Slow Query IP sq-ip sq-duration This visualization highlights the IP addresses ('sq-ip') associated with the slowest queries and their respective durations ('sq-duration'). It helps identify sources of slow queries and potential areas for optimization. Slow Query Scatter Plot sq-duration sq-ip sq-query This scatter plot visualization represents the relationship between the duration of slow queries ('sq-duration'), the IP addresses ('sq-ip') from which they originated, and the actual query content ('sq-query'). It helps in understanding query performance patterns and identifying potential issues related to specific queries and their sources. Slow Query Pie sq-query This pie chart visualization shows the distribution of slow queries based on their content ('sq-query'). It provides an overview of the types of queries causing performance issues, allowing you to focus on optimizing specific query patterns. Slow Query Table Name Pie sq-table-name This pie chart visualization displays the distribution of slow queries based on the table names ('sq-table-name') they access. It helps identify which tables are affected by slow queries, enabling targeted optimization efforts for specific tables. Top Slow Query sq-query This visualization presents the slowest individual queries based on their content ('sq-query'). It is helpful in pinpointing specific queries that have the most significant impact on performance, allowing developers and administrators to focus on optimizing these critical queries. Slow Query Logs db-identifier sq-db-name sq-table-name sq-query sq-ip sq-host-name sq-rows-examined sq-rows-sent sq-id sq-duration sq-lock-wait This visualization provides detailed logs of slow queries, including database ('sq-db-name'), table ('sq-table-name'), query content ('sq-query'), IP address ('sq-ip'), host name ('sq-host-name'), rows examined ('sq-rows-examined'), rows sent ('sq-rows-sent'), query ID ('sq-id'), query duration ('sq-duration'), and lock wait time ('sq-lock-wait'). It is beneficial for in-depth analysis and troubleshooting of slow query performance. Total Deadlock Queries log event This visualization shows the total number of deadlock occurrences based on the log events. Deadlocks are critical issues that can cause database transactions to fail, and monitoring their frequency is essential for ensuring database stability. Deadlock History log event This visualization displays the historical data of deadlock occurrences based on the log events. Understanding the pattern of deadlocks over time can help identify recurring issues and take preventive measures to reduce their impact on the database. Deadlock Query Logs db-identifier log-detail deadlock-ip-1 deadlock-action-1 deadlock-os-thread-handle-1 deadlock-query-1 deadlock-query-id-1 deadlock-thread-id-1 deadlock-user-1 deadlock-action-2 deadlock-ip-2 deadlock-os-thread-handle-2 deadlock-query-2 deadlock-query-id-2 deadlock-thread-id-2 deadlock-user-2 This visualization provides detailed logs of deadlock occurrences Total Error Logs log event This visualization presents the total count of error log events. Monitoring error logs helps identify database issues and potential errors that need attention and resolution. Error History log event This visualization shows the historical data of error log events. Understanding the error patterns over time can aid in identifying recurring issues and taking corrective actions to improve the database's overall health and stability. Error Logs db-identifier err-label err-code err-detail err-sub-system err-thread This visualization displays the error logs generated by the AWS RDS instance. It provides valuable insights into any errors, warnings, or issues encountered within the database system, helping to identify and troubleshoot problems effectively. Monitoring error logs is essential for maintaining the health and reliability of the database. Audit History log event This visualization presents the audit history of the AWS RDS instance. It tracks the various log events and activities related to database access, modifications, and security-related events. Monitoring the audit logs is crucial for ensuring compliance, detecting unauthorized access, and keeping track of changes made to the database. Audit Logs db-identifier audit-operation audit-ip audit-query audit-retcode audit-connection-id audit-host-name audit-query-id audit-user This visualization provides an overview of the audit logs generated by the AWS RDS instance. It shows the operations performed on the database, including queries executed, connection details, IP addresses, and associated users. Monitoring audit logs enhances the security and governance of the database, helping to detect suspicious activities and track user actions.","title":"View dashboard"},{"location":"implementation-guide/aws-services/rds/#sample-dashboard","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Sample Dashboard"},{"location":"implementation-guide/aws-services/s3/","text":"Amazon S3 Logs Amazon S3 server access logging provides detailed records for the requests made to the bucket. S3 access logs can be enabled and saved in another S3 bucket. Create log ingestion You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The S3 Bucket region must be the same as the Centralized Logging with OpenSearch solution region. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Using the Centralized Logging with OpenSearch Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Amazon S3 . Choose Next . Under Specify settings , choose Automatic or Manual for S3 Access Log enabling . The automatic mode will enable the S3 Access Log and save the logs to a centralized S3 bucket if logging is not enabled yet. For Automatic mode , choose the S3 bucket from the dropdown list. For Manual mode , enter the Bucket Name and S3 Access Log location . (Optional) If you are ingesting Amazon S3 logs from another account, select a linked account from the Account dropdown list first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your bucket name. In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create . Using the standalone CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - S3 Access Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Total Requests log event A visualization showing the total number of requests made to the AWS S3 bucket, including all types of operations (e.g., GET, PUT, DELETE). Unique Visitors log event This visualization displays the count of unique visitors accessing the AWS S3 bucket, identified by their IP addresses. Access History log event Provides a chronological log of all access events made to the AWS S3 bucket, including details about the operations and their outcomes. Request By Operation operation This visualization categorizes and shows the distribution of requests based on different operations, such as GET, PUT, DELETE, etc. Status Code http_status Displays the count of requests made to the AWS S3 bucket, grouped by HTTP status codes returned by the server (e.g., 200, 404, 403, etc.). Status Code History http_status Shows the historical trend of HTTP status codes returned by the AWS S3 server over a specific period of time. Status Code Pie http_status Represents the distribution of requests based on different HTTP status codes using a pie chart. Average Time total_time This visualization calculates and presents the average time taken for various operations in the AWS S3 bucket (e.g., average time for GET, PUT requests, etc.). Average Turn Around Time turn_around_time Shows the average turnaround time for different operations, which is the time between receiving a request and sending the response back to the client. Data Transfer bytes_sent object_size operation Provides insights into data transfer activities, including the total bytes transferred, object sizes, and different operations involved. Top Client IPs remote_ip Displays the top client IP addresses with the highest number of requests made to the AWS S3 bucket. Top Request Keys key object_size Shows the top requested keys in the AWS S3 bucket along with the corresponding object sizes. Delete Events operation key version_id object_size remote_ip http_status error_code Focuses on delete events, including the operation, key, version ID, object size, client IP, HTTP status, and error code associated with the delete requests. Access Failures operation key version_id object_size remote_ip http_status error_code Highlights access failures, showing the details of the failed requests, including operation, key, version ID, object size, client IP, HTTP status, and error code. Sample Dashboard You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Amazon S3"},{"location":"implementation-guide/aws-services/s3/#amazon-s3-logs","text":"Amazon S3 server access logging provides detailed records for the requests made to the bucket. S3 access logs can be enabled and saved in another S3 bucket.","title":"Amazon S3 Logs"},{"location":"implementation-guide/aws-services/s3/#create-log-ingestion","text":"You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important The S3 Bucket region must be the same as the Centralized Logging with OpenSearch solution region. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"Create log ingestion"},{"location":"implementation-guide/aws-services/s3/#using-the-centralized-logging-with-opensearch-console","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose Amazon S3 . Choose Next . Under Specify settings , choose Automatic or Manual for S3 Access Log enabling . The automatic mode will enable the S3 Access Log and save the logs to a centralized S3 bucket if logging is not enabled yet. For Automatic mode , choose the S3 bucket from the dropdown list. For Manual mode , enter the Bucket Name and S3 Access Log location . (Optional) If you are ingesting Amazon S3 logs from another account, select a linked account from the Account dropdown list first. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your bucket name. In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create .","title":"Using the Centralized Logging with OpenSearch Console"},{"location":"implementation-guide/aws-services/s3/#using-the-standalone-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - S3 Access Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the standalone CloudFormation Stack"},{"location":"implementation-guide/aws-services/s3/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Total Requests log event A visualization showing the total number of requests made to the AWS S3 bucket, including all types of operations (e.g., GET, PUT, DELETE). Unique Visitors log event This visualization displays the count of unique visitors accessing the AWS S3 bucket, identified by their IP addresses. Access History log event Provides a chronological log of all access events made to the AWS S3 bucket, including details about the operations and their outcomes. Request By Operation operation This visualization categorizes and shows the distribution of requests based on different operations, such as GET, PUT, DELETE, etc. Status Code http_status Displays the count of requests made to the AWS S3 bucket, grouped by HTTP status codes returned by the server (e.g., 200, 404, 403, etc.). Status Code History http_status Shows the historical trend of HTTP status codes returned by the AWS S3 server over a specific period of time. Status Code Pie http_status Represents the distribution of requests based on different HTTP status codes using a pie chart. Average Time total_time This visualization calculates and presents the average time taken for various operations in the AWS S3 bucket (e.g., average time for GET, PUT requests, etc.). Average Turn Around Time turn_around_time Shows the average turnaround time for different operations, which is the time between receiving a request and sending the response back to the client. Data Transfer bytes_sent object_size operation Provides insights into data transfer activities, including the total bytes transferred, object sizes, and different operations involved. Top Client IPs remote_ip Displays the top client IP addresses with the highest number of requests made to the AWS S3 bucket. Top Request Keys key object_size Shows the top requested keys in the AWS S3 bucket along with the corresponding object sizes. Delete Events operation key version_id object_size remote_ip http_status error_code Focuses on delete events, including the operation, key, version ID, object size, client IP, HTTP status, and error code associated with the delete requests. Access Failures operation key version_id object_size remote_ip http_status error_code Highlights access failures, showing the details of the failed requests, including operation, key, version ID, object size, client IP, HTTP status, and error code.","title":"View dashboard"},{"location":"implementation-guide/aws-services/s3/#sample-dashboard","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Sample Dashboard"},{"location":"implementation-guide/aws-services/vpc/","text":"VPC Flow Logs VPC Flow Logs enable you to capture information about the IP traffic going to and from network interfaces in your VPC. Create log ingestion You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important Centralized Logging with OpenSearch supports VPCs who publish the flow log data to an Amazon S3 bucket or a CloudWatch log group. When publishing to S3, The S3 Bucket region must be the same as the Centralized Logging with OpenSearch solution region. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Using the Centralized Logging with OpenSearch Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose VPC Flow Logs . Choose Next . Under Specify settings , choose Automatic or Manual for VPC Flow Log enabling . The automatic mode will enable the VPC Flow Log and save the logs to a centralized S3 bucket if logging is not enabled yet. For Automatic mode , choose the VPC from the dropdown list. For Manual mode , enter the VPC Name and VPC Flow Logs location . (Optional) If you are ingesting VPC Flow logs from another account, select a linked account from the Account dropdown list first. Under Log Source , select S3 or CloudWatch as the source. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your VPC name. In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create . Using the standalone CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - VPC Flow Logs Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Global Filters account-id region vpc-id subnet-id action flow-direction log-status protocol-code type The charts are filtered according to Account ID, Region, VPC ID and other conditions. Total Requests log event Shows the total number of network requests logged by VPC Flow Logs during a selected time period. Request History log event Presents a bar chart that displays the distribution of events over time. Requests By VPC ID vpc-id Displays the proportional breakdown of network requests by source VPC using a pie chart. Total Requests By Action action Displays the total volume of requests segmented by action over time. Total Bytes bytes Provides visibility into overall bandwidth usage and traffic patterns across the monitored VPCs, subnets, network interfaces and security groups. Total Packets packets Displays total logged packets over time to visualize trends, surges and dips. Bytes Metric bytes flow-direction Shows the distribution of incoming (Ingress) and outgoing (Egress) network traffic volumes in bytes across the range of flows logged by VPC Flow Logs over a time period. Requests By Direction flow-direction Provides visibility into the proportional composition of incoming versus outgoing requests. Requests By Direction flow-direction Displays the total number of network flows logged by VPC Flow Logs segmented by traffic direction - Ingress vs Egress. Requests By Type type Shows the volume of flows for each type. This provides visibility into the protocol composition of network requests traversing the environment. Top Source Bytes srcaddr bytes Displays the source IP addresses transmitting the highest outbound volume of data during the selected time period. Top Destination Bytes dstaddr bytes Enables you to monitor and analyze outbound traffic from your VPC to external destinations. Top Source Requests srcaddr Allows you to see which resources inside your VPC are initiating external requests. Top Destination Requests dstaddr Allows you to see which external hosts are being contacted most by your VPC resources. Requests by Protocol protocol-code Displays network flows logged by VPC Flow Logs segmented by traffic type - TCP, UDP, ICMP etc. Requests by Status log-status Provides a breakdown of network flows by their traffic status - Accepted, Rejected or Other. Top Sources AWS Services pkt-src-aws-service Show the proportional distribution of flows originating from top AWS sources like S3, CloudFront, Lambda, etc. during the selected time period. Top Destination AWS Services pkt-dst-aws-service Provide visibility into IP traffic going to and from AWS services located outside your VPC. By enabling flow logs on VPC subnets/interfaces and filtering on traffic with an ACCEPT action, you can view outbound flows from your VPC to various AWS services. Network Flow srcaddr dstaddr Allows you to view information about the IP traffic going to and from network interfaces in your VPC. Heat Map srcaddr dstaddr Offers a visual summary of connections between source and destination IPs in your flow log data. Egress Traffic Path traffic-path Allows you to enable flow logging on VPC network interfaces to capture information about all IP traffic going to and from that interface. Search @timestamp account-id vpc-id flow-direction action protocol-code srcaddr scaport dstaddr dstport bytes packets log-status Searching through the detailed flow log data allows pinpoint analysis of traffic around security events, network issues, changes in usage patterns, and more. Sample Dashboard You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"VPC Flow Log"},{"location":"implementation-guide/aws-services/vpc/#vpc-flow-logs","text":"VPC Flow Logs enable you to capture information about the IP traffic going to and from network interfaces in your VPC.","title":"VPC Flow Logs"},{"location":"implementation-guide/aws-services/vpc/#create-log-ingestion","text":"You can create a log ingestion into Amazon OpenSearch Service either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important Centralized Logging with OpenSearch supports VPCs who publish the flow log data to an Amazon S3 bucket or a CloudWatch log group. When publishing to S3, The S3 Bucket region must be the same as the Centralized Logging with OpenSearch solution region. The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"Create log ingestion"},{"location":"implementation-guide/aws-services/vpc/#using-the-centralized-logging-with-opensearch-console","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose VPC Flow Logs . Choose Next . Under Specify settings , choose Automatic or Manual for VPC Flow Log enabling . The automatic mode will enable the VPC Flow Log and save the logs to a centralized S3 bucket if logging is not enabled yet. For Automatic mode , choose the VPC from the dropdown list. For Manual mode , enter the VPC Name and VPC Flow Logs location . (Optional) If you are ingesting VPC Flow logs from another account, select a linked account from the Account dropdown list first. Under Log Source , select S3 or CloudWatch as the source. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated built-in Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is your VPC name. In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create .","title":"Using the Centralized Logging with OpenSearch Console"},{"location":"implementation-guide/aws-services/vpc/#using-the-standalone-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - VPC Flow Logs Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<Log Type>-<Other Suffix> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Age to Warm Storage <Optional> The age required to move the index into warm storage (e.g. 7d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when warm storage is enabled in OpenSearch. Age to Cold Storage <Optional> The age required to move the index into cold storage (e.g. 30d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). This is only effective when cold storage is enabled in OpenSearch. Age to Retain <Optional> The age to retain the index (e.g. 180d). Index age is the time between its creation and the present. Supported units are d (days) and h (hours). If value is \"\", the index will not be deleted. Rollover Index Size <Optional> The minimum size of the shard storage required to roll over the index (e.g. 30GB). Index Suffix yyyy-MM-dd The common suffix format of OpenSearch index for the log(Example: yyyy-MM-dd, yyyy-MM-dd-HH). The index name will be <Index Prefix>-<Log Type>-<Index Suffix>-000001 . Compression type best_compression The compression type to use to compress stored data. Available values are best_compression and default. Refresh Interval 1s How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. EnableS3Notification True An option to enable or disable notifications for Amazon S3 buckets. The default option is recommended for most cases. LogProcessorRoleName <Optional> Specify a role name for the log processor. The name should NOT duplicate an existing role name. If no name is specified, a random name is generated. QueueName <Optional> Specify a queue name for an SQS. The name should NOT duplicate an existing queue name. If no name is given, a random name will be generated. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the standalone CloudFormation Stack"},{"location":"implementation-guide/aws-services/vpc/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Global Filters account-id region vpc-id subnet-id action flow-direction log-status protocol-code type The charts are filtered according to Account ID, Region, VPC ID and other conditions. Total Requests log event Shows the total number of network requests logged by VPC Flow Logs during a selected time period. Request History log event Presents a bar chart that displays the distribution of events over time. Requests By VPC ID vpc-id Displays the proportional breakdown of network requests by source VPC using a pie chart. Total Requests By Action action Displays the total volume of requests segmented by action over time. Total Bytes bytes Provides visibility into overall bandwidth usage and traffic patterns across the monitored VPCs, subnets, network interfaces and security groups. Total Packets packets Displays total logged packets over time to visualize trends, surges and dips. Bytes Metric bytes flow-direction Shows the distribution of incoming (Ingress) and outgoing (Egress) network traffic volumes in bytes across the range of flows logged by VPC Flow Logs over a time period. Requests By Direction flow-direction Provides visibility into the proportional composition of incoming versus outgoing requests. Requests By Direction flow-direction Displays the total number of network flows logged by VPC Flow Logs segmented by traffic direction - Ingress vs Egress. Requests By Type type Shows the volume of flows for each type. This provides visibility into the protocol composition of network requests traversing the environment. Top Source Bytes srcaddr bytes Displays the source IP addresses transmitting the highest outbound volume of data during the selected time period. Top Destination Bytes dstaddr bytes Enables you to monitor and analyze outbound traffic from your VPC to external destinations. Top Source Requests srcaddr Allows you to see which resources inside your VPC are initiating external requests. Top Destination Requests dstaddr Allows you to see which external hosts are being contacted most by your VPC resources. Requests by Protocol protocol-code Displays network flows logged by VPC Flow Logs segmented by traffic type - TCP, UDP, ICMP etc. Requests by Status log-status Provides a breakdown of network flows by their traffic status - Accepted, Rejected or Other. Top Sources AWS Services pkt-src-aws-service Show the proportional distribution of flows originating from top AWS sources like S3, CloudFront, Lambda, etc. during the selected time period. Top Destination AWS Services pkt-dst-aws-service Provide visibility into IP traffic going to and from AWS services located outside your VPC. By enabling flow logs on VPC subnets/interfaces and filtering on traffic with an ACCEPT action, you can view outbound flows from your VPC to various AWS services. Network Flow srcaddr dstaddr Allows you to view information about the IP traffic going to and from network interfaces in your VPC. Heat Map srcaddr dstaddr Offers a visual summary of connections between source and destination IPs in your flow log data. Egress Traffic Path traffic-path Allows you to enable flow logging on VPC network interfaces to capture information about all IP traffic going to and from that interface. Search @timestamp account-id vpc-id flow-direction action protocol-code srcaddr scaport dstaddr dstport bytes packets log-status Searching through the detailed flow log data allows pinpoint analysis of traffic around security events, network issues, changes in usage patterns, and more.","title":"View dashboard"},{"location":"implementation-guide/aws-services/vpc/#sample-dashboard","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Sample Dashboard"},{"location":"implementation-guide/aws-services/waf/","text":"AWS WAF Logs WAF Access logs provide detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can create a log ingestion into Amazon OpenSearch Service or Light Engine either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important You must deploy Centralized Logging with OpenSearch solution in the same region as your Web ACLs, or you will not be able to create a WAF pipeline. For example: If your Web ACL is associated with Global Cloudfront, your must deploy the solution in us-east-1. If your Web ACL is associated with other resources in regions like Ohio, your Centralized Logging with OpenSearch stack must also be deployed in that region. The WAF logging bucket must be the same as the Centralized Logging with OpenSearch solution. WAF Classic logs are not supported in Centralized Logging with OpenSearch. Learn more about migrating rules from WAF Classic to the new AWS WAF . The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings. Create log ingestion (Amazon OpenSearch for log analytics) Using the Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose AWS WAF . Choose Open Search , choose Next . Under Specify settings , choose Automatic or Manual . For Automatic mode, choose a Web ACL in the dropdown list. For Manual mode, enter the Web ACL name . (Optional) If you are ingesting WAF logs from another account, select a linked account from the Account dropdown list first. Specify an Ingest Options . Choose between Sampled Request or Full Request . For Sampled Request , enter how often you want to ingest sampled requests in minutes. For Full Request , if the Web ACL log is not enabled, choose Enable to enable the access log, or enter Log location in Manual mode. Note that Centralized Logging with OpenSearch will automatically enable logging with a Kinesis Data Firehose stream as destination for your WAF. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Web ACL Name . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create . Using the CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - WAF Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions (Full Request) Template AWS China Regions (Full Request) Template AWS Regions (Sampled Request) Template AWS China Regions (Sampled Request) Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameters for Full Request only Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Parameters for Sampled Request only Parameter Default Description WebACL Names <Requires input> The list of Web ACL names, delimited by comma. Interval 1 The default interval (in minutes) to get sampled logs. Common parameters Parameter Default Description Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<log-type>-<YYYY-MM-DD> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional input> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View dashboard The dashboard includes the following visualizations. Visualization Name Source Field Description Filters Filters The following data can be filtered by query filter conditions. Web ACLs log event webaclName Displays the count of requests made to the WAF, grouped by Web ACL Names. Total Requests log event Displays the total number of web requests. Request Timeline log event Presents a bar chart that displays the distribution of events over time. WAF Rules terminatingRuleId Presents a pie chart that displays the distribution of events over the WAF rules in the Web ACL. Total Blocked Requests log event Displays the total number of blocked web requests. Unique Client IPs Request.ClientIP Displays unique visitors identified by client IP. Country or Region By Request Request.Country Displays the count of requests made to the Web ACL (grouped by the corresponding country or region resolved by the client IP). Http Methods Request.HTTPMethod Displays the count of requests made to the Web ACL using a pie chart, grouped by http request method names (e.g., POST, GET, HEAD, etc.). Http Versions Request.HTTPVersion Displays the count of requests made to the Web ACL using a pie chart, grouped by http protocol version (e.g., HTTP/2.0, HTTP/1.1, etc.). Top WebACLs webaclName webaclId.keyword The web requests view enables you to analyze the top web requests. Top Hosts host Lists the source IP addresses associated with events, enabling you to identify and investigate potentially suspicious or unauthorized activities. Top Request URIs Request.URI Top 10 request URIs. Top Countries or Regions Request.country Top 10 countries with the Web ACL Access. Top Rules terminatingRuleId Top 10 rules in the web ACL that matched the request. Top Client IPs Request.ClientIP Provides the top 10 IP address. Top User Agents userAgent Provides the top 10 user agents Block Allow Host Uri host Request.URI action Provides blocked or allowed web requests. Top Labels with Host, Uri labels.name host Request.URI Top 10 detailed logs by labels with host, URI View by Matching Rule sc-status This visualization provides detailed logs by DQL \"terminatingRuleId:*\". View by httpRequest args,uri,path sc-status This visualization provides detailed logs by DQL. Sample Dashboard You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard. Create log ingestion (Light Engine for log analytics) Using the Console Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose AWS WAF . Choose Light Engine , choose Next . Under the Specified Settings , select Automatic or Manual . For the Automatic mode, choose a Web ACL from the dropdown list. For the Manual mode, enter the Web ACL name. (Optional) If you need to ingest logs across AWS accounts, select a linked AWS account from the account dropdown list. In the Ingestion Options section, select Full Request . For Full Request , if Web ACL logging is not enabled, click Enable Access Logging to enable access logs. Alternatively, enter the log location in manual mode. Note that using the log delivery stream will automatically enable using Kinesis Data Firehose as the target for WAF logs. Select Next . In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . If desired, add tags. Select Create . Using the CloudFormation Stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - WAF Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Region(Full Request) Template AWS China Regions (Full Request) Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameters for Pipeline settings Parameter Defaul Description Pipeline Id < <\u9700\u8981\u8f93\u5165> input> The unique identifier for the pipeline is essential if you need to create multiple WAF pipelines and write different WAF logs into separate tables. To ensure uniqueness, you can generate a unique pipeline identifier using uuidgenerator . Staging Bucket Prefix AWSLogs/WAFLogs The storage directory for logs in the temporary storage area should ensure the uniqueness and non-overlapping of the Prefix for different pipelines. Parameters for Destination settings Parameters Default Description Centralized Bucket Name <Requires input> Input centralized s3 bucket name\uff0cfor expample:centralized-logging-bucket\u3002 Centralized Bucket Prefix datalake Input centralized bucket prefix\uff0cdefault is datalake which means your data base's location is s3://{Centralized Bucket Name}/{Centralized Bucket Prefix}/amazon_cl_centralized\u3002 Centralized Table Name WAF Table name for writing data to the centralized database, can be defined as needed, default value is 'waf'. Parameters for Scheduler settings Parameters Default Description LogProcessor Schedule Expression rate(5 minutes) Task scheduling expression for performing log processing, with a default value of executing the LogProcessor every 5 minutes. Configuration for reference \u3002 LogMerger Schedule Expression cron(0 1 * ? ) \u6267ask scheduling expression for performing log merging, with a default value of executing the LogMerger at 1 AM every day. Configuration for reference \u3002 LogArchive Schedule Expression cron(0 2 * ? ) Task scheduling expression for performing log archiving, with a default value of executing the LogArchive at 2 AM every day. Configuration for reference \u3002 Age to Merge 7 Small file retention days, with a default value of 7, indicates that logs older than 7 days will be merged into small files. It can be adjusted as needed. Age to Archive 30 Log retention days, with a default value of 30, indicates that data older than 30 days will be archived and deleted. It can be adjusted as needed. Parameters for Notification settings Parameters Default Description Notification Service SNS Notification method for alerts. If your main stack is using China, you can only choose the SNS method. If your main stack is using Global, you can choose either the SNS or SES method. Recipients <Requires Input> Alert notification: If the Notification Service is SNS, enter the SNS Topic ARN here, ensuring that you have the necessary permissions. If the Notification Service is SES, enter the email addresses separated by commas here, ensuring that the email addresses are already Verified Identities in SES. The adminEmail provided during the creation of the main stack will receive a verification email by default. Parameters for Dashboard settings Parameters Default Description Import Dashboards FALSE Whether to import the Dashboard into Grafana, with a default value of false. If set to true, you must provide the Grafana URL and Grafana Service Account Token.\u3002 Grafana URL <Requires Input> Grafana access URL\uff0cfor example: https://alb-72277319.us-west-2.elb.amazonaws.com\u3002 Grafana Service Account Token <Requires Input> Grafana Service Account Token\uff1aService Account Token created in Grafana\u3002 Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes. View Dashboard Visualization Name Source Field Description Filters Filters The following data can be filtered by query filter conditions. Total Requests log event Displays the total number of web requests. Total Blocked Requests log event Displays the total number of blocked web requests. Requests History log event Presents a bar chart that displays the distribution of events over time. WAF ACLs log event webaclName Displays the count of requests made to the WAF, grouped by Web ACL Names. WAF Rules terminatingRuleId Presents a pie chart that displays the distribution of events over the WAF rules in the Web ACL. Sources httpSourceId Presents a pie chart that displays the distribution of events over the id of the associated resource. HTTP Methods httpRequest.HTTPMethod Displays the count of requests made to the Web ACL using a pie chart, grouped by HTTP request method names (e.g., POST, GET, HEAD, etc.). Country or Region By Blocked Requests HTTPRequest.Country Displays the count of blocked web requests made to the Web ACL (grouped by the corresponding country or region resolved by the client IP). Top WebACLs webaclName The web requests view enables you to analyze the top web requests. Top Sources httpSourceId Top 10 id of the associated resource. Top Requests URIs httpRequest.URI Top 10 request URIs. Top Countries or Regions httpRequest.country Top 10 countries with the Web ACL Access. Top Rules terminatingRuleId Top 10 rules in the web ACL that matched the request. Top Client IPs httpRequest.ClientIP Provides the top 10 IP addresses. Top Blocked / Allowed Hosts URI host httpRequest.URI action Provides blocked or allowed web requests. Top Labels with Host, URI labels host httpRequest.URI Top 10 detailed logs by labels with host, URI. Metrics webaclId webaclName terminatingRuleId terminatingRuleType httpSourceId httpRequest.HTTPMethod httpRequest.country httpRequest.ClientIP labels httpRequest.URI action Provides a detailed list of log events, including timestamps, WebACL, client IP, etc.","title":"AWS WAF"},{"location":"implementation-guide/aws-services/waf/#aws-waf-logs","text":"WAF Access logs provide detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can create a log ingestion into Amazon OpenSearch Service or Light Engine either by using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Important You must deploy Centralized Logging with OpenSearch solution in the same region as your Web ACLs, or you will not be able to create a WAF pipeline. For example: If your Web ACL is associated with Global Cloudfront, your must deploy the solution in us-east-1. If your Web ACL is associated with other resources in regions like Ohio, your Centralized Logging with OpenSearch stack must also be deployed in that region. The WAF logging bucket must be the same as the Centralized Logging with OpenSearch solution. WAF Classic logs are not supported in Centralized Logging with OpenSearch. Learn more about migrating rules from WAF Classic to the new AWS WAF . The Amazon OpenSearch Service index is rotated on a daily basis by default, and you can adjust the index in the Additional Settings.","title":"AWS WAF Logs"},{"location":"implementation-guide/aws-services/waf/#create-log-ingestion-amazon-opensearch-for-log-analytics","text":"","title":"Create log ingestion (Amazon OpenSearch for log analytics)"},{"location":"implementation-guide/aws-services/waf/#using-the-console","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose AWS WAF . Choose Open Search , choose Next . Under Specify settings , choose Automatic or Manual . For Automatic mode, choose a Web ACL in the dropdown list. For Manual mode, enter the Web ACL name . (Optional) If you are ingesting WAF logs from another account, select a linked account from the Account dropdown list first. Specify an Ingest Options . Choose between Sampled Request or Full Request . For Sampled Request , enter how often you want to ingest sampled requests in minutes. For Full Request , if the Web ACL log is not enabled, choose Enable to enable the access log, or enter Log location in Manual mode. Note that Centralized Logging with OpenSearch will automatically enable logging with a Kinesis Data Firehose stream as destination for your WAF. Choose Next . In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard if you want to ingest an associated templated Amazon OpenSearch Service dashboard. You can change the Index Prefix of the target Amazon OpenSearch Service index if needed. The default prefix is the Web ACL Name . In the Log Lifecycle section, enter the number of days to manage the Amazon OpenSearch Service index lifecycle. The Centralized Logging with OpenSearch will create the associated Index State Management (ISM) policy automatically for this pipeline. Choose Next . Add tags if needed. Choose Create .","title":"Using the Console"},{"location":"implementation-guide/aws-services/waf/#using-the-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - WAF Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Regions (Full Request) Template AWS China Regions (Full Request) Template AWS Regions (Sampled Request) Template AWS China Regions (Sampled Request) Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameters for Full Request only Parameter Default Description Log Bucket Name <Requires input> The S3 bucket name which stores the logs. Log Bucket Prefix <Requires input> The S3 bucket path prefix which stores the logs. Parameters for Sampled Request only Parameter Default Description WebACL Names <Requires input> The list of Web ACL names, delimited by comma. Interval 1 The default interval (in minutes) to get sampled logs. Common parameters Parameter Default Description Log Source Account ID <Optional> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <Optional> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <Optional> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <Requires input> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <Requires input> The OpenSearch endpoint URL. For example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com Index Prefix <Requires input> The common prefix of OpenSearch index for the log. The index name will be <Index Prefix>-<log-type>-<YYYY-MM-DD> . Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <Requires input> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <Requires input> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <Requires input> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <Requires input> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <Optional input> The KMS-CMK ARN for encryption. Leave it blank to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the CloudFormation Stack"},{"location":"implementation-guide/aws-services/waf/#view-dashboard","text":"The dashboard includes the following visualizations. Visualization Name Source Field Description Filters Filters The following data can be filtered by query filter conditions. Web ACLs log event webaclName Displays the count of requests made to the WAF, grouped by Web ACL Names. Total Requests log event Displays the total number of web requests. Request Timeline log event Presents a bar chart that displays the distribution of events over time. WAF Rules terminatingRuleId Presents a pie chart that displays the distribution of events over the WAF rules in the Web ACL. Total Blocked Requests log event Displays the total number of blocked web requests. Unique Client IPs Request.ClientIP Displays unique visitors identified by client IP. Country or Region By Request Request.Country Displays the count of requests made to the Web ACL (grouped by the corresponding country or region resolved by the client IP). Http Methods Request.HTTPMethod Displays the count of requests made to the Web ACL using a pie chart, grouped by http request method names (e.g., POST, GET, HEAD, etc.). Http Versions Request.HTTPVersion Displays the count of requests made to the Web ACL using a pie chart, grouped by http protocol version (e.g., HTTP/2.0, HTTP/1.1, etc.). Top WebACLs webaclName webaclId.keyword The web requests view enables you to analyze the top web requests. Top Hosts host Lists the source IP addresses associated with events, enabling you to identify and investigate potentially suspicious or unauthorized activities. Top Request URIs Request.URI Top 10 request URIs. Top Countries or Regions Request.country Top 10 countries with the Web ACL Access. Top Rules terminatingRuleId Top 10 rules in the web ACL that matched the request. Top Client IPs Request.ClientIP Provides the top 10 IP address. Top User Agents userAgent Provides the top 10 user agents Block Allow Host Uri host Request.URI action Provides blocked or allowed web requests. Top Labels with Host, Uri labels.name host Request.URI Top 10 detailed logs by labels with host, URI View by Matching Rule sc-status This visualization provides detailed logs by DQL \"terminatingRuleId:*\". View by httpRequest args,uri,path sc-status This visualization provides detailed logs by DQL.","title":"View dashboard"},{"location":"implementation-guide/aws-services/waf/#sample-dashboard","text":"You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see Access Dashboard . You can click the below image to view the high-resolution sample dashboard.","title":"Sample Dashboard"},{"location":"implementation-guide/aws-services/waf/#create-log-ingestion-light-engine-for-log-analytics","text":"","title":"Create log ingestion (Light Engine for log analytics)"},{"location":"implementation-guide/aws-services/waf/#using-the-console_1","text":"Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, under Log Analytics Pipelines , choose Service Log . Choose the Create a log ingestion button. In the AWS Services section, choose AWS WAF . Choose Light Engine , choose Next . Under the Specified Settings , select Automatic or Manual . For the Automatic mode, choose a Web ACL from the dropdown list. For the Manual mode, enter the Web ACL name. (Optional) If you need to ingest logs across AWS accounts, select a linked AWS account from the account dropdown list. In the Ingestion Options section, select Full Request . For Full Request , if Web ACL logging is not enabled, click Enable Access Logging to enable access logs. Alternatively, enter the log location in manual mode. Note that using the log delivery stream will automatically enable using Kinesis Data Firehose as the target for WAF logs. Select Next . In the Specify Light Engine Configuration section, if you want to ingest associated templated Grafana dashboards, select Yes for the sample dashboard. You can choose an existing Grafana, or if you need to import a new one, you can go to Grafana for configuration. Select an S3 bucket to store partitioned logs and define a name for the log table. We have provided a predefined table name, but you can modify it according to your business needs. The log processing frequency is set to 5 minutes by default, with a minimum processing frequency of 1 minute. In the Log Lifecycle section, enter the log merge time and log archive time. We have provided default values, but you can adjust them based on your business requirements. Select Next . If desired, add tags. Select Create .","title":"Using the Console"},{"location":"implementation-guide/aws-services/waf/#using-the-cloudformation-stack_1","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - WAF Log Ingestion solution in the AWS Cloud. Launch in AWS Console Download Template AWS Region(Full Request) Template AWS China Regions (Full Request) Template Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameters for Pipeline settings Parameter Defaul Description Pipeline Id < <\u9700\u8981\u8f93\u5165> input> The unique identifier for the pipeline is essential if you need to create multiple WAF pipelines and write different WAF logs into separate tables. To ensure uniqueness, you can generate a unique pipeline identifier using uuidgenerator . Staging Bucket Prefix AWSLogs/WAFLogs The storage directory for logs in the temporary storage area should ensure the uniqueness and non-overlapping of the Prefix for different pipelines. Parameters for Destination settings Parameters Default Description Centralized Bucket Name <Requires input> Input centralized s3 bucket name\uff0cfor expample:centralized-logging-bucket\u3002 Centralized Bucket Prefix datalake Input centralized bucket prefix\uff0cdefault is datalake which means your data base's location is s3://{Centralized Bucket Name}/{Centralized Bucket Prefix}/amazon_cl_centralized\u3002 Centralized Table Name WAF Table name for writing data to the centralized database, can be defined as needed, default value is 'waf'. Parameters for Scheduler settings Parameters Default Description LogProcessor Schedule Expression rate(5 minutes) Task scheduling expression for performing log processing, with a default value of executing the LogProcessor every 5 minutes. Configuration for reference \u3002 LogMerger Schedule Expression cron(0 1 * ? ) \u6267ask scheduling expression for performing log merging, with a default value of executing the LogMerger at 1 AM every day. Configuration for reference \u3002 LogArchive Schedule Expression cron(0 2 * ? ) Task scheduling expression for performing log archiving, with a default value of executing the LogArchive at 2 AM every day. Configuration for reference \u3002 Age to Merge 7 Small file retention days, with a default value of 7, indicates that logs older than 7 days will be merged into small files. It can be adjusted as needed. Age to Archive 30 Log retention days, with a default value of 30, indicates that data older than 30 days will be archived and deleted. It can be adjusted as needed. Parameters for Notification settings Parameters Default Description Notification Service SNS Notification method for alerts. If your main stack is using China, you can only choose the SNS method. If your main stack is using Global, you can choose either the SNS or SES method. Recipients <Requires Input> Alert notification: If the Notification Service is SNS, enter the SNS Topic ARN here, ensuring that you have the necessary permissions. If the Notification Service is SES, enter the email addresses separated by commas here, ensuring that the email addresses are already Verified Identities in SES. The adminEmail provided during the creation of the main stack will receive a verification email by default. Parameters for Dashboard settings Parameters Default Description Import Dashboards FALSE Whether to import the Dashboard into Grafana, with a default value of false. If set to true, you must provide the Grafana URL and Grafana Service Account Token.\u3002 Grafana URL <Requires Input> Grafana access URL\uff0cfor example: https://alb-72277319.us-west-2.elb.amazonaws.com\u3002 Grafana Service Account Token <Requires Input> Grafana Service Account Token\uff1aService Account Token created in Grafana\u3002 Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.","title":"Using the CloudFormation Stack"},{"location":"implementation-guide/aws-services/waf/#view-dashboard_1","text":"Visualization Name Source Field Description Filters Filters The following data can be filtered by query filter conditions. Total Requests log event Displays the total number of web requests. Total Blocked Requests log event Displays the total number of blocked web requests. Requests History log event Presents a bar chart that displays the distribution of events over time. WAF ACLs log event webaclName Displays the count of requests made to the WAF, grouped by Web ACL Names. WAF Rules terminatingRuleId Presents a pie chart that displays the distribution of events over the WAF rules in the Web ACL. Sources httpSourceId Presents a pie chart that displays the distribution of events over the id of the associated resource. HTTP Methods httpRequest.HTTPMethod Displays the count of requests made to the Web ACL using a pie chart, grouped by HTTP request method names (e.g., POST, GET, HEAD, etc.). Country or Region By Blocked Requests HTTPRequest.Country Displays the count of blocked web requests made to the Web ACL (grouped by the corresponding country or region resolved by the client IP). Top WebACLs webaclName The web requests view enables you to analyze the top web requests. Top Sources httpSourceId Top 10 id of the associated resource. Top Requests URIs httpRequest.URI Top 10 request URIs. Top Countries or Regions httpRequest.country Top 10 countries with the Web ACL Access. Top Rules terminatingRuleId Top 10 rules in the web ACL that matched the request. Top Client IPs httpRequest.ClientIP Provides the top 10 IP addresses. Top Blocked / Allowed Hosts URI host httpRequest.URI action Provides blocked or allowed web requests. Top Labels with Host, URI labels host httpRequest.URI Top 10 detailed logs by labels with host, URI. Metrics webaclId webaclName terminatingRuleId terminatingRuleType httpSourceId httpRequest.HTTPMethod httpRequest.country httpRequest.ClientIP labels httpRequest.URI action Provides a detailed list of log events, including timestamps, WebACL, client IP, etc.","title":"View Dashboard"},{"location":"implementation-guide/deployment/","text":"Overview Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account. Prerequisites Review all the considerations and make sure you have the following in the target region you want to deploy the solution: At least one vacancy to create new VPCs, if you choose to launch with new VPC. At least two vacant Elastic IP (EIP) addresses, if you choose to launch with new VPC. At least four vacant S3 buckets. Deployment in AWS Regions Centralized Logging with OpenSearch provides two ways to authenticate and log into the Centralized Logging with OpenSearch console. For some AWS regions where Cognito User Pool is not available (for example, Hong Kong), you need to launch the solution with OpenID Connect provider. Launch with Cognito User Pool Launch with OpenID Connect For more information about supported regions, see Regional deployments . Deployment in AWS China Regions AWS China Regions do not have Cognito User Pool. You must launch the solution with OpenID Connect. Launch with OpenID Connect","title":"Overview"},{"location":"implementation-guide/deployment/#overview","text":"Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.","title":"Overview"},{"location":"implementation-guide/deployment/#prerequisites","text":"Review all the considerations and make sure you have the following in the target region you want to deploy the solution: At least one vacancy to create new VPCs, if you choose to launch with new VPC. At least two vacant Elastic IP (EIP) addresses, if you choose to launch with new VPC. At least four vacant S3 buckets.","title":"Prerequisites"},{"location":"implementation-guide/deployment/#deployment-in-aws-regions","text":"Centralized Logging with OpenSearch provides two ways to authenticate and log into the Centralized Logging with OpenSearch console. For some AWS regions where Cognito User Pool is not available (for example, Hong Kong), you need to launch the solution with OpenID Connect provider. Launch with Cognito User Pool Launch with OpenID Connect For more information about supported regions, see Regional deployments .","title":"Deployment in AWS Regions"},{"location":"implementation-guide/deployment/#deployment-in-aws-china-regions","text":"AWS China Regions do not have Cognito User Pool. You must launch the solution with OpenID Connect. Launch with OpenID Connect","title":"Deployment in AWS China Regions"},{"location":"implementation-guide/deployment/with-cognito/","text":"Launch with Cognito User Pool Time to deploy : Approximately 15 minutes Deployment Overview Use the following steps to deploy this solution on AWS. Step 1. Launch the stack Step 2. Launch the web console Step 1. Launch the stack This AWS CloudFormation template automatically deploys the Centralized Logging with OpenSearch solution on AWS. Sign in to the AWS Management Console and select the button to launch the AWS CloudFormation template. Launch in AWS Console Launch with a new VPC Launch with an existing VPC The template is launched in the default region after you log in to the console. To launch the Centralized Logging with OpenSearch solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for the template and modify them as necessary. If you are launching the solution in a new VPC, this solution uses the following parameters: Parameter Default Description Admin User Email <Requires input> Specify the email of the Administrator. This email address will receive a temporary password to access the Centralized Logging with OpenSearch web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. If you are launching the solution in an existing VPC, this solution uses the following parameters: Parameter Default Description Admin User Email <Requires input> Specify the email of the Administrator. This email address will receive a temporary password to access the Centralized Logging with OpenSearch web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. VPC ID <Requires input> Specify the existing VPC ID in which you are launching the Centralized Logging with OpenSearch solution. Public Subnet IDs <Requires input> Specify the two public subnets in the selected VPC. The subnets must have routes point to an Internet Gateway . Private Subnet IDs <Requires input> Specify the two private subnets in the selected VPC. The subnets must have routes point to an NAT Gateway . Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Select the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. Step 2. Launch the web Console After the stack is successfully created, this solution generates a CloudFront domain name that gives you access to the Centralized Logging with OpenSearch web console. Meanwhile, an auto-generated temporary password (excluding the last digit . ) will be sent to your email address. Sign in to the AWS CloudFormation console . On the Stacks page, select the solution\u2019s stack. Choose the Outputs tab and record the domain name. Open the WebConsoleUrl using a web browser, and navigate to a sign-in page. Enter the Email and the temporary password. a. Set a new account password. b. (Optional) Verify your email address for account recovery. After the verification is complete, the system opens the Centralized Logging with OpenSearch web console. Once you have logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain and build log analytics pipelines.","title":"Launch with Cognito User Pool"},{"location":"implementation-guide/deployment/with-cognito/#launch-with-cognito-user-pool","text":"Time to deploy : Approximately 15 minutes","title":"Launch with Cognito User Pool"},{"location":"implementation-guide/deployment/with-cognito/#deployment-overview","text":"Use the following steps to deploy this solution on AWS. Step 1. Launch the stack Step 2. Launch the web console","title":"Deployment Overview"},{"location":"implementation-guide/deployment/with-cognito/#step-1-launch-the-stack","text":"This AWS CloudFormation template automatically deploys the Centralized Logging with OpenSearch solution on AWS. Sign in to the AWS Management Console and select the button to launch the AWS CloudFormation template. Launch in AWS Console Launch with a new VPC Launch with an existing VPC The template is launched in the default region after you log in to the console. To launch the Centralized Logging with OpenSearch solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for the template and modify them as necessary. If you are launching the solution in a new VPC, this solution uses the following parameters: Parameter Default Description Admin User Email <Requires input> Specify the email of the Administrator. This email address will receive a temporary password to access the Centralized Logging with OpenSearch web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. If you are launching the solution in an existing VPC, this solution uses the following parameters: Parameter Default Description Admin User Email <Requires input> Specify the email of the Administrator. This email address will receive a temporary password to access the Centralized Logging with OpenSearch web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. VPC ID <Requires input> Specify the existing VPC ID in which you are launching the Centralized Logging with OpenSearch solution. Public Subnet IDs <Requires input> Specify the two public subnets in the selected VPC. The subnets must have routes point to an Internet Gateway . Private Subnet IDs <Requires input> Specify the two private subnets in the selected VPC. The subnets must have routes point to an NAT Gateway . Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Select the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Step 1. Launch the stack"},{"location":"implementation-guide/deployment/with-cognito/#step-2-launch-the-web-console","text":"After the stack is successfully created, this solution generates a CloudFront domain name that gives you access to the Centralized Logging with OpenSearch web console. Meanwhile, an auto-generated temporary password (excluding the last digit . ) will be sent to your email address. Sign in to the AWS CloudFormation console . On the Stacks page, select the solution\u2019s stack. Choose the Outputs tab and record the domain name. Open the WebConsoleUrl using a web browser, and navigate to a sign-in page. Enter the Email and the temporary password. a. Set a new account password. b. (Optional) Verify your email address for account recovery. After the verification is complete, the system opens the Centralized Logging with OpenSearch web console. Once you have logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain and build log analytics pipelines.","title":"Step 2. Launch the web Console"},{"location":"implementation-guide/deployment/with-oidc/","text":"Launch with OpenID Connect (OIDC) Time to deploy : Approximately 30 minutes Prerequisites Important The Centralized Logging with OpenSearch console is served via CloudFront distribution which is considered as an Internet information service. If you are deploying the solution in AWS China Regions , the domain must have a valid ICP Recordal . A domain. You will use this domain to access the Centralized Logging with OpenSearch console (Required for AWS China Regions, optional for AWS Regions). An SSL certificate in AWS IAM. The SSL must be associated with the given domain. Follow this guide to upload SSL certificate to IAM. Note that this is required for AWS China Regions, but is not recommended for AWS Regions. Make sure to request or import the ACM certificate in the US East (N. Virginia) Region (us-east-1). Note that this is not required for AWS China Regions, and is optional for AWS Regions. Deployment Overview Use the following steps to deploy this solution on AWS. Step 1. Create OIDC client Step 2. Launch the stack Step 3. Setup DNS Resolver Step 4. Launch the web console Step 1. Create OIDC client You can use different kinds of OpenID Connector (OIDC) providers. This section introduces Option 1 to Option 4. (Option 1) Using Amazon Cognito from another region as OIDC provider. (Option 2) Authing , which is an example of a third-party authentication provider. (Option 3) Keycloak , which is a solution maintained by AWS and can serve as an authentication identity provider. (Option 4) ADFS , which is a service offered by Microsoft. (Option 5) Other third-party authentication platforms such as Auth0 . Follow the steps below to create an OIDC client, and obtain the client_id and issuer . (Option 1) Using Cognito User Pool from another region You can leverage the Cognito User Pool in a supported AWS Standard Region as the OIDC provider. Go to the Amazon Cognito console in an AWS Standard Region. Set up the hosted UI with the Amazon Cognito console based on this guide . Choose Public client when selecting the App type . Enter the Callback URL and Sign out URL using your domain name for Centralized Logging with OpenSearch console. If your hosted UI is set up, you should be able to see something like below. Save the App client ID, User pool ID and the AWS Region to a file, which will be used later. In Step 2. Launch the stack , the OidcClientID is the App client ID , and OidcProvider is https://cognito-idp.${REGION}.amazonaws.com/${USER_POOL_ID} . (Option 2) Authing.cn OIDC client Go to the Authing console . Create a user pool if you don't have one. Select the user pool. On the left navigation bar, select Self-built App under Applications . Click the Create button. Enter the Application Name , and Subdomain . Save the App ID (that is, client_id ) and Issuer to a text file from Endpoint Information, which will be used later. Update the Login Callback URL and Logout Callback URL to your IPC recorded domain name. Set the Authorization Configuration. You have successfully created an authing self-built application. (Option 3) Keycloak OIDC client Deploy the Keycloak solution in AWS China Regions following this guide . Sign in to the Keycloak console. On the left navigation bar, select Add realm . Skip this step if you already have a realm. Go to the realm setting page. Choose Endpoints , and then OpenID Endpoint Configuration from the list. In the JSON file that opens up in your browser, record the issuer value which will be used later. Go back to Keycloak console and select Clients on the left navigation bar, and choose Create . Enter a Client ID, which must contain 24 letters (case-insensitive) or numbers. Record the Client ID which will be used later. Change client settings. Enter https://<Centralized Logging with OpenSearch Console domain> in Valid Redirect URIs \uff0cand enter * and + in Web Origins . In the Advanced Settings, set the Access Token Lifespan to at least 5 minutes. Select Users on the left navigation bar. Click Add user and enter Username . After the user is created, select Credentials , and enter Password . The issuer value is https://<KEYCLOAK_DOMAIN_NAME>/auth/realms/<REALM_NAME> . (Option 4) ADFS OpenID Connect Client Make sure your ADFS is installed. For information about how to install ADFS, refer to this guide . Make sure you can log in to the ADFS Sign On page. The URL should be https://adfs.domain.com/adfs/ls/idpinitiatedSignOn.aspx , and you need to replace adfs.domain.com with your real ADFS domain. Log on your Domain Controller , and open Active Directory Users and Computers . Create a Security Group for Centralized Logging with OpenSearch Users, and add your planned Centralized Logging with OpenSearch users to this Security Group. Log on to ADFS server, and open ADFS Management . Right click Application Groups , choose Application Group , and enter the name for the Application Group. Select Web browser accessing a web application option under Client-Server Applications , and choose Next . Record the Client Identifier ( client_id ) under Redirect URI , enter your Centralized Logging with OpenSearch domain (for example, xx.domain.com ), and choose Add , and then choose Next . In the Choose Access Control Policy window, select Permit specific group , choose parameters under Policy part, add the created Security Group in Step 4, then click Next . You can configure other access control policy based on your requirements. Under Summary window, choose Next , and choose Close . Open the Windows PowerShell on ADFS Server, and run the following commands to configure ADFS to allow CORS for your planned URL. Set-AdfsResponseHeaders -EnableCORS $true Set-AdfsResponseHeaders -CORSTrustedOrigins https://<your-centralized-logging-with-opensearch-domain> Under Windows PowerShell on ADFS server, run the following command to get the Issuer ( issuer ) of ADFS, which is similar to https://adfs.domain.com/adfs . Get-ADFSProperties | Select IdTokenIssuer Step 2. Launch the stack Important You can only have one active Centralized Logging with OpenSearch solution stack in one region of an AWS account. If your deployment failed (for example, not meeting the requirements in prerequisites ), make sure you have deleted the failed stack before retrying the deployment. Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template. Launch in AWS Console Launch with a new VPC in AWS Regions Launch with an existing VPC in AWS Regions Launch with a new VPC in AWS China Regions Launch with an existing VPC in AWS China Regions The template is launched in the default region after you log in to the console. To launch the Centralized Logging with OpenSearch solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for the template and modify them as necessary. If you are launching the solution in a new VPC, this solution uses the following parameters: Parameter Default Description OidcClientId <Requires input> OpenID Connector client Id. OidcProvider <Requires input> OpenID Connector provider issuer. The issuer must begin with https:// Domain <Optional> Custom domain for Centralized Logging with OpenSearch console. Do NOT add http(s) prefix. IamCertificateID <Optional> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the list-server-certificates command to retrieve the ID. AcmCertificateArn <Optional> Arn for ACM certificates requested (or imported) the certificate in the US East (N. Virginia) Region (us-east-1). If you are launching the solution in an existing VPC, this solution uses the following parameters: Parameter Default Description OidcClientId <Requires input> OpenID Connector client Id. OidcProvider <Requires input> OpenID Connector provider issuer. The issuer must begin with https:// Domain <Optional> Custom domain for Centralized Logging with OpenSearch console. Do NOT add http(s) prefix. IamCertificateID <Optional> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the list-server-certificates command to retrieve the ID. AcmCertificateArn <Optional> Arn for ACM certificates requested (or imported) the certificate in the US East (N. Virginia) Region (us-east-1). VPC ID <Requires input> Specify the existing VPC ID in which you are launching the solution. Public Subnet IDs <Requires input> Specify the two public subnets in the selected VPC. The subnets must have routes pointing to an Internet Gateway . Private Subnet IDs <Requires input> Specify the two private subnets in the selected VPC. The subnets must have routes pointing to an NAT Gateway . Important If you are deploying the solution in AWS China Regions , you must enter Domain, and IamCertificateID. If you are deploying the solution in AWS Regions , when a custom domain name is required, you must enter Domain, and AcmCertificateArn. when no custom domain name is required, leave it blank for Domain, IamCertificateID, and AcmCertificateArn. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. Step 3. Setup DNS Resolver This solution provisions a CloudFront distribution that gives you access to the Centralized Logging with OpenSearch console. Sign in to the AWS CloudFormation console . Select the solution's stack. Choose the Outputs tab. Obtain the WebConsoleUrl as the endpoint. Create a CNAME record in DNS resolver, which points to the endpoint address. Step 4. Launch the web console Important You login credentials is managed by the OIDC provider. Before signing in to the Centralized Logging with OpenSearch console, make sure you have created at least one user in the OIDC provider's user pool. Use the previous assigned CNAME to open the OIDC Customer Domain URL using a web browser. Choose Sign in to Centralized Logging with OpenSearch , and navigate to OIDC provider. Enter sign-in credentials. You may be asked to change your default password for first-time login, which depends on your OIDC provider's policy. After the verification is complete, the system opens the Centralized Logging with OpenSearch web console. Once you have logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain and build log analytics pipelines.","title":"Launch with OpenID Connect"},{"location":"implementation-guide/deployment/with-oidc/#launch-with-openid-connect-oidc","text":"Time to deploy : Approximately 30 minutes","title":"Launch with OpenID Connect (OIDC)"},{"location":"implementation-guide/deployment/with-oidc/#prerequisites","text":"Important The Centralized Logging with OpenSearch console is served via CloudFront distribution which is considered as an Internet information service. If you are deploying the solution in AWS China Regions , the domain must have a valid ICP Recordal . A domain. You will use this domain to access the Centralized Logging with OpenSearch console (Required for AWS China Regions, optional for AWS Regions). An SSL certificate in AWS IAM. The SSL must be associated with the given domain. Follow this guide to upload SSL certificate to IAM. Note that this is required for AWS China Regions, but is not recommended for AWS Regions. Make sure to request or import the ACM certificate in the US East (N. Virginia) Region (us-east-1). Note that this is not required for AWS China Regions, and is optional for AWS Regions.","title":"Prerequisites"},{"location":"implementation-guide/deployment/with-oidc/#deployment-overview","text":"Use the following steps to deploy this solution on AWS. Step 1. Create OIDC client Step 2. Launch the stack Step 3. Setup DNS Resolver Step 4. Launch the web console","title":"Deployment Overview"},{"location":"implementation-guide/deployment/with-oidc/#step-1-create-oidc-client","text":"You can use different kinds of OpenID Connector (OIDC) providers. This section introduces Option 1 to Option 4. (Option 1) Using Amazon Cognito from another region as OIDC provider. (Option 2) Authing , which is an example of a third-party authentication provider. (Option 3) Keycloak , which is a solution maintained by AWS and can serve as an authentication identity provider. (Option 4) ADFS , which is a service offered by Microsoft. (Option 5) Other third-party authentication platforms such as Auth0 . Follow the steps below to create an OIDC client, and obtain the client_id and issuer .","title":"Step 1. Create OIDC client"},{"location":"implementation-guide/deployment/with-oidc/#option-1-using-cognito-user-pool-from-another-region","text":"You can leverage the Cognito User Pool in a supported AWS Standard Region as the OIDC provider. Go to the Amazon Cognito console in an AWS Standard Region. Set up the hosted UI with the Amazon Cognito console based on this guide . Choose Public client when selecting the App type . Enter the Callback URL and Sign out URL using your domain name for Centralized Logging with OpenSearch console. If your hosted UI is set up, you should be able to see something like below. Save the App client ID, User pool ID and the AWS Region to a file, which will be used later. In Step 2. Launch the stack , the OidcClientID is the App client ID , and OidcProvider is https://cognito-idp.${REGION}.amazonaws.com/${USER_POOL_ID} .","title":"(Option 1) Using Cognito User Pool from another region"},{"location":"implementation-guide/deployment/with-oidc/#option-2-authingcn-oidc-client","text":"Go to the Authing console . Create a user pool if you don't have one. Select the user pool. On the left navigation bar, select Self-built App under Applications . Click the Create button. Enter the Application Name , and Subdomain . Save the App ID (that is, client_id ) and Issuer to a text file from Endpoint Information, which will be used later. Update the Login Callback URL and Logout Callback URL to your IPC recorded domain name. Set the Authorization Configuration. You have successfully created an authing self-built application.","title":"(Option 2) Authing.cn OIDC client"},{"location":"implementation-guide/deployment/with-oidc/#option-3-keycloak-oidc-client","text":"Deploy the Keycloak solution in AWS China Regions following this guide . Sign in to the Keycloak console. On the left navigation bar, select Add realm . Skip this step if you already have a realm. Go to the realm setting page. Choose Endpoints , and then OpenID Endpoint Configuration from the list. In the JSON file that opens up in your browser, record the issuer value which will be used later. Go back to Keycloak console and select Clients on the left navigation bar, and choose Create . Enter a Client ID, which must contain 24 letters (case-insensitive) or numbers. Record the Client ID which will be used later. Change client settings. Enter https://<Centralized Logging with OpenSearch Console domain> in Valid Redirect URIs \uff0cand enter * and + in Web Origins . In the Advanced Settings, set the Access Token Lifespan to at least 5 minutes. Select Users on the left navigation bar. Click Add user and enter Username . After the user is created, select Credentials , and enter Password . The issuer value is https://<KEYCLOAK_DOMAIN_NAME>/auth/realms/<REALM_NAME> .","title":"(Option 3) Keycloak OIDC client"},{"location":"implementation-guide/deployment/with-oidc/#option-4-adfs-openid-connect-client","text":"Make sure your ADFS is installed. For information about how to install ADFS, refer to this guide . Make sure you can log in to the ADFS Sign On page. The URL should be https://adfs.domain.com/adfs/ls/idpinitiatedSignOn.aspx , and you need to replace adfs.domain.com with your real ADFS domain. Log on your Domain Controller , and open Active Directory Users and Computers . Create a Security Group for Centralized Logging with OpenSearch Users, and add your planned Centralized Logging with OpenSearch users to this Security Group. Log on to ADFS server, and open ADFS Management . Right click Application Groups , choose Application Group , and enter the name for the Application Group. Select Web browser accessing a web application option under Client-Server Applications , and choose Next . Record the Client Identifier ( client_id ) under Redirect URI , enter your Centralized Logging with OpenSearch domain (for example, xx.domain.com ), and choose Add , and then choose Next . In the Choose Access Control Policy window, select Permit specific group , choose parameters under Policy part, add the created Security Group in Step 4, then click Next . You can configure other access control policy based on your requirements. Under Summary window, choose Next , and choose Close . Open the Windows PowerShell on ADFS Server, and run the following commands to configure ADFS to allow CORS for your planned URL. Set-AdfsResponseHeaders -EnableCORS $true Set-AdfsResponseHeaders -CORSTrustedOrigins https://<your-centralized-logging-with-opensearch-domain> Under Windows PowerShell on ADFS server, run the following command to get the Issuer ( issuer ) of ADFS, which is similar to https://adfs.domain.com/adfs . Get-ADFSProperties | Select IdTokenIssuer","title":"(Option 4) ADFS OpenID Connect Client"},{"location":"implementation-guide/deployment/with-oidc/#step-2-launch-the-stack","text":"Important You can only have one active Centralized Logging with OpenSearch solution stack in one region of an AWS account. If your deployment failed (for example, not meeting the requirements in prerequisites ), make sure you have deleted the failed stack before retrying the deployment. Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template. Launch in AWS Console Launch with a new VPC in AWS Regions Launch with an existing VPC in AWS Regions Launch with a new VPC in AWS China Regions Launch with an existing VPC in AWS China Regions The template is launched in the default region after you log in to the console. To launch the Centralized Logging with OpenSearch solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for the template and modify them as necessary. If you are launching the solution in a new VPC, this solution uses the following parameters: Parameter Default Description OidcClientId <Requires input> OpenID Connector client Id. OidcProvider <Requires input> OpenID Connector provider issuer. The issuer must begin with https:// Domain <Optional> Custom domain for Centralized Logging with OpenSearch console. Do NOT add http(s) prefix. IamCertificateID <Optional> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the list-server-certificates command to retrieve the ID. AcmCertificateArn <Optional> Arn for ACM certificates requested (or imported) the certificate in the US East (N. Virginia) Region (us-east-1). If you are launching the solution in an existing VPC, this solution uses the following parameters: Parameter Default Description OidcClientId <Requires input> OpenID Connector client Id. OidcProvider <Requires input> OpenID Connector provider issuer. The issuer must begin with https:// Domain <Optional> Custom domain for Centralized Logging with OpenSearch console. Do NOT add http(s) prefix. IamCertificateID <Optional> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the list-server-certificates command to retrieve the ID. AcmCertificateArn <Optional> Arn for ACM certificates requested (or imported) the certificate in the US East (N. Virginia) Region (us-east-1). VPC ID <Requires input> Specify the existing VPC ID in which you are launching the solution. Public Subnet IDs <Requires input> Specify the two public subnets in the selected VPC. The subnets must have routes pointing to an Internet Gateway . Private Subnet IDs <Requires input> Specify the two private subnets in the selected VPC. The subnets must have routes pointing to an NAT Gateway . Important If you are deploying the solution in AWS China Regions , you must enter Domain, and IamCertificateID. If you are deploying the solution in AWS Regions , when a custom domain name is required, you must enter Domain, and AcmCertificateArn. when no custom domain name is required, leave it blank for Domain, IamCertificateID, and AcmCertificateArn. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Step 2. Launch the stack"},{"location":"implementation-guide/deployment/with-oidc/#step-3-setup-dns-resolver","text":"This solution provisions a CloudFront distribution that gives you access to the Centralized Logging with OpenSearch console. Sign in to the AWS CloudFormation console . Select the solution's stack. Choose the Outputs tab. Obtain the WebConsoleUrl as the endpoint. Create a CNAME record in DNS resolver, which points to the endpoint address.","title":"Step 3. Setup DNS Resolver"},{"location":"implementation-guide/deployment/with-oidc/#step-4-launch-the-web-console","text":"Important You login credentials is managed by the OIDC provider. Before signing in to the Centralized Logging with OpenSearch console, make sure you have created at least one user in the OIDC provider's user pool. Use the previous assigned CNAME to open the OIDC Customer Domain URL using a web browser. Choose Sign in to Centralized Logging with OpenSearch , and navigate to OIDC provider. Enter sign-in credentials. You may be asked to change your default password for first-time login, which depends on your OIDC provider's policy. After the verification is complete, the system opens the Centralized Logging with OpenSearch web console. Once you have logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain and build log analytics pipelines.","title":"Step 4. Launch the web console"},{"location":"implementation-guide/domains/","text":"This chapter describes how to manage Amazon OpenSearch Service domains through the Centralized Logging with OpenSearch console. An Amazon OpenSearch Service domain is synonymous with an Amazon OpenSearch Service cluster. In this chapter, you will learn: Import & remove an Amazon OpenSearch Service Domain Create an access proxy Create recommended alarms You can read the Getting Started chapter first and walk through the basic steps for using the Centralized Logging with OpenSearch solution.","title":"Overview"},{"location":"implementation-guide/domains/alarms/","text":"Recommended Alarms Amazon OpenSearch provides a set of recommended CloudWatch alarms to monitor the health of Amazon OpenSearch Service domains. Centralized Logging with OpenSearch helps you to create the alarms automatically, and send notification to your email (or SMS) via SNS. Create alarms Using the Centralized Logging with OpenSearch console Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Under General configuration , choose Enable at the Alarms label. Enter the Email . Choose the alarms you want to create and adjust the settings if necessary. Choose Create . Using the CloudFormation stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Alarms solution in the AWS Cloud. Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Endpoint <Requires input> The endpoint of the OpenSearch domain, for example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com . DomainName <Requires input> The name of the OpenSearch domain. Email <Requires input> The notification email address. Alarms will be sent to this email address via SNS. ClusterStatusRed Yes Whether to enable alarm when at least one primary shard and its replicas are not allocated to a node. ClusterStatusYellow Yes Whether to enable alarm when at least one replica shard is not allocated to a node. FreeStorageSpace 10 Whether to enable alarm when a node in your cluster is down to the free storage space you entered in GiB. We recommend setting it to 25% of the storage space for each node. 0 means the alarm is disabled. ClusterIndexWritesBlocked 1 Index writes blocked error occurs for >= x times in 5 minutes, 1 consecutive time. Input 0 to disable this alarm. UnreachableNodeNumber 3 Nodes minimum is < x for 1 day, 1 consecutive time. 0 means the alarm is disabled. AutomatedSnapshotFailure Yes Whether to enable alarm when automated snapshot failed. AutomatedSnapshotFailure maximum is >= 1 for 1 minute, 1 consecutive time. CPUUtilization Yes Whether to enable alarm when sustained high usage of CPU occurred. CPUUtilization or WarmCPUUtilization maximum is >= 80% for 15 minutes, 3 consecutive times. JVMMemoryPressure Yes Whether to enable alarm when JVM RAM usage peak occurred. JVMMemoryPressure or WarmJVMMemoryPressure maximum is >= 80% for 5 minutes, 3 consecutive times. MasterCPUUtilization Yes Whether to enable alarm when sustained high usage of CPU occurred in master nodes. MasterCPUUtilization maximum is >= 50% for 15 minutes, 3 consecutive times. MasterJVMMemoryPressure Yes Whether to enable alarm when JVM RAM usage peak occurred in master nodes. MasterJVMMemoryPressure maximum is >= 80% for 15 minutes, 1 consecutive time. KMSKeyError Yes Whether to enable alarm when KMS encryption key is disabled. KMSKeyError is >= 1 for 1 minute, 1 consecutive time. KMSKeyInaccessible Yes Whether to enable alarm when KMS encryption key has been deleted or has revoked its grants to OpenSearch Service. KMSKeyInaccessible is >= 1 for 1 minute, 1 consecutive time. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 5 minutes. Once you have created the alarms, a confirmation email will be sent to your email address. You need to click the Confirm link in the email. Go to the CloudWatch Alarms page by choosing the General configuration > Alarms > CloudWatch Alarms link on the Centralized Logging with OpenSearch console, and the link location is shown as follows: Make sure that all the alarms are in OK status because you might have missed the notification if alarms have changed its status before subscription. Note The alarm will not send SNS notification to your email address if triggered before subscription. We recommend you check the alarms status after enabling the OpenSearch alarms. If you see any alarm which is in In Alarm status, you should fix that issue first. Delete alarms Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Choose the Alarms tab. Choose the Delete . On the confirmation prompt, choose Delete .","title":"Domain alarms"},{"location":"implementation-guide/domains/alarms/#recommended-alarms","text":"Amazon OpenSearch provides a set of recommended CloudWatch alarms to monitor the health of Amazon OpenSearch Service domains. Centralized Logging with OpenSearch helps you to create the alarms automatically, and send notification to your email (or SMS) via SNS.","title":"Recommended Alarms"},{"location":"implementation-guide/domains/alarms/#create-alarms","text":"","title":"Create alarms"},{"location":"implementation-guide/domains/alarms/#using-the-centralized-logging-with-opensearch-console","text":"Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Under General configuration , choose Enable at the Alarms label. Enter the Email . Choose the alarms you want to create and adjust the settings if necessary. Choose Create .","title":"Using the Centralized Logging with OpenSearch console"},{"location":"implementation-guide/domains/alarms/#using-the-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Alarms solution in the AWS Cloud. Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description Endpoint <Requires input> The endpoint of the OpenSearch domain, for example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com . DomainName <Requires input> The name of the OpenSearch domain. Email <Requires input> The notification email address. Alarms will be sent to this email address via SNS. ClusterStatusRed Yes Whether to enable alarm when at least one primary shard and its replicas are not allocated to a node. ClusterStatusYellow Yes Whether to enable alarm when at least one replica shard is not allocated to a node. FreeStorageSpace 10 Whether to enable alarm when a node in your cluster is down to the free storage space you entered in GiB. We recommend setting it to 25% of the storage space for each node. 0 means the alarm is disabled. ClusterIndexWritesBlocked 1 Index writes blocked error occurs for >= x times in 5 minutes, 1 consecutive time. Input 0 to disable this alarm. UnreachableNodeNumber 3 Nodes minimum is < x for 1 day, 1 consecutive time. 0 means the alarm is disabled. AutomatedSnapshotFailure Yes Whether to enable alarm when automated snapshot failed. AutomatedSnapshotFailure maximum is >= 1 for 1 minute, 1 consecutive time. CPUUtilization Yes Whether to enable alarm when sustained high usage of CPU occurred. CPUUtilization or WarmCPUUtilization maximum is >= 80% for 15 minutes, 3 consecutive times. JVMMemoryPressure Yes Whether to enable alarm when JVM RAM usage peak occurred. JVMMemoryPressure or WarmJVMMemoryPressure maximum is >= 80% for 5 minutes, 3 consecutive times. MasterCPUUtilization Yes Whether to enable alarm when sustained high usage of CPU occurred in master nodes. MasterCPUUtilization maximum is >= 50% for 15 minutes, 3 consecutive times. MasterJVMMemoryPressure Yes Whether to enable alarm when JVM RAM usage peak occurred in master nodes. MasterJVMMemoryPressure maximum is >= 80% for 15 minutes, 1 consecutive time. KMSKeyError Yes Whether to enable alarm when KMS encryption key is disabled. KMSKeyError is >= 1 for 1 minute, 1 consecutive time. KMSKeyInaccessible Yes Whether to enable alarm when KMS encryption key has been deleted or has revoked its grants to OpenSearch Service. KMSKeyInaccessible is >= 1 for 1 minute, 1 consecutive time. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 5 minutes. Once you have created the alarms, a confirmation email will be sent to your email address. You need to click the Confirm link in the email. Go to the CloudWatch Alarms page by choosing the General configuration > Alarms > CloudWatch Alarms link on the Centralized Logging with OpenSearch console, and the link location is shown as follows: Make sure that all the alarms are in OK status because you might have missed the notification if alarms have changed its status before subscription. Note The alarm will not send SNS notification to your email address if triggered before subscription. We recommend you check the alarms status after enabling the OpenSearch alarms. If you see any alarm which is in In Alarm status, you should fix that issue first.","title":"Using the CloudFormation stack"},{"location":"implementation-guide/domains/alarms/#delete-alarms","text":"Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Choose the Alarms tab. Choose the Delete . On the confirmation prompt, choose Delete .","title":"Delete alarms"},{"location":"implementation-guide/domains/import/","text":"Domain Operations Once logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain. Prerequisite Centralized Logging with OpenSearch supports Amazon OpenSearch Service, engine version Elasticsearch 7.10 or later, and engine version OpenSearch 1.0 or later. Centralized Logging with OpenSearch supports OpenSearch clusters within VPC. If you don't have an Amazon OpenSearch Service domain yet, you can create an Amazon OpenSearch Service domain within VPC. See Launching your Amazon OpenSearch Service domains within a VPC . Centralized Logging with OpenSearch supports OpenSearch clusters with fine-grained access control only. In the security configuration, the Access policy should look like the image below: Import an Amazon OpenSearch Service Domain Sign in to the Centralized Logging with OpenSearch console. In the left navigation panel, under Domains , choose Import OpenSearch Domain . On the Select domain page, choose a domain from the dropdown list. The dropdown list will display only domains in the same region as the solution. Choose Next . On the Configure network page, under Network creation , choose Manual and click Next ; or choose Automatic , and go to step 9. Under VPC , choose a VPC from the list. By default, the solution creates a standalone VPC, and you can choose the one named LogHubVpc/DefaultVPC . You can also choose the same VPC as your Amazon OpenSearch Service domains. Under Log Processing Subnet Group , select at least 2 subnets from the dropdown list. By default, the solution creates two private subnets. You can choose subnets named LogHubVpc/DefaultVPC/privateSubnet1 and LogHubVpc/DefaultVPC/privateSubnet2 . Under Log Processing Security Group , select one from the dropdown list. By default, the solution creates one Security Group named ProcessSecurityGroup . On the Create tags page, add tags if needed. Choose Import . Set up VPC Peering By default, the solution creates a standalone VPC. You need to create VPC Peering to allow the log processing layer to have access to your Amazon OpenSearch Service domains. Note Automatic mode will create VPC peering and configure route table automatically. You do not need to set up VPC peering again. Follow this section to create VPC peering, update security group and update route tables. Create VPC Peering Connection Sign in to the Centralized Logging with OpenSearch console. In the left navigation panel, under Domains , select OpenSearch Domains . Find the domain you imported and select the domain name. Choose the Network tab. Copy the VPC ID in both sections OpenSearch domain network and Log processing network . You will create Peering Connection between these two VPCs. Navigate to VPC Console Peering Connections . Select the Create peering connection button. On the Create peering connection page, enter a name. For the Select a local VPC to peer with, VPC ID (Requester) , select the VPC ID of the Log processing network . For the Select another VPC to peer with, VPC ID (Accepter) , select the VPC ID of the OpenSearch domain network . Choose Create peering connection , and navigate to the peering connection detail page. Click the Actions button and choose Accept request . Update Route Tables Go to the Centralized Logging with OpenSearch console. In the OpenSearch domain network section, click the subnet under AZs and Subnets to open the subnet console in a new tab. Select the subnet, and choose the Route table tab. Select the associated route table of the subnet to open the route table configuration page. Select the Routes tab, and choose Edit routes . Add a route 10.255.0.0/16 (the CIDR of Centralized Logging with OpenSearch, if you created the solution with existing VPC, please change this value) pointing to the Peering Connection you just created. Go back to the Centralized Logging with OpenSearch console. Click the VPC ID under the OpenSearch domain network section. Select the VPC ID on the VPC Console and find its IPv4 CIDR . On the Centralized Logging with OpenSearch console, in the Log processing network section, click the subnets under AZs and Subnets to open the subnets in new tabs. Repeat step 3, 4, 5, 6 to add an opposite route. Namely, configure the IPv4 CIDR of the OpenSearch VPC to point to the Peering Connection. You need to repeat the steps for each subnet of Log processing network. Update Security Group of OpenSearch Domain On the Centralized Logging with OpenSearch console, under the OpenSearch domain network section, select the Security Group ID in Security Groups to open the Security Group in a new tab. On the console, select Edit inbound rules . Add the rule ALLOW TCP/443 from 10.255.0.0/16 (the CIDR of Centralized Logging with OpenSearch, if you created Centralized Logging with OpenSearch with existing VPC, change this value). Choose Save rules . Remove an Amazon OpenSearch Service domain If needed, you can remove the Amazon OpenSearch Service domains. Important Removing the domain from Centralized Logging with OpenSearch will NOT delete the Amazon OpenSearch Service domain in your AWS account. It will NOT impact any existing log analytics pipelines. Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch Domains . Select the domain from the table. Choose Remove . In the confirmation dialog box, choose Remove .","title":"Domain operations"},{"location":"implementation-guide/domains/import/#domain-operations","text":"Once logged into the Centralized Logging with OpenSearch console, you can import an Amazon OpenSearch Service domain.","title":"Domain Operations"},{"location":"implementation-guide/domains/import/#prerequisite","text":"Centralized Logging with OpenSearch supports Amazon OpenSearch Service, engine version Elasticsearch 7.10 or later, and engine version OpenSearch 1.0 or later. Centralized Logging with OpenSearch supports OpenSearch clusters within VPC. If you don't have an Amazon OpenSearch Service domain yet, you can create an Amazon OpenSearch Service domain within VPC. See Launching your Amazon OpenSearch Service domains within a VPC . Centralized Logging with OpenSearch supports OpenSearch clusters with fine-grained access control only. In the security configuration, the Access policy should look like the image below:","title":"Prerequisite"},{"location":"implementation-guide/domains/import/#import-an-amazon-opensearch-service-domain","text":"Sign in to the Centralized Logging with OpenSearch console. In the left navigation panel, under Domains , choose Import OpenSearch Domain . On the Select domain page, choose a domain from the dropdown list. The dropdown list will display only domains in the same region as the solution. Choose Next . On the Configure network page, under Network creation , choose Manual and click Next ; or choose Automatic , and go to step 9. Under VPC , choose a VPC from the list. By default, the solution creates a standalone VPC, and you can choose the one named LogHubVpc/DefaultVPC . You can also choose the same VPC as your Amazon OpenSearch Service domains. Under Log Processing Subnet Group , select at least 2 subnets from the dropdown list. By default, the solution creates two private subnets. You can choose subnets named LogHubVpc/DefaultVPC/privateSubnet1 and LogHubVpc/DefaultVPC/privateSubnet2 . Under Log Processing Security Group , select one from the dropdown list. By default, the solution creates one Security Group named ProcessSecurityGroup . On the Create tags page, add tags if needed. Choose Import .","title":"Import an Amazon OpenSearch Service Domain"},{"location":"implementation-guide/domains/import/#set-up-vpc-peering","text":"By default, the solution creates a standalone VPC. You need to create VPC Peering to allow the log processing layer to have access to your Amazon OpenSearch Service domains. Note Automatic mode will create VPC peering and configure route table automatically. You do not need to set up VPC peering again. Follow this section to create VPC peering, update security group and update route tables.","title":"Set up VPC Peering"},{"location":"implementation-guide/domains/import/#create-vpc-peering-connection","text":"Sign in to the Centralized Logging with OpenSearch console. In the left navigation panel, under Domains , select OpenSearch Domains . Find the domain you imported and select the domain name. Choose the Network tab. Copy the VPC ID in both sections OpenSearch domain network and Log processing network . You will create Peering Connection between these two VPCs. Navigate to VPC Console Peering Connections . Select the Create peering connection button. On the Create peering connection page, enter a name. For the Select a local VPC to peer with, VPC ID (Requester) , select the VPC ID of the Log processing network . For the Select another VPC to peer with, VPC ID (Accepter) , select the VPC ID of the OpenSearch domain network . Choose Create peering connection , and navigate to the peering connection detail page. Click the Actions button and choose Accept request .","title":"Create VPC Peering Connection"},{"location":"implementation-guide/domains/import/#update-route-tables","text":"Go to the Centralized Logging with OpenSearch console. In the OpenSearch domain network section, click the subnet under AZs and Subnets to open the subnet console in a new tab. Select the subnet, and choose the Route table tab. Select the associated route table of the subnet to open the route table configuration page. Select the Routes tab, and choose Edit routes . Add a route 10.255.0.0/16 (the CIDR of Centralized Logging with OpenSearch, if you created the solution with existing VPC, please change this value) pointing to the Peering Connection you just created. Go back to the Centralized Logging with OpenSearch console. Click the VPC ID under the OpenSearch domain network section. Select the VPC ID on the VPC Console and find its IPv4 CIDR . On the Centralized Logging with OpenSearch console, in the Log processing network section, click the subnets under AZs and Subnets to open the subnets in new tabs. Repeat step 3, 4, 5, 6 to add an opposite route. Namely, configure the IPv4 CIDR of the OpenSearch VPC to point to the Peering Connection. You need to repeat the steps for each subnet of Log processing network.","title":"Update Route Tables"},{"location":"implementation-guide/domains/import/#update-security-group-of-opensearch-domain","text":"On the Centralized Logging with OpenSearch console, under the OpenSearch domain network section, select the Security Group ID in Security Groups to open the Security Group in a new tab. On the console, select Edit inbound rules . Add the rule ALLOW TCP/443 from 10.255.0.0/16 (the CIDR of Centralized Logging with OpenSearch, if you created Centralized Logging with OpenSearch with existing VPC, change this value). Choose Save rules .","title":"Update Security Group of OpenSearch Domain"},{"location":"implementation-guide/domains/import/#remove-an-amazon-opensearch-service-domain","text":"If needed, you can remove the Amazon OpenSearch Service domains. Important Removing the domain from Centralized Logging with OpenSearch will NOT delete the Amazon OpenSearch Service domain in your AWS account. It will NOT impact any existing log analytics pipelines. Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch Domains . Select the domain from the table. Choose Remove . In the confirmation dialog box, choose Remove .","title":"Remove an Amazon OpenSearch Service domain"},{"location":"implementation-guide/domains/proxy/","text":"By default, an Amazon OpenSearch Service domain within VPC cannot be accessed from the Internet. Centralized Logging with OpenSearch creates a highly available Nginx cluster which allows you to access the OpenSearch Dashboards from the Internet. Alternatively, you can choose to access the Amazon OpenSearch Service domains using SSH Tunnel . This section introduces the proxy stack architecture and how to complete the following: Create a proxy Create an associated DNS record Access Amazon OpenSearch Service via proxy Delete a proxy Architecture Centralized Logging with OpenSearch creates an Auto Scaling Group (ASG) together with an Application Load Balancer (ALB) . The workflow is as follows: Users access the custom domain for the proxy, and the domain needs to be resolved via DNS service (for example, using Route 53 on AWS). The DNS service routes the traffic to internet-facing ALB. The ALB distributes traffic to backend Nginx server running on Amazon EC2 within ASG. The Nginx server redirects the requests to OpenSearch Dashboards. (optional) VPC peering is required if the VPC for the proxy is not the same as the OpenSearch service. Create a proxy You can create the Nginx-based proxy using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Prerequisites Make sure an Amazon OpenSearch Service domain within VPC is available. The domain associated SSL certificate is created or uploaded in Amazon Certificate Manager (ACM) . Make sure you have the EC2 private key (.pem) file. Using the Centralized Logging with OpenSearch console Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Under General configuration , choose Enable at the Access Proxy label. Note Once the access proxy is enabled, a link to the access proxy will be available. On the Create access proxy page, under Public access proxy , select at least 2 subnets for Public Subnets . You can choose 2 public subnets named LogHubVPC/DefaultVPC/publicSubnet , which are created by Centralized Logging with OpenSearch by default. Choose a Security Group of the ALB in Public Security Group . You can choose a security group named ProxySecurityGroup , which is created by Centralized Logging with OpenSearch default. Enter the Domain Name . Choose Load Balancer SSL Certificate associated with the domain name. Choose the Nginx Instance Key Name . Choose Create . Using the CloudFormation stack This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Nginx access proxy solution in the AWS Cloud. Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description VPCId <Requires input> The VPC to deploy the Nginx proxy resources, for example, vpc-bef13dc7 . PublicSubnetIds <Requires input> The public subnets where ELB are deployed. You need to select at least two public subnets, for example, subnet-12345abc, subnet-54321cba . PrivateSubnetIds <Requires input> The private subnets where Nginx instances are deployed. You need to select at least two private subnets, for example, subnet-12345abc, subnet-54321cba . KeyName <Requires input> The PEM key name of the Nginx instances. NginxSecurityGroupId <Requires input> The Security group associated with the Nginx instances. The security group must allow access from ELB security group. ProxyInstanceType t3.large OpenSearch proxy instance type. e.g. t3.micro ProxyInstanceNumber 2 OpenSearch proxy instance number. e.g. 1 to 4 Endpoint <Requires input> The OpenSearch endpoint, for example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com . EngineType OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. CognitoEndpoint <Optional> The Cognito User Pool endpoint URL of the OpenSearch domain, for example, mydomain.auth.us-east-1.amazoncognito.com . Leave empty if your OpenSearch domain is not authenticated through Cognito User Pool. ELBSecurityGroupId <Requires input> The Security group being associated with the ELB, for example, sg-123456 . ELBDomain <Requires input> The custom domain name of the ELB, for example, dashboard.example.com . ELBDomainCertificateArn <Requires input> The SSL certificate ARN associated with the ELBDomain. The certificate must be created from Amazon Certificate Manager (ACM) . ELBAccessLogBucketName <Requires input> The Access Log Bucket Name for Proxy ELB SsmParameterValueawsserviceamiamazonlinuxlatestamzn2amihvmx8664gp2C96584B6F00A464EAD1953AFF4B05118Parameter /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 The SSM parameter of the proxy instance AMI. You can use the default value in most cases. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. Recommended Proxy Configuration The following table provides a list of recommended proxy configuration examples for different number of concurrent users. You can create proxy according to your own use cases. Number of Concurrent Users Proxy Instance Type Number of Proxy Instances 4 t3.nano 1 6 t3.micro 1 8 t3.nano 2 10 t3.small 1 12 t3.micro 2 20 t3.small 2 25 t3.large 1 50+ t3.large 2 Create an associated DNS record After provisioning the proxy infrastructure, you need to create an associated DNS record in your DNS resolver. The following introduces how to find the ALB domain, and then create a CNAME record pointing to this domain. Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Choose the Access Proxy tab. You can see Load Balancer Domain which is the ALB domain. Go to the DNS resolver, create a CNAME record pointing to this domain. If your domain is managed by Amazon Route 53 , refer to Creating records by using the Amazon Route 53 console . Access Amazon OpenSearch Service via proxy After the DNS record takes effect, you can access the Amazon OpenSearch Service built-in dashboard from anywhere via proxy. You can enter the domain of the proxy in your browser, or click the Link button under Access Proxy in the General Configuration section. Delete a Proxy Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Choose the Access Proxy tab. Choose the Delete . On the confirmation prompt, choose Delete .","title":"Access proxy"},{"location":"implementation-guide/domains/proxy/#architecture","text":"Centralized Logging with OpenSearch creates an Auto Scaling Group (ASG) together with an Application Load Balancer (ALB) . The workflow is as follows: Users access the custom domain for the proxy, and the domain needs to be resolved via DNS service (for example, using Route 53 on AWS). The DNS service routes the traffic to internet-facing ALB. The ALB distributes traffic to backend Nginx server running on Amazon EC2 within ASG. The Nginx server redirects the requests to OpenSearch Dashboards. (optional) VPC peering is required if the VPC for the proxy is not the same as the OpenSearch service.","title":"Architecture"},{"location":"implementation-guide/domains/proxy/#create-a-proxy","text":"You can create the Nginx-based proxy using the Centralized Logging with OpenSearch console or by deploying a standalone CloudFormation stack. Prerequisites Make sure an Amazon OpenSearch Service domain within VPC is available. The domain associated SSL certificate is created or uploaded in Amazon Certificate Manager (ACM) . Make sure you have the EC2 private key (.pem) file.","title":"Create a proxy"},{"location":"implementation-guide/domains/proxy/#using-the-centralized-logging-with-opensearch-console","text":"Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Under General configuration , choose Enable at the Access Proxy label. Note Once the access proxy is enabled, a link to the access proxy will be available. On the Create access proxy page, under Public access proxy , select at least 2 subnets for Public Subnets . You can choose 2 public subnets named LogHubVPC/DefaultVPC/publicSubnet , which are created by Centralized Logging with OpenSearch by default. Choose a Security Group of the ALB in Public Security Group . You can choose a security group named ProxySecurityGroup , which is created by Centralized Logging with OpenSearch default. Enter the Domain Name . Choose Load Balancer SSL Certificate associated with the domain name. Choose the Nginx Instance Key Name . Choose Create .","title":"Using the Centralized Logging with OpenSearch console"},{"location":"implementation-guide/domains/proxy/#using-the-cloudformation-stack","text":"This automated AWS CloudFormation template deploys the Centralized Logging with OpenSearch - Nginx access proxy solution in the AWS Cloud. Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation. To launch the stack in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your stack. Under Parameters , review the parameters for the template and modify them as necessary. This solution uses the following parameters. Parameter Default Description VPCId <Requires input> The VPC to deploy the Nginx proxy resources, for example, vpc-bef13dc7 . PublicSubnetIds <Requires input> The public subnets where ELB are deployed. You need to select at least two public subnets, for example, subnet-12345abc, subnet-54321cba . PrivateSubnetIds <Requires input> The private subnets where Nginx instances are deployed. You need to select at least two private subnets, for example, subnet-12345abc, subnet-54321cba . KeyName <Requires input> The PEM key name of the Nginx instances. NginxSecurityGroupId <Requires input> The Security group associated with the Nginx instances. The security group must allow access from ELB security group. ProxyInstanceType t3.large OpenSearch proxy instance type. e.g. t3.micro ProxyInstanceNumber 2 OpenSearch proxy instance number. e.g. 1 to 4 Endpoint <Requires input> The OpenSearch endpoint, for example, vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com . EngineType OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. CognitoEndpoint <Optional> The Cognito User Pool endpoint URL of the OpenSearch domain, for example, mydomain.auth.us-east-1.amazoncognito.com . Leave empty if your OpenSearch domain is not authenticated through Cognito User Pool. ELBSecurityGroupId <Requires input> The Security group being associated with the ELB, for example, sg-123456 . ELBDomain <Requires input> The custom domain name of the ELB, for example, dashboard.example.com . ELBDomainCertificateArn <Requires input> The SSL certificate ARN associated with the ELBDomain. The certificate must be created from Amazon Certificate Manager (ACM) . ELBAccessLogBucketName <Requires input> The Access Log Bucket Name for Proxy ELB SsmParameterValueawsserviceamiamazonlinuxlatestamzn2amihvmx8664gp2C96584B6F00A464EAD1953AFF4B05118Parameter /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 The SSM parameter of the proxy instance AMI. You can use the default value in most cases. Choose Next . On the Configure stack options page, choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Using the CloudFormation stack"},{"location":"implementation-guide/domains/proxy/#recommended-proxy-configuration","text":"The following table provides a list of recommended proxy configuration examples for different number of concurrent users. You can create proxy according to your own use cases. Number of Concurrent Users Proxy Instance Type Number of Proxy Instances 4 t3.nano 1 6 t3.micro 1 8 t3.nano 2 10 t3.small 1 12 t3.micro 2 20 t3.small 2 25 t3.large 1 50+ t3.large 2","title":"Recommended Proxy Configuration"},{"location":"implementation-guide/domains/proxy/#create-an-associated-dns-record","text":"After provisioning the proxy infrastructure, you need to create an associated DNS record in your DNS resolver. The following introduces how to find the ALB domain, and then create a CNAME record pointing to this domain. Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Choose the Access Proxy tab. You can see Load Balancer Domain which is the ALB domain. Go to the DNS resolver, create a CNAME record pointing to this domain. If your domain is managed by Amazon Route 53 , refer to Creating records by using the Amazon Route 53 console .","title":"Create an associated DNS record"},{"location":"implementation-guide/domains/proxy/#access-amazon-opensearch-service-via-proxy","text":"After the DNS record takes effect, you can access the Amazon OpenSearch Service built-in dashboard from anywhere via proxy. You can enter the domain of the proxy in your browser, or click the Link button under Access Proxy in the General Configuration section.","title":"Access Amazon OpenSearch Service via proxy"},{"location":"implementation-guide/domains/proxy/#delete-a-proxy","text":"Log in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Choose the Access Proxy tab. Choose the Delete . On the confirmation prompt, choose Delete .","title":"Delete a Proxy"},{"location":"implementation-guide/getting-started/","text":"Getting Started After deploying the solution , refer to this section to quickly learn how to leverage Centralized Logging with OpenSearch for log ingestion (Amazon CloudTrail logs as an example), and log visualization. You can also choose to start with Domain management , then build AWS Service Log Analytics Pipelines and Application Log Analytics Pipelines . Steps Step 1: Import an Amazon OpenSearch Service domain . Import an existing Amazon OpenSearch Service domain into the solution. Step 2: Create Access Proxy . Create a public access proxy which allows you to access the templated dashboard from anywhere. Step 3: Ingest CloudTrail Logs . Ingest CloudTrail logs into the specified Amazon OpenSearch Service domain. Step 4: Access built-in dashboard . View the dashboard of CloudTrail logs.","title":"Overview"},{"location":"implementation-guide/getting-started/#getting-started","text":"After deploying the solution , refer to this section to quickly learn how to leverage Centralized Logging with OpenSearch for log ingestion (Amazon CloudTrail logs as an example), and log visualization. You can also choose to start with Domain management , then build AWS Service Log Analytics Pipelines and Application Log Analytics Pipelines .","title":"Getting Started"},{"location":"implementation-guide/getting-started/#steps","text":"Step 1: Import an Amazon OpenSearch Service domain . Import an existing Amazon OpenSearch Service domain into the solution. Step 2: Create Access Proxy . Create a public access proxy which allows you to access the templated dashboard from anywhere. Step 3: Ingest CloudTrail Logs . Ingest CloudTrail logs into the specified Amazon OpenSearch Service domain. Step 4: Access built-in dashboard . View the dashboard of CloudTrail logs.","title":"Steps"},{"location":"implementation-guide/getting-started/1.import-domain/","text":"Step 1: Import an Amazon OpenSearch domain To use the Centralized Logging with OpenSearch solution for the first time, you must import Amazon OpenSearch Service domains first. Centralized Logging with OpenSearch supports Amazon OpenSearch domain with fine-grained access control enabled within a VPC only. Important Currently, Centralized Logging with OpenSearch supports Amazon OpenSearch Service with engine version Elasticsearch 7.10 or later, and OpenSearch 1.0 or later. Prerequisite At least one Amazon OpenSearch Service domain within VPC. If you don't have an Amazon OpenSearch Service domain yet, you can create an Amazon OpenSearch Service domain within VPC. See Launching your Amazon OpenSearch Service domains within a VPC . Steps Use the following procedure to import an Amazon OpenSearch Service domain through the Centralized Logging with OpenSearch console. Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose Import OpenSearch Domain . On the Step 1. Select domain page, choose a domain from the dropdown list. Choose Next . On the Step 2. Configure network page, under Network creation , choose Automatic . If your Centralized Logging with OpenSearch and OpenSearch domains reside in two different VPCs, the Automatic mode will create a VPC Peering Connection between them, and update route tables. See details in Set up VPC Peering . On the Step 3. Create tags page, choose Import .","title":"Step 1. Import domain"},{"location":"implementation-guide/getting-started/1.import-domain/#step-1-import-an-amazon-opensearch-domain","text":"To use the Centralized Logging with OpenSearch solution for the first time, you must import Amazon OpenSearch Service domains first. Centralized Logging with OpenSearch supports Amazon OpenSearch domain with fine-grained access control enabled within a VPC only. Important Currently, Centralized Logging with OpenSearch supports Amazon OpenSearch Service with engine version Elasticsearch 7.10 or later, and OpenSearch 1.0 or later.","title":"Step 1: Import an Amazon OpenSearch domain"},{"location":"implementation-guide/getting-started/1.import-domain/#prerequisite","text":"At least one Amazon OpenSearch Service domain within VPC. If you don't have an Amazon OpenSearch Service domain yet, you can create an Amazon OpenSearch Service domain within VPC. See Launching your Amazon OpenSearch Service domains within a VPC .","title":"Prerequisite"},{"location":"implementation-guide/getting-started/1.import-domain/#steps","text":"Use the following procedure to import an Amazon OpenSearch Service domain through the Centralized Logging with OpenSearch console. Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose Import OpenSearch Domain . On the Step 1. Select domain page, choose a domain from the dropdown list. Choose Next . On the Step 2. Configure network page, under Network creation , choose Automatic . If your Centralized Logging with OpenSearch and OpenSearch domains reside in two different VPCs, the Automatic mode will create a VPC Peering Connection between them, and update route tables. See details in Set up VPC Peering . On the Step 3. Create tags page, choose Import .","title":"Steps"},{"location":"implementation-guide/getting-started/2.create-proxy/","text":"Step 2: Create Access Proxy Note Access proxy is optional and it incurs additional cost. If you can connect to Amazon OpenSearch's VPC (such as through VPN connection), you don't need to activate access proxy. You need to use it only if you want to connect to Amazon OpenSearch dashboard from public Internet. You can create a Nginx proxy and create an DNS record pointing to the proxy, so that you can access the Amazon OpenSearch Service dashboard securely from public network. For more information, refer to Access Proxy in the Domain Management chapter. Create a Nginx proxy Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Under General configuration , choose Enable at the Access Proxy label. On the Create access proxy page, under Public access proxy , select at least 2 subnets which contain LogHubVpc/DefaultVPC/publicSubnetX for the Public Subnets . For Public Security Group , choose the Security Group which contains ProxySecurityGroup . Enter the Domain Name . Choose the associated Load Balancer SSL Certificate which applies to the domain name. Choose the Nginx Instance Key Name . Choose Create . After provisioning the proxy infrastructure, you need to create an associated DNS record in your DNS resolver. The following introduces how to find the Application Load Balancing (ALB) domain, and then create a CNAME record pointing to this domain. Create an DNS record Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Choose the Access Proxy tab. Find Load Balancer Domain , which is the ALB domain. Go to the DNS resolver, and create a CNAME record pointing to this domain. If your domain is managed by Amazon Route 53 , refer to Creating records by using the Amazon Route 53 console .","title":"Step 2. Create proxy"},{"location":"implementation-guide/getting-started/2.create-proxy/#step-2-create-access-proxy","text":"Note Access proxy is optional and it incurs additional cost. If you can connect to Amazon OpenSearch's VPC (such as through VPN connection), you don't need to activate access proxy. You need to use it only if you want to connect to Amazon OpenSearch dashboard from public Internet. You can create a Nginx proxy and create an DNS record pointing to the proxy, so that you can access the Amazon OpenSearch Service dashboard securely from public network. For more information, refer to Access Proxy in the Domain Management chapter.","title":"Step 2: Create Access Proxy"},{"location":"implementation-guide/getting-started/2.create-proxy/#create-a-nginx-proxy","text":"Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Under General configuration , choose Enable at the Access Proxy label. On the Create access proxy page, under Public access proxy , select at least 2 subnets which contain LogHubVpc/DefaultVPC/publicSubnetX for the Public Subnets . For Public Security Group , choose the Security Group which contains ProxySecurityGroup . Enter the Domain Name . Choose the associated Load Balancer SSL Certificate which applies to the domain name. Choose the Nginx Instance Key Name . Choose Create . After provisioning the proxy infrastructure, you need to create an associated DNS record in your DNS resolver. The following introduces how to find the Application Load Balancing (ALB) domain, and then create a CNAME record pointing to this domain.","title":"Create a Nginx proxy"},{"location":"implementation-guide/getting-started/2.create-proxy/#create-an-dns-record","text":"Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Domains , choose OpenSearch domains . Select the domain from the table. Choose the Access Proxy tab. Find Load Balancer Domain , which is the ALB domain. Go to the DNS resolver, and create a CNAME record pointing to this domain. If your domain is managed by Amazon Route 53 , refer to Creating records by using the Amazon Route 53 console .","title":"Create an DNS record"},{"location":"implementation-guide/getting-started/3.build-cloudtrail-pipeline/","text":"Step 3: Ingest Amazon CloudTrail Logs You can build a log analytics pipeline to ingest Amazon CloudTrail logs. Important Make sure your CloudTrail and Centralized Logging with OpenSearch are in the same AWS Region. Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, select AWS Service Log Analytics Pipelines . Choose Create a log ingestion . In the AWS Services section, choose Amazon CloudTrail . Choose Next . Under Specify settings , for Trail , select one from the dropdown list. Choose Next . In the Specify OpenSearch domain section, select the imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard . Keep default values and choose Next . Choose Create .","title":"Step 3. Ingest CloudTrail logs"},{"location":"implementation-guide/getting-started/3.build-cloudtrail-pipeline/#step-3-ingest-amazon-cloudtrail-logs","text":"You can build a log analytics pipeline to ingest Amazon CloudTrail logs. Important Make sure your CloudTrail and Centralized Logging with OpenSearch are in the same AWS Region. Sign in to the Centralized Logging with OpenSearch Console. In the navigation pane, select AWS Service Log Analytics Pipelines . Choose Create a log ingestion . In the AWS Services section, choose Amazon CloudTrail . Choose Next . Under Specify settings , for Trail , select one from the dropdown list. Choose Next . In the Specify OpenSearch domain section, select the imported domain for Amazon OpenSearch domain . Choose Yes for Sample dashboard . Keep default values and choose Next . Choose Create .","title":"Step 3: Ingest Amazon CloudTrail Logs"},{"location":"implementation-guide/getting-started/4.view-dashboard/","text":"Step 4: Access built-in Dashboard After the DNS record takes effect, you can access the built-in dashboard from anywhere via proxy. Enter the domain of the proxy in your browser. Alternatively, click the Link button under Access Proxy in the General Configuration section of the domain. Enter your credentials to log in to Amazon OpenSearch Dashboard. Click the username icon of Amazon OpenSearch Service dashboard from the top right corner. Choose Switch Tenants . On the Select your tenant page, choose Global , and click Confirm . On the left navigation panel, choose Dashboards . Choose the dashboard created automatically and start to explore your data.","title":"Step 4. Access dashboard"},{"location":"implementation-guide/getting-started/4.view-dashboard/#step-4-access-built-in-dashboard","text":"After the DNS record takes effect, you can access the built-in dashboard from anywhere via proxy. Enter the domain of the proxy in your browser. Alternatively, click the Link button under Access Proxy in the General Configuration section of the domain. Enter your credentials to log in to Amazon OpenSearch Dashboard. Click the username icon of Amazon OpenSearch Service dashboard from the top right corner. Choose Switch Tenants . On the Select your tenant page, choose Global , and click Confirm . On the left navigation panel, choose Dashboards . Choose the dashboard created automatically and start to explore your data.","title":"Step 4: Access built-in Dashboard"},{"location":"implementation-guide/link-account/","text":"Cross-Account Ingestion Centralized Logging with OpenSearch supports ingesting AWS Service logs and Application logs in different AWS accounts within the same region. After deploying Centralized Logging with OpenSearch in one account (main account), you can launch the CloudFormation stack in a different account (member account), and associate the two accounts (main account and member account) to implement cross-account ingestion. Concepts Main account : One account in which you deployed the Centralized Logging with OpenSearch console. The OpenSearch cluster(s) must also be in the same account. Member account : Another account from which you want to ingest AWS Service logs or application logs. The CloudFormation stack in the member account has the least privileges. Centralized Logging with OpenSearch need to provision some AWS resources in the member account to collect logs, and will assume an IAM role provisioned in the member account to list or create resources. For more information, refer to the Architecture section. Add a member account Step 1. Launch a CloudFormation stack in the member account Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Resources , choose Member Accounts . Choose the Link an Account button. It displays the steps to deploy the CloudFormation stack in the member account. Important You need to copy the template URL, which will be used later. Go to the CloudFormation console of the member account. Choose the Create stack button and choose With new resources (standard) . In the Create stack page, enter the template URL you have copied in Amazon S3 URL . Follow the steps to create the CloudFormation stack and wait until the CloudFormation stack is provisioned. Go to the Outputs tab to check the parameters which will be used in Step 2 . Step 2. Add a member account Go back to the Centralized Logging with OpenSearch console. (Optional) In the navigation panel, under Resources , choose Member Accounts . In Step 2. Link an account , enter the parameters using the Outputs parameters from Step 1 . Parameter CloudFormation Outputs Description Account Name N/A Name of the member account. Account ID N/A 12-digit AWS account ID. Cross Account Role ARN CrossAccountRoleARN Centralized Logging with OpenSearch will assume this role to operate resources in the member account. FluentBit Agent Installation Document AgentInstallDocument Centralized Logging with OpenSearch will use this SSM Document to install Fluent Bit agent on EC2 instances in the member account. FluentBit Agent Configuration Document AgentConfigDocument Centralized Logging with OpenSearch will use this SSM Document to deliver Fluent Bit configuration to EC2 instances. Cross Account S3 Bucket CrossAccountS3Bucket You can use the Centralized Logging with OpenSearch console to enable some AWS Service logs and output them to Amazon S3. The logs will be stored in this account. Cross Account Stack ID CrossAccountStackId CloudFormation stack ID in the member account. Cross Account KMS Key CrossAccountKMSKeyARN Centralized Logging with OpenSearch will use the Key Management Services (KMS) key to encrypt Simple Queue Service (SQS). Click the Link button.","title":"Cross-account ingestion"},{"location":"implementation-guide/link-account/#cross-account-ingestion","text":"Centralized Logging with OpenSearch supports ingesting AWS Service logs and Application logs in different AWS accounts within the same region. After deploying Centralized Logging with OpenSearch in one account (main account), you can launch the CloudFormation stack in a different account (member account), and associate the two accounts (main account and member account) to implement cross-account ingestion.","title":"Cross-Account Ingestion"},{"location":"implementation-guide/link-account/#concepts","text":"Main account : One account in which you deployed the Centralized Logging with OpenSearch console. The OpenSearch cluster(s) must also be in the same account. Member account : Another account from which you want to ingest AWS Service logs or application logs. The CloudFormation stack in the member account has the least privileges. Centralized Logging with OpenSearch need to provision some AWS resources in the member account to collect logs, and will assume an IAM role provisioned in the member account to list or create resources. For more information, refer to the Architecture section.","title":"Concepts"},{"location":"implementation-guide/link-account/#add-a-member-account","text":"","title":"Add a member account"},{"location":"implementation-guide/link-account/#step-1-launch-a-cloudformation-stack-in-the-member-account","text":"Sign in to the Centralized Logging with OpenSearch console. In the navigation pane, under Resources , choose Member Accounts . Choose the Link an Account button. It displays the steps to deploy the CloudFormation stack in the member account. Important You need to copy the template URL, which will be used later. Go to the CloudFormation console of the member account. Choose the Create stack button and choose With new resources (standard) . In the Create stack page, enter the template URL you have copied in Amazon S3 URL . Follow the steps to create the CloudFormation stack and wait until the CloudFormation stack is provisioned. Go to the Outputs tab to check the parameters which will be used in Step 2 .","title":"Step 1. Launch a CloudFormation stack in the member account"},{"location":"implementation-guide/link-account/#step-2-add-a-member-account","text":"Go back to the Centralized Logging with OpenSearch console. (Optional) In the navigation panel, under Resources , choose Member Accounts . In Step 2. Link an account , enter the parameters using the Outputs parameters from Step 1 . Parameter CloudFormation Outputs Description Account Name N/A Name of the member account. Account ID N/A 12-digit AWS account ID. Cross Account Role ARN CrossAccountRoleARN Centralized Logging with OpenSearch will assume this role to operate resources in the member account. FluentBit Agent Installation Document AgentInstallDocument Centralized Logging with OpenSearch will use this SSM Document to install Fluent Bit agent on EC2 instances in the member account. FluentBit Agent Configuration Document AgentConfigDocument Centralized Logging with OpenSearch will use this SSM Document to deliver Fluent Bit configuration to EC2 instances. Cross Account S3 Bucket CrossAccountS3Bucket You can use the Centralized Logging with OpenSearch console to enable some AWS Service logs and output them to Amazon S3. The logs will be stored in this account. Cross Account Stack ID CrossAccountStackId CloudFormation stack ID in the member account. Cross Account KMS Key CrossAccountKMSKeyARN Centralized Logging with OpenSearch will use the Key Management Services (KMS) key to encrypt Simple Queue Service (SQS). Click the Link button.","title":"Step 2. Add a member account"},{"location":"implementation-guide/plan-deployment/considerations/","text":"This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List . Centralized Logging with OpenSearch provides two types of authentication, Cognito User Pool and OpenID Connect (OIDC) Provider . You must choose to launch the solution with OpenID Connect if one of the following cases occurs: Cognito User Pool is not available in your AWS Region. You already have an OpenID Connect Provider and want to authenticate against it. Supported regions for deployment Region Name Launch with Cognito User Pool Launch with OpenID Connect US East (N. Virginia) US East (Ohio) US West (N. California) US West (Oregon) Africa (Cape Town) Asia Pacific (Hong Kong) Asia Pacific (Mumbai) Asia Pacific (Osaka) Asia Pacific (Seoul) Asia Pacific (Singapore) Asia Pacific (Sydney) Asia Pacific (Tokyo) Canada (Central) Europe (Frankfurt) Europe (Ireland) Europe (London) Europe (Milan) Europe (Paris) Europe (Stockholm) Middle East (Bahrain) South America (Sao Paulo) China (Beijing) Region Operated by Sinnet China (Ningxia) Regions operated by NWCD Important You can have only one active Centralized Logging with OpenSearch solution stack in one region. If your deployment failed, make sure you have deleted the failed stack before retrying the deployment.","title":"Supported AWS Regions"},{"location":"implementation-guide/plan-deployment/cost/","text":"Cost Estimation Important The following cost estimations are examples and may vary depending on your environment. You will be responsible for the cost of the AWS services used when running the solution. The main factors affecting the solution cost include: Type of logs to be ingested Volume of logs to be ingested/processed Size of the log message Location of logs Additional features As of this revision, the following examples demonstrate the cost estimation of 10/100/1000 GB daily log ingestion for running this solution with default settings in the US East (N. Virginia) Region. The total cost is composed of Amazon OpenSearch Cost Or Light Engine Cost , Solution Console Cost and Additional Features Cost . Use OpenSearch as log process engine Amazon OpenSearch Cost OD : On Demand AURI_1 : All Upfront Reserved Instance 1 Year Tiering : The days stored in each tier. For example, 7H + 23W + 60C indicates that the log is stored in hot tier for 7 days, warm tier for 23 days, and cold tier for 60 days. Replica : The number of shard replicas. Daily log Volume (GB) Retention (days) Tiering Replica OD Monthly (USD) AURI_1 Monthly (USD) Dedicated Master Data Node EBS (GB) UltraWarm Nodes UltraWarm/Cold S3 Storage (GB) OD cost per GB (USD) AURI_1 cost per GB ($) 10 30 30H 0 216.28 158.54 N/A c6g.large[2] 380 N/A 0 0.72093 0.52847 10 30 30H 1 289.35 223.94 N/A m6g.large[2] 760 N/A 0 0.9645 0.74647 100 30 7H + 23W 0 989.49 825.97 m6g.large[3] m6g.large[2] 886 medium[2] 0 0.32983 0.27532 100 30 7H + 23W 1 1295.85 1066.92 m6g.large[3] m6g.large[4] 1772 medium[2] 0 0.43195 0.35564 100 90 7H + 23W + 60C 0 1133.49 969.97 m6g.large[3] m6g.large[2] 886 medium[2] 8300 0.12594 0.10777 100 90 7H + 23W + 60C 1 1439.85 1210.92 m6g.large[3] m6g.large[4] 1772 medium[2] 8300 0.15998 0.13455 100 180 7H + 23W + 150C 0 1349.49 1185.97 m6g.large[3] m6g.large[2] 886 medium[2] 17300 0.07497 0.06589 100 180 7H + 23W + 150C 1 1655.85 1426.92 m6g.large[3] m6g.large[4] 1772 medium[2] 17300 0.09199 0.07927 1000 30 7H + 23W 0 6101.15 5489.48 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 23000 0.20337 0.18298 1000 30 7H + 23W 1 8759.49 7635.8 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 23000 0.29198 0.25453 1000 90 7H + 23W + 60C 0 8027.33 7245.45 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 83000 0.08919 0.0805 1000 90 7H + 23W + 60C 1 10199.49 9075.8 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 83000 0.11333 0.10084 1000 180 7H + 23W + 150C 0 9701.15 9089.48 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 173000 0.0539 0.0505 1000 180 7H + 23W + 150C 1 12644.19 11420.86 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 173000 0.07025 0.06345 Processing Cost Log ingestion through Amazon S3 This section is applicable to: AWS service logs including Amazon S3 access logs, CloudFront standard logs, CloudTrail logs (S3), Application Load Balancing access logs, WAF logs, VPC Flow logs (S3), AWS Config logs, Amazon RDS/Aurora logs, and AWS Lambda Logs. Application Logs that use Amazon S3 as data buffer. Assumptions: The logs stored in Amazon S3 are in gzip format. A 4MB compressed log file in S3 is roughly 100 MB in raw log size. A Lambda with 1 GB memory takes about 26 seconds to process a 4 MB compressed log file, namely 260 milliseconds (ms) per MB raw logs. The maximum compressed log file size is 5 MB. Ingesting logs from S3 will incur SQS and S3 request fees which are very low, or usually within the free tier. You have N GB raw log per day, and the daily cost estimation is as follows: When you use Lambda as log processor: Lambda Cost = 260 ms per MB x 1024 MB x N GB/day x $0.0000000167 per ms S3 Storage Cost = $0.023 per GB x N GB/day x 4% (compression) When you use OSI as log processor: OSI Pipeline Cost = $0.24 per OCU per hour The maximum amount of S3 data 1 OCU can handle is around 20MB/s The total monthly cost for ingesting AWS service logs is: Total Monthly Cost (Lambda as processor) = (Lambda Cost + S3 Storage Cost) x 30 days Daily Log Volume Daily Lambda Cost (USD) Daily S3 Storage Cost (USD) Monthly Cost (USD) 10 0.044 0.009 1.610 100 0.445 0.092 16.099 1000 4.446 0.920 160.986 5000 22.23 4.600 804.900 Total Monthly Cost (OSI as processor) = (OSI Cost + S3 Storage Cost) x 30 days Daily Log Volume Daily OSI Cost (USD) Daily S3 Storage Cost (USD) Monthly Cost (USD) 10 5.760 0.001 173.1 100 5.760 0.009 175.5 1000 11.52 0.920 373.2 5000 34.56 4.600 1174.8 For Amazon RDS/Aurora logs and AWS Lambda Logs that deliver to CloudWatch Logs, apart from the S3 and Lambda costs listed above, there is additional cost of using Kinesis Data Firehose (KDF) to subscribe to the CloudWatch Logs Stream and put them into an Amazon S3 bucket, and KDF is charging for a 5KB increments (less than 5KB per record is billed as 5KB). Assuming Log size is 0.2 KB per record, then the daily KDF cost is estimated as below: Kinesis Data Firehose Cost = $0.029 per GB x N GB/day x (5KB/0.2 KB) For example, for 1GB logs per day, the extra monthly cost of KDF is $21.75. Important If you want to save cost charged by Kinesis Data Firehose, make sure you activate logs only when needed. For example, you can choose not to activate RDS general logs unless required. Logs ingestion through Amazon Kinesis Data Streams This section is applicable to: AWS Services Logs including CloudFront real-time logs, CloudTrail logs (CloudWatch), and VPC Flow logs (CloudWatch). Application Logs that use Amazon KDS as data buffer Important The cost estimation does not include the logging cost of service. For example, CloudFront real-time logs are charged based on the number of log lines generated ($0.01 for every 1,000,000 log lines). There are also logs delivery to CloudWatch charges for CloudTrail and VPC Flow logs that enabled CloudWatch Logging. Please check the service pricing for more details. The cost estimation is based on the following assumptions and facts: The average log message size is 1 KB. The daily log volume is L GB. The Lambda processor memory is 1024 MB. Every Lambda invocation processes 1 MB logs. One Lambda invocation processes one shard of Kinesis, and Lambda can scale up to more concurrent invocations to process multiple shards. The Lambda runtime to process log less than 5 MB is 500ms. 30% additional shards are provided to handle traffic jitter. One Kinesis shard intake log size is = 1 MB /second x 3600 seconds per hour x 24 hours x 0.7 = 60.48 GB/day. The desired Kinesis Shard number S is = Round_up_to_next_integer(Daily log volume L / 60.48). Based on the above assumptions, here is the daily cost estimation formula: Kinesis Shard Hour Cost = $0.015 / shard hour x 24 hours per day x S shards Kinesis PUT Payload Unit Cost = $0.014 per million units x 1 millions per GB x L GB per day Lambda Cost = $0.0000000167 per 1ms x 500 ms per invocation x 1,000 invocations per GB x L GB per day Total Monthly Cost = (Kinesis Shard Hour Cost + Kinesis PUT Payload Unit Cost + Lambda Cost) x 30 days Daily Log Volume (GB) Shards Daily Kinesis Shard Hour Cost (USD) Daily Kinesis PUT Payload Unit Cost (USD) Daily Lambda Cost (USD) Monthly Cost (USD) 10 1 0.36 0.14 0.0835 17.505 100 2 0.72 1.4 0.835 88.65 1000 17 6.12 14 8.35 854.1 Use Light Engine as log process engine Sample1 -- Raw log size: 10GB/day and Query size: 50GB/day Services Monthly cost (USD) Amazon S3 $1.49 Amazon Lambda $0.37 Amazon SQS $0.00 Amazon DynamoDB $3.79 Amazon Step Function $8.07 Amazon SNS $0.18 Amazon Athena $7.25 Amazon EC2* $29.20 Total $50.35 Sample2 --Raw log size: 100GB/day and Query size: 300GB/day Services Monthly cost (USD) Amazon S3 $19.98.00 Amazon Lambda $0.73 Amazon SQS $0.00 Amazon DynamoDB $3.79 Amazon Step Function $16.14 Amazon SNS $0.18 Amazon Athena $43.51 Amazon EC2* $29.20 Total $113.53 Sample3 --Raw log size: 1TB/day and Query size: 1TB/day Services Monthly cost (USD) Amazon S3 $148.99 Amazon Lambda $1.10 Amazon SQS $0.00 Amazon DynamoDB $3.79 Amazon Step Function $26.90 Amazon SNS $0.18 Amazon Athena $148.54 Amazon EC2* $29.20 Total $358.70 Solution Console Cost A web console is created automatically when you deploy the solution. Assume the visits to the console are 3,000 times in a month (30 days), it will incur the following cost: Note AWS Step Functions, Amazon CloudWatch, AWS Systems Manager, and Amazon EventBridge are all within free-tier. Service Monthly Cost (USD) Amazon CloudFront (1GB Data Transfer Out to Internet and 1GB Data Transfer Out to Origin) 0.25 Amazon S3 0.027 Amazon Cognito 0.05 AWS AppSync 0.01 Amazon DynamoDB 1.00 AWS Lambda 0.132 Total 1.469 Additional Features Cost Note You will not be charged if you do not use the additional features in the Centralized Logging with OpenSearch console. Access Proxy If you deploy the Access Proxy through Centralized Logging with OpenSearch, additional charges will apply. The total cost varies depending on the instance type and number of instances. As of this revision, the following are two examples for the cost estimation in the US East (N. Virginia) Region. Example 1: Instance Type - t3.nano, Instance Number - 2 - EC2 cost = t3.nano 1Y RI All Upfront price $26.28 x 2 / 12 months = $4.38/month - EBS Cost = EBS $0.1 GB/month x 8 GB x 2 = $1.6/month (The EBS attached to the EC2 instance is 8 GB) - Elastic Load Balancer Cost = $0.0225 per ALB-hour x 720 hours/month = $16.2/month Total Monthly Cost = $4.38 EC2 Cost + $1.6 EBS Cost + $16.2 Elastic Load Balancer Cost = $22.18 Example 2: Instance Type - t3.large, Instance Number - 2 - EC2 Cost = t3.large 1Y RI All Upfront $426.612 x 2 / 12 months = $71.1/month - EBS Cost = $0.1 GB/month x 8 GB x 2 = $1.6/month (The EBS attached to the EC2 instance is 8 GB) - Elastic Load Balancer Cost = $0.0225 per ALB-hour x 720 hours/month = $16.2/month Total Monthly Cost = $71.1 EC2 Cost + $1.6 EBS Cost + $16.2 Elastic Load Balancer Cost = $88.9 Amazon OpenSearch Alarms If you deploy the alarms through Centralized Logging with OpenSearch, the Amazon CloudWatch Pricing will apply. Pipeline Alarms Log Type Alarm Count Number of Standard Resolution Alarm Metrics Monthly Cost per Ingestion per Pipeline AWS Service logs 4 0.1 USD 0.4 USD Application logs 5 0.1 USD 0.5 USD Pipeline Monitoring Log processor Assumptions: Deployment in the US East (N. Virginia) Region (us-east-1) A processor Lambda will be triggered every 60 seconds. The monthly metric put request number is 60 (requests) x 24 (hours) x 30 (days) = 43,200 PutMetricData: 43,200 requests x 0.00001 USD = 0.432 USD There are 4 metrics for Service Logs ( total logs, failed logs, loaded logs, excluded logs ) and 3 metrics ( total logs, failed logs, loaded logs ) for Application logs Amazon CloudWatch Logs API = PutMetricData x Number of Metrics Amazon CloudWatch Logs Metric = Number of Metrics x 0.3 Log Type Monthly Metric Put Request Number Number of Metrics Amazon CloudWatch Logs API Amazon CloudWatch Logs Metric Monthly Cost Per Source/Per Pipeline AWS Service logs 43,200 4 1.728 USD 1.2 USD 2.928 USD Application logs 43,200 3 1.296 USD 0.9 USD 2.196 USD Fluent Bit Assumptions: Deployment in the US East (N. Virginia) Region (us-east-1) There are 7 metrics: FluentBitOutputProcRecords , FluentBitOutputProcBytes , FluentBitOutputDroppedRecords , FluentBitOutputErrors , FluentBitOutputRetriedRecords , FluentBitOutputRetriesFailed , FluentBitOutputRetries . For more information, refer to the Monitoring section. Number of Metrics requested: an interval of 60 seconds to put logs from Fluent Bit to Amazon CloudWatch (60 requests in an hour). Monthly put requests are 60 (requests) x 24 (hours) x 30 (days) = 43,200 PutMetricData: 43,200 requests x 0.00001 USD = 0.432 USD CloudWatch Logs API = PutMetricData x Number of Metrics x Number of Instances CloudWatch Logs Metric = Number of Metrics x 0.3 Number of EC2 Instances / EKS Nodes Amazon CloudWatch Logs API Amazon CloudWatch Logs Log Storage & Ingested (Calculated by AWS Pricing Calculator ) Amazon CloudWatch Logs Metric Monthly Cost Per Source/Per Pipeline 1 3.024 USD 0.04 USD 2.1 USD 5.164 USD 10 30.24 USD 0.35 USD 2.1 USD 32.69 USD 100 302.4 USD 3.53 USD 2.1 USD 308.03 USD","title":"Cost"},{"location":"implementation-guide/plan-deployment/cost/#cost-estimation","text":"Important The following cost estimations are examples and may vary depending on your environment. You will be responsible for the cost of the AWS services used when running the solution. The main factors affecting the solution cost include: Type of logs to be ingested Volume of logs to be ingested/processed Size of the log message Location of logs Additional features As of this revision, the following examples demonstrate the cost estimation of 10/100/1000 GB daily log ingestion for running this solution with default settings in the US East (N. Virginia) Region. The total cost is composed of Amazon OpenSearch Cost Or Light Engine Cost , Solution Console Cost and Additional Features Cost .","title":"Cost Estimation"},{"location":"implementation-guide/plan-deployment/cost/#use-opensearch-as-log-process-engine","text":"","title":"Use OpenSearch as log process engine"},{"location":"implementation-guide/plan-deployment/cost/#amazon-opensearch-cost","text":"OD : On Demand AURI_1 : All Upfront Reserved Instance 1 Year Tiering : The days stored in each tier. For example, 7H + 23W + 60C indicates that the log is stored in hot tier for 7 days, warm tier for 23 days, and cold tier for 60 days. Replica : The number of shard replicas. Daily log Volume (GB) Retention (days) Tiering Replica OD Monthly (USD) AURI_1 Monthly (USD) Dedicated Master Data Node EBS (GB) UltraWarm Nodes UltraWarm/Cold S3 Storage (GB) OD cost per GB (USD) AURI_1 cost per GB ($) 10 30 30H 0 216.28 158.54 N/A c6g.large[2] 380 N/A 0 0.72093 0.52847 10 30 30H 1 289.35 223.94 N/A m6g.large[2] 760 N/A 0 0.9645 0.74647 100 30 7H + 23W 0 989.49 825.97 m6g.large[3] m6g.large[2] 886 medium[2] 0 0.32983 0.27532 100 30 7H + 23W 1 1295.85 1066.92 m6g.large[3] m6g.large[4] 1772 medium[2] 0 0.43195 0.35564 100 90 7H + 23W + 60C 0 1133.49 969.97 m6g.large[3] m6g.large[2] 886 medium[2] 8300 0.12594 0.10777 100 90 7H + 23W + 60C 1 1439.85 1210.92 m6g.large[3] m6g.large[4] 1772 medium[2] 8300 0.15998 0.13455 100 180 7H + 23W + 150C 0 1349.49 1185.97 m6g.large[3] m6g.large[2] 886 medium[2] 17300 0.07497 0.06589 100 180 7H + 23W + 150C 1 1655.85 1426.92 m6g.large[3] m6g.large[4] 1772 medium[2] 17300 0.09199 0.07927 1000 30 7H + 23W 0 6101.15 5489.48 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 23000 0.20337 0.18298 1000 30 7H + 23W 1 8759.49 7635.8 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 23000 0.29198 0.25453 1000 90 7H + 23W + 60C 0 8027.33 7245.45 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 83000 0.08919 0.0805 1000 90 7H + 23W + 60C 1 10199.49 9075.8 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 83000 0.11333 0.10084 1000 180 7H + 23W + 150C 0 9701.15 9089.48 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 173000 0.0539 0.0505 1000 180 7H + 23W + 150C 1 12644.19 11420.86 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 173000 0.07025 0.06345","title":"Amazon OpenSearch Cost"},{"location":"implementation-guide/plan-deployment/cost/#processing-cost","text":"","title":"Processing Cost"},{"location":"implementation-guide/plan-deployment/cost/#log-ingestion-through-amazon-s3","text":"This section is applicable to: AWS service logs including Amazon S3 access logs, CloudFront standard logs, CloudTrail logs (S3), Application Load Balancing access logs, WAF logs, VPC Flow logs (S3), AWS Config logs, Amazon RDS/Aurora logs, and AWS Lambda Logs. Application Logs that use Amazon S3 as data buffer. Assumptions: The logs stored in Amazon S3 are in gzip format. A 4MB compressed log file in S3 is roughly 100 MB in raw log size. A Lambda with 1 GB memory takes about 26 seconds to process a 4 MB compressed log file, namely 260 milliseconds (ms) per MB raw logs. The maximum compressed log file size is 5 MB. Ingesting logs from S3 will incur SQS and S3 request fees which are very low, or usually within the free tier. You have N GB raw log per day, and the daily cost estimation is as follows: When you use Lambda as log processor: Lambda Cost = 260 ms per MB x 1024 MB x N GB/day x $0.0000000167 per ms S3 Storage Cost = $0.023 per GB x N GB/day x 4% (compression) When you use OSI as log processor: OSI Pipeline Cost = $0.24 per OCU per hour The maximum amount of S3 data 1 OCU can handle is around 20MB/s The total monthly cost for ingesting AWS service logs is: Total Monthly Cost (Lambda as processor) = (Lambda Cost + S3 Storage Cost) x 30 days Daily Log Volume Daily Lambda Cost (USD) Daily S3 Storage Cost (USD) Monthly Cost (USD) 10 0.044 0.009 1.610 100 0.445 0.092 16.099 1000 4.446 0.920 160.986 5000 22.23 4.600 804.900 Total Monthly Cost (OSI as processor) = (OSI Cost + S3 Storage Cost) x 30 days Daily Log Volume Daily OSI Cost (USD) Daily S3 Storage Cost (USD) Monthly Cost (USD) 10 5.760 0.001 173.1 100 5.760 0.009 175.5 1000 11.52 0.920 373.2 5000 34.56 4.600 1174.8 For Amazon RDS/Aurora logs and AWS Lambda Logs that deliver to CloudWatch Logs, apart from the S3 and Lambda costs listed above, there is additional cost of using Kinesis Data Firehose (KDF) to subscribe to the CloudWatch Logs Stream and put them into an Amazon S3 bucket, and KDF is charging for a 5KB increments (less than 5KB per record is billed as 5KB). Assuming Log size is 0.2 KB per record, then the daily KDF cost is estimated as below: Kinesis Data Firehose Cost = $0.029 per GB x N GB/day x (5KB/0.2 KB) For example, for 1GB logs per day, the extra monthly cost of KDF is $21.75. Important If you want to save cost charged by Kinesis Data Firehose, make sure you activate logs only when needed. For example, you can choose not to activate RDS general logs unless required.","title":"Log ingestion through Amazon S3"},{"location":"implementation-guide/plan-deployment/cost/#logs-ingestion-through-amazon-kinesis-data-streams","text":"This section is applicable to: AWS Services Logs including CloudFront real-time logs, CloudTrail logs (CloudWatch), and VPC Flow logs (CloudWatch). Application Logs that use Amazon KDS as data buffer Important The cost estimation does not include the logging cost of service. For example, CloudFront real-time logs are charged based on the number of log lines generated ($0.01 for every 1,000,000 log lines). There are also logs delivery to CloudWatch charges for CloudTrail and VPC Flow logs that enabled CloudWatch Logging. Please check the service pricing for more details. The cost estimation is based on the following assumptions and facts: The average log message size is 1 KB. The daily log volume is L GB. The Lambda processor memory is 1024 MB. Every Lambda invocation processes 1 MB logs. One Lambda invocation processes one shard of Kinesis, and Lambda can scale up to more concurrent invocations to process multiple shards. The Lambda runtime to process log less than 5 MB is 500ms. 30% additional shards are provided to handle traffic jitter. One Kinesis shard intake log size is = 1 MB /second x 3600 seconds per hour x 24 hours x 0.7 = 60.48 GB/day. The desired Kinesis Shard number S is = Round_up_to_next_integer(Daily log volume L / 60.48). Based on the above assumptions, here is the daily cost estimation formula: Kinesis Shard Hour Cost = $0.015 / shard hour x 24 hours per day x S shards Kinesis PUT Payload Unit Cost = $0.014 per million units x 1 millions per GB x L GB per day Lambda Cost = $0.0000000167 per 1ms x 500 ms per invocation x 1,000 invocations per GB x L GB per day Total Monthly Cost = (Kinesis Shard Hour Cost + Kinesis PUT Payload Unit Cost + Lambda Cost) x 30 days Daily Log Volume (GB) Shards Daily Kinesis Shard Hour Cost (USD) Daily Kinesis PUT Payload Unit Cost (USD) Daily Lambda Cost (USD) Monthly Cost (USD) 10 1 0.36 0.14 0.0835 17.505 100 2 0.72 1.4 0.835 88.65 1000 17 6.12 14 8.35 854.1","title":"Logs ingestion through Amazon Kinesis Data Streams"},{"location":"implementation-guide/plan-deployment/cost/#use-light-engine-as-log-process-engine","text":"Sample1 -- Raw log size: 10GB/day and Query size: 50GB/day Services Monthly cost (USD) Amazon S3 $1.49 Amazon Lambda $0.37 Amazon SQS $0.00 Amazon DynamoDB $3.79 Amazon Step Function $8.07 Amazon SNS $0.18 Amazon Athena $7.25 Amazon EC2* $29.20 Total $50.35 Sample2 --Raw log size: 100GB/day and Query size: 300GB/day Services Monthly cost (USD) Amazon S3 $19.98.00 Amazon Lambda $0.73 Amazon SQS $0.00 Amazon DynamoDB $3.79 Amazon Step Function $16.14 Amazon SNS $0.18 Amazon Athena $43.51 Amazon EC2* $29.20 Total $113.53 Sample3 --Raw log size: 1TB/day and Query size: 1TB/day Services Monthly cost (USD) Amazon S3 $148.99 Amazon Lambda $1.10 Amazon SQS $0.00 Amazon DynamoDB $3.79 Amazon Step Function $26.90 Amazon SNS $0.18 Amazon Athena $148.54 Amazon EC2* $29.20 Total $358.70","title":"Use Light Engine as log process engine"},{"location":"implementation-guide/plan-deployment/cost/#solution-console-cost","text":"A web console is created automatically when you deploy the solution. Assume the visits to the console are 3,000 times in a month (30 days), it will incur the following cost: Note AWS Step Functions, Amazon CloudWatch, AWS Systems Manager, and Amazon EventBridge are all within free-tier. Service Monthly Cost (USD) Amazon CloudFront (1GB Data Transfer Out to Internet and 1GB Data Transfer Out to Origin) 0.25 Amazon S3 0.027 Amazon Cognito 0.05 AWS AppSync 0.01 Amazon DynamoDB 1.00 AWS Lambda 0.132 Total 1.469","title":"Solution Console Cost"},{"location":"implementation-guide/plan-deployment/cost/#additional-features-cost","text":"Note You will not be charged if you do not use the additional features in the Centralized Logging with OpenSearch console.","title":"Additional Features Cost"},{"location":"implementation-guide/plan-deployment/cost/#access-proxy","text":"If you deploy the Access Proxy through Centralized Logging with OpenSearch, additional charges will apply. The total cost varies depending on the instance type and number of instances. As of this revision, the following are two examples for the cost estimation in the US East (N. Virginia) Region. Example 1: Instance Type - t3.nano, Instance Number - 2 - EC2 cost = t3.nano 1Y RI All Upfront price $26.28 x 2 / 12 months = $4.38/month - EBS Cost = EBS $0.1 GB/month x 8 GB x 2 = $1.6/month (The EBS attached to the EC2 instance is 8 GB) - Elastic Load Balancer Cost = $0.0225 per ALB-hour x 720 hours/month = $16.2/month Total Monthly Cost = $4.38 EC2 Cost + $1.6 EBS Cost + $16.2 Elastic Load Balancer Cost = $22.18 Example 2: Instance Type - t3.large, Instance Number - 2 - EC2 Cost = t3.large 1Y RI All Upfront $426.612 x 2 / 12 months = $71.1/month - EBS Cost = $0.1 GB/month x 8 GB x 2 = $1.6/month (The EBS attached to the EC2 instance is 8 GB) - Elastic Load Balancer Cost = $0.0225 per ALB-hour x 720 hours/month = $16.2/month Total Monthly Cost = $71.1 EC2 Cost + $1.6 EBS Cost + $16.2 Elastic Load Balancer Cost = $88.9","title":"Access Proxy"},{"location":"implementation-guide/plan-deployment/cost/#amazon-opensearch-alarms","text":"If you deploy the alarms through Centralized Logging with OpenSearch, the Amazon CloudWatch Pricing will apply.","title":"Amazon OpenSearch Alarms"},{"location":"implementation-guide/plan-deployment/cost/#pipeline-alarms","text":"Log Type Alarm Count Number of Standard Resolution Alarm Metrics Monthly Cost per Ingestion per Pipeline AWS Service logs 4 0.1 USD 0.4 USD Application logs 5 0.1 USD 0.5 USD","title":"Pipeline Alarms"},{"location":"implementation-guide/plan-deployment/cost/#pipeline-monitoring","text":"Log processor Assumptions: Deployment in the US East (N. Virginia) Region (us-east-1) A processor Lambda will be triggered every 60 seconds. The monthly metric put request number is 60 (requests) x 24 (hours) x 30 (days) = 43,200 PutMetricData: 43,200 requests x 0.00001 USD = 0.432 USD There are 4 metrics for Service Logs ( total logs, failed logs, loaded logs, excluded logs ) and 3 metrics ( total logs, failed logs, loaded logs ) for Application logs Amazon CloudWatch Logs API = PutMetricData x Number of Metrics Amazon CloudWatch Logs Metric = Number of Metrics x 0.3 Log Type Monthly Metric Put Request Number Number of Metrics Amazon CloudWatch Logs API Amazon CloudWatch Logs Metric Monthly Cost Per Source/Per Pipeline AWS Service logs 43,200 4 1.728 USD 1.2 USD 2.928 USD Application logs 43,200 3 1.296 USD 0.9 USD 2.196 USD Fluent Bit Assumptions: Deployment in the US East (N. Virginia) Region (us-east-1) There are 7 metrics: FluentBitOutputProcRecords , FluentBitOutputProcBytes , FluentBitOutputDroppedRecords , FluentBitOutputErrors , FluentBitOutputRetriedRecords , FluentBitOutputRetriesFailed , FluentBitOutputRetries . For more information, refer to the Monitoring section. Number of Metrics requested: an interval of 60 seconds to put logs from Fluent Bit to Amazon CloudWatch (60 requests in an hour). Monthly put requests are 60 (requests) x 24 (hours) x 30 (days) = 43,200 PutMetricData: 43,200 requests x 0.00001 USD = 0.432 USD CloudWatch Logs API = PutMetricData x Number of Metrics x Number of Instances CloudWatch Logs Metric = Number of Metrics x 0.3 Number of EC2 Instances / EKS Nodes Amazon CloudWatch Logs API Amazon CloudWatch Logs Log Storage & Ingested (Calculated by AWS Pricing Calculator ) Amazon CloudWatch Logs Metric Monthly Cost Per Source/Per Pipeline 1 3.024 USD 0.04 USD 2.1 USD 5.164 USD 10 30.24 USD 0.35 USD 2.1 USD 32.69 USD 100 302.4 USD 3.53 USD 2.1 USD 308.03 USD","title":"Pipeline Monitoring"},{"location":"implementation-guide/plan-deployment/security/","text":"Security When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared responsibility model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, see AWS Cloud Security . IAM Roles AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions, AWS AppSync and Amazon Cognito access to create regional resources. Security Groups The security groups created in this solution are designed to control and isolate network traffic between the solution components. We recommend that you review the security groups and further restrict access as needed once the deployment is up and running. Amazon CloudFront This solution deploys a web console hosted in an Amazon Simple Storage Service (Amazon S3) bucket. To help reduce latency and improve security, this solution includes an Amazon CloudFront distribution with an origin access identity, which is a CloudFront user that provides public access to the solution\u2019s website bucket contents. For more information, refer to Restricting Access to Amazon S3 Content by Using an Origin Access Identity in the Amazon CloudFront Developer Guide. Amazon EC2 This solution creates a Nginx based proxy , which will allow you to access the OpenSearch provisioned within VPC environment. The Nginx is hosted using EC2 instances. We recommend you to use AWS Systems Manager Patch Manager to patch the instances periodically. Patch Manager is a capability of AWS Systems Manager that automates the process of patching managed nodes with updates. You can choose to show only a report of missing patches (a Scan operation), or to automatically install all patches which are missing (a Scan and install operation).","title":"Security"},{"location":"implementation-guide/plan-deployment/security/#security","text":"When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared responsibility model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, see AWS Cloud Security .","title":"Security"},{"location":"implementation-guide/plan-deployment/security/#iam-roles","text":"AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions, AWS AppSync and Amazon Cognito access to create regional resources.","title":"IAM Roles"},{"location":"implementation-guide/plan-deployment/security/#security-groups","text":"The security groups created in this solution are designed to control and isolate network traffic between the solution components. We recommend that you review the security groups and further restrict access as needed once the deployment is up and running.","title":"Security Groups"},{"location":"implementation-guide/plan-deployment/security/#amazon-cloudfront","text":"This solution deploys a web console hosted in an Amazon Simple Storage Service (Amazon S3) bucket. To help reduce latency and improve security, this solution includes an Amazon CloudFront distribution with an origin access identity, which is a CloudFront user that provides public access to the solution\u2019s website bucket contents. For more information, refer to Restricting Access to Amazon S3 Content by Using an Origin Access Identity in the Amazon CloudFront Developer Guide.","title":"Amazon CloudFront"},{"location":"implementation-guide/plan-deployment/security/#amazon-ec2","text":"This solution creates a Nginx based proxy , which will allow you to access the OpenSearch provisioned within VPC environment. The Nginx is hosted using EC2 instances. We recommend you to use AWS Systems Manager Patch Manager to patch the instances periodically. Patch Manager is a capability of AWS Systems Manager that automates the process of patching managed nodes with updates. You can choose to show only a report of missing patches (a Scan operation), or to automatically install all patches which are missing (a Scan and install operation).","title":"Amazon EC2"},{"location":"implementation-guide/resources/grafana/","text":"Setting up Grafana Environment - Optional This section introduces how to set up Grafana environment. If you want the solution to generate dashboards in Grafana automatically, you need to perform the following deployment. If you only want to store the data in Amazon S3 without creating dashboards, you can skip this section. Step 1: Install Grafana Note Skip this step if you already have a Grafana environment. Prerequisites: An EC2 instance has been launched, supporting both x86 and ARM architectures. The following steps provide an example using m6g.medium instance type, ARM architecture, and Amazon 2023. For details, refer to Install Grafana . Follow below steps: # Edit/etc/yum.repos.d/grafana.repo file\uff0cinput below content [ grafana ] name = grafana baseurl = https://rpm.grafana.com repo_gpgcheck = 1 enabled = 1 gpgcheck = 1 gpgkey = https://rpm.grafana.com/gpg.key sslverify = 1 sslcacert = /etc/pki/tls/certs/ca-bundle.crt # install grafana yum install -y grafana # Start grafana\uff0cand check its running status systemctl start grafana-server systemctl status grafana-server # grafana listens on port 3000 by default, Users can edit /etc/grafana/grafana.ini to modify the configuration # Access grafana\uff0cusing the default credentials admin / admin\uff0cyou will be promoted to change the password on the first login. http:// { instance-ip } :3000/ # If you need public access, please configure an Application Load Balancer (ALB) on your own. # When configuring the ALB, modify the Idle timeout to 1800 to avoid the following error during large data queries (when a single API call exceeds 60 seconds)\uff1a # \u201ca padding to disable MSIE and Chrome friendly error page\u201d Step 2: Authorize the EC2 where Grafana is located to access Athena Prerequisites: You have deployed Grafana on EC2. EC2 has been configured with an IAM Instance Profile. The corresponding Role Arn of the Instance Profile should be recorded. All the following content will be referred to as \"EC2 IAM Instance Profile.\" Follow below steps: Access IAM ManagementConsole . Search for the role including \"AthenaPublicAccessRole\" and click on it to access the details page. Record the Role Arn, which will be used later. Click on \"Trust relationships.\" Click on \"Edit trust policy.\" Click on \"Add a principal.\" Select \"IAM Roles.\" Enter the \"EC2 IAM Instance Profile.\" Click on \"Add principal.\" Click on \"Update Policy.\" Step 3: Install Amazon Athena Plugins Prerequisites: Grafana is installed. Grafana is accessible over the public network. Follow below steps: Open the Grafana console page. Select \"Administration\" from the left menu bar, then choose \"Plugins.\" On the right side, select \"All\" in the \"State\" section. In the search box, enter \"Athena\" and click on the \"Amazon Athena\" result to access the details page. Click the \"Install\" button on the page and wait for the plugin installation to complete. Step 4: Create Service Accounts Follow below steps: Open the Grafana console page. Select \"Administration\" from the left menu bar, then choose \"Service accounts.\" Select \"Add service account.\" Enter the Display name, for example, \"test\". Select the Role as \"Admin.\" Click \"Create.\" Click \"Add service account token\" on the page. Click \"Generate token.\" Click \"Copy to clipboard and close.\" Save and record this token, which will be used when you need to create a pipeline.","title":"Grafana"},{"location":"implementation-guide/resources/grafana/#setting-up-grafana-environment-optional","text":"This section introduces how to set up Grafana environment. If you want the solution to generate dashboards in Grafana automatically, you need to perform the following deployment. If you only want to store the data in Amazon S3 without creating dashboards, you can skip this section.","title":"Setting up Grafana Environment - Optional"},{"location":"implementation-guide/resources/grafana/#step-1-install-grafana","text":"Note Skip this step if you already have a Grafana environment. Prerequisites: An EC2 instance has been launched, supporting both x86 and ARM architectures. The following steps provide an example using m6g.medium instance type, ARM architecture, and Amazon 2023. For details, refer to Install Grafana . Follow below steps: # Edit/etc/yum.repos.d/grafana.repo file\uff0cinput below content [ grafana ] name = grafana baseurl = https://rpm.grafana.com repo_gpgcheck = 1 enabled = 1 gpgcheck = 1 gpgkey = https://rpm.grafana.com/gpg.key sslverify = 1 sslcacert = /etc/pki/tls/certs/ca-bundle.crt # install grafana yum install -y grafana # Start grafana\uff0cand check its running status systemctl start grafana-server systemctl status grafana-server # grafana listens on port 3000 by default, Users can edit /etc/grafana/grafana.ini to modify the configuration # Access grafana\uff0cusing the default credentials admin / admin\uff0cyou will be promoted to change the password on the first login. http:// { instance-ip } :3000/ # If you need public access, please configure an Application Load Balancer (ALB) on your own. # When configuring the ALB, modify the Idle timeout to 1800 to avoid the following error during large data queries (when a single API call exceeds 60 seconds)\uff1a # \u201ca padding to disable MSIE and Chrome friendly error page\u201d","title":"Step 1: Install Grafana"},{"location":"implementation-guide/resources/grafana/#step-2-authorize-the-ec2-where-grafana-is-located-to-access-athena","text":"Prerequisites: You have deployed Grafana on EC2. EC2 has been configured with an IAM Instance Profile. The corresponding Role Arn of the Instance Profile should be recorded. All the following content will be referred to as \"EC2 IAM Instance Profile.\" Follow below steps: Access IAM ManagementConsole . Search for the role including \"AthenaPublicAccessRole\" and click on it to access the details page. Record the Role Arn, which will be used later. Click on \"Trust relationships.\" Click on \"Edit trust policy.\" Click on \"Add a principal.\" Select \"IAM Roles.\" Enter the \"EC2 IAM Instance Profile.\" Click on \"Add principal.\" Click on \"Update Policy.\"","title":"Step 2: Authorize the EC2 where Grafana is located to access Athena"},{"location":"implementation-guide/resources/grafana/#step-3-install-amazon-athena-plugins","text":"Prerequisites: Grafana is installed. Grafana is accessible over the public network. Follow below steps: Open the Grafana console page. Select \"Administration\" from the left menu bar, then choose \"Plugins.\" On the right side, select \"All\" in the \"State\" section. In the search box, enter \"Athena\" and click on the \"Amazon Athena\" result to access the details page. Click the \"Install\" button on the page and wait for the plugin installation to complete.","title":"Step 3: Install Amazon Athena Plugins"},{"location":"implementation-guide/resources/grafana/#step-4-create-service-accounts","text":"Follow below steps: Open the Grafana console page. Select \"Administration\" from the left menu bar, then choose \"Service accounts.\" Select \"Add service account.\" Enter the Display name, for example, \"test\". Select the Role as \"Admin.\" Click \"Create.\" Click \"Add service account token\" on the page. Click \"Generate token.\" Click \"Copy to clipboard and close.\" Save and record this token, which will be used when you need to create a pipeline.","title":"Step 4: Create Service Accounts"},{"location":"implementation-guide/resources/open-ssl/","text":"OpenSSL 1.1 Installation Centralized Logging with OpenSearch uses Fluent Bit as the log agent, which requires OpenSSL 1.1 or later. You can install the dependency according to your operating system (OS). It is recommended to make your own AMI with OpenSSL 1.1 installed. Important If your OS is not listed below, you can follow the official installation guide to install OpenSSL. Amazon Linux 2 sudo yum install openssl11 Ubuntu 22.04 ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3 ln -s /snap/core18/current/usr/lib/x86_64-linux-gnu/libssl.so.1.1 /usr/lib/libssl.so.1.1 ln -s /snap/core18/current/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 /usr/lib/libcrypto.so.1.1 20.04 ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3 18.04 ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3 Debian GNU/10 ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3 GNU/11 ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3 Red Hat Enterprise Linux 8.X OpenSSL 1.1 is installed by default. 7.X sudo su - yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm systemctl enable amazon-ssm-agent systemctl start amazon-ssm-agent yum install -y wget perl unzip gcc zlib-devel mkdir /tmp/openssl cd /tmp/openssl wget https://www.openssl.org/source/openssl-1.1.1s.tar.gz tar xzvf openssl-1.1.1s.tar.gz cd openssl-1.1.1s ./config --prefix = /usr/local/openssl11 --openssldir = /usr/local/openssl11 shared zlib make make install echo /usr/local/openssl11/lib/ >> /etc/ld.so.conf ldconfig SUSE Linux Enterprise Server 15 OpenSSL 1.1 is installed by default.","title":"OpenSSL installation"},{"location":"implementation-guide/resources/open-ssl/#openssl-11-installation","text":"Centralized Logging with OpenSearch uses Fluent Bit as the log agent, which requires OpenSSL 1.1 or later. You can install the dependency according to your operating system (OS). It is recommended to make your own AMI with OpenSSL 1.1 installed. Important If your OS is not listed below, you can follow the official installation guide to install OpenSSL.","title":"OpenSSL 1.1 Installation"},{"location":"implementation-guide/resources/open-ssl/#amazon-linux-2","text":"sudo yum install openssl11","title":"Amazon Linux 2"},{"location":"implementation-guide/resources/open-ssl/#ubuntu","text":"","title":"Ubuntu"},{"location":"implementation-guide/resources/open-ssl/#2204","text":"ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3 ln -s /snap/core18/current/usr/lib/x86_64-linux-gnu/libssl.so.1.1 /usr/lib/libssl.so.1.1 ln -s /snap/core18/current/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 /usr/lib/libcrypto.so.1.1","title":"22.04"},{"location":"implementation-guide/resources/open-ssl/#2004","text":"ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3","title":"20.04"},{"location":"implementation-guide/resources/open-ssl/#1804","text":"ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3","title":"18.04"},{"location":"implementation-guide/resources/open-ssl/#debian","text":"","title":"Debian"},{"location":"implementation-guide/resources/open-ssl/#gnu10","text":"ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3","title":"GNU/10"},{"location":"implementation-guide/resources/open-ssl/#gnu11","text":"ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3","title":"GNU/11"},{"location":"implementation-guide/resources/open-ssl/#red-hat-enterprise-linux","text":"","title":"Red Hat Enterprise Linux"},{"location":"implementation-guide/resources/open-ssl/#8x","text":"OpenSSL 1.1 is installed by default.","title":"8.X"},{"location":"implementation-guide/resources/open-ssl/#7x","text":"sudo su - yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm systemctl enable amazon-ssm-agent systemctl start amazon-ssm-agent yum install -y wget perl unzip gcc zlib-devel mkdir /tmp/openssl cd /tmp/openssl wget https://www.openssl.org/source/openssl-1.1.1s.tar.gz tar xzvf openssl-1.1.1s.tar.gz cd openssl-1.1.1s ./config --prefix = /usr/local/openssl11 --openssldir = /usr/local/openssl11 shared zlib make make install echo /usr/local/openssl11/lib/ >> /etc/ld.so.conf ldconfig","title":"7.X"},{"location":"implementation-guide/resources/open-ssl/#suse-linux-enterprise-server","text":"","title":"SUSE Linux Enterprise Server"},{"location":"implementation-guide/resources/open-ssl/#15","text":"OpenSSL 1.1 is installed by default.","title":"15"},{"location":"implementation-guide/resources/upload-ssl-certificate/","text":"Upload SSL Certificate to IAM Upload the SSL certificate by running the AWS CLI command upload-server-certificate similar to the following: aws iam upload-server-certificate --path /cloudfront/ \\ --server-certificate-name YourCertificate \\ --certificate-body file://Certificate.pem \\ --certificate-chain file://CertificateChain.pem \\ --private-key file://PrivateKey.pem Replace the file names and Your Certificate with the names for your uploaded files and certificate. You must specify the file:// prefix in the certificate-body, certificate-chain and private-key parameters in the API request. Otherwise, the request fails with a MalformedCertificate: Unknown error message. Note You must specify a path using the --path option. The path must begin with /cloudfront and must include a trailing slash (for example, /cloudfront/test/). After the certificate is uploaded, the AWS command upload-server-certificate returns metadata for the uploaded certificate, including the certificate's Amazon Resource Name (ARN), friendly name, identifier (ID), and expiration date. To view the uploaded certificate, run the AWS CLI command list-server-certificates : aws iam list-server-certificates For more information, see uploading a server certificate to IAM.","title":"Upload certificate"},{"location":"implementation-guide/resources/upload-ssl-certificate/#upload-ssl-certificate-to-iam","text":"Upload the SSL certificate by running the AWS CLI command upload-server-certificate similar to the following: aws iam upload-server-certificate --path /cloudfront/ \\ --server-certificate-name YourCertificate \\ --certificate-body file://Certificate.pem \\ --certificate-chain file://CertificateChain.pem \\ --private-key file://PrivateKey.pem Replace the file names and Your Certificate with the names for your uploaded files and certificate. You must specify the file:// prefix in the certificate-body, certificate-chain and private-key parameters in the API request. Otherwise, the request fails with a MalformedCertificate: Unknown error message. Note You must specify a path using the --path option. The path must begin with /cloudfront and must include a trailing slash (for example, /cloudfront/test/). After the certificate is uploaded, the AWS command upload-server-certificate returns metadata for the uploaded certificate, including the certificate's Amazon Resource Name (ARN), friendly name, identifier (ID), and expiration date. To view the uploaded certificate, run the AWS CLI command list-server-certificates : aws iam list-server-certificates For more information, see uploading a server certificate to IAM.","title":"Upload SSL Certificate to IAM"},{"location":"implementation-guide/solution-overview/features/","text":"The solution has the following features: All-in-one log ingestion : provides a single web console to ingest both application logs and AWS service logs into the Amazon OpenSearch Service domains. For supported AWS service logs, refer to AWS Service Logs . For supported application logs, refer to Application Logs . Codeless log processor : supports log processor plugins developed by AWS. You are allowed to enrich the raw log data through a few steps on the web console. Out-of-the-box dashboard template : offers a collection of reference designs of visualization templates, for both commonly used software such as Nginx and Apache HTTP Server, and AWS services such as Amazon S3 and Amazon CloudTrail.","title":"Features and benefits"},{"location":"implementation-guide/solution-overview/use-cases/","text":"The solution can be applied to the following use cases: Security and compliance regulations Comply with regulatory requirements such as MLPS, GDPR, PCI DSS, and HIPAA. Easily store equipment, network, and application logs in a centralized place for log auditing and threat detection. Business operations and data analysis Identify trends and patterns in minutes, and build interactive and intuitive visualization. Derive business insights from logs and empower business decisions with data. Application and infrastructure troubleshooting Monitor both application and cloud infrastructure logs with ease, understand and resolve the root cause of issues quickly. Improve observability of your workloads, and achieve better business stability.","title":"Use cases"}]}