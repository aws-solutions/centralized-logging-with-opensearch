{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>The Log Hub solution provides comprehensive log management and analysis functions to help you simplify the build of log analytics pipelines. Built on top of Amazon OpenSearch Service, the solution allows you to streamline effectively log ingestion, log processing, and log visualization. You can leverage the solution in multiple use cases such as to abide by security and compliance regulations, achieve refined business operations, and enhance IT troubleshooting and maintenance.</p> <p>The solution has the following features:</p> <ul> <li> <p>All-in-one log ingestion: provides a single web console to ingest both application logs and AWS service logs into the Amazon OpenSearch (AOS) domains. For supported AWS service logs, see AWS service logs. For supported application logs, see Application logs.</p> </li> <li> <p>Codeless log processor: supports log processor plugins developed by AWS. You are allowed to enrich the raw log data through a few clicks on the web console.</p> </li> <li> <p>Out-of-box dashboard template: offers a collection of reference designs of visualization templates, for both commonly used software such as Nginx and Apache HTTP Server, and AWS services such as Amazon S3 and Amazon CloudTrail.</p> </li> </ul> <p>This guide includes a getting started chapter to walk you through the process of building log analytics pipelines, and a domain management chapter to introduce how to import AOS domains on the Log Hub web console.</p> <p>This implementation guide describes architectural considerations and configuration steps for deploying the Log Hub solution in the AWS cloud. It includes links to CloudFormation templates that launches and configures the AWS services required to deploy this solution using AWS best practices for security and availability.</p> <p>The guide is intended for IT architects, developers, DevOps, data engineers with practical experience architecting on the AWS Cloud.</p>"},{"location":"designs/","title":"Log Hub","text":"<p>Important</p> <p>If you are looking for the user/implementation guide of Log Hub, please refer to Implementation Guide.  The content here are design documentations of Log Hub solution. We have a plan to open source all our designs (including historical designs) on this site. However, we do NOT guarantee  all documentations here are implemented in the Log Hub solution. </p> <p>Log Hub is an AWS Solution simplifies the build of log analytics pipelines on top of Amazon OpenSearch Service, which helps customers to improve engineering efficiency on log ingestion, log processing and log visualization. The Log Hub solution provides to customers, as a complementary of Amazon OpenSearch Service, capabilities of centralized log ingestion across multiple regions and account, one-click creation of codeless log processors and templated dashboards for visualization. With the unified web console, the Log Hub solution allows customers to create end-to-end log analytics workloads by automating the orchestration of different AWS services within minutes.</p>"},{"location":"designs/#resources","title":"Resources","text":"<ul> <li>GitHub Repo</li> <li>Feature Request/Bug Report</li> </ul>"},{"location":"designs/FAQ/","title":"Internal FAQ","text":""},{"location":"designs/FAQ/#will-the-solution-support-self-hosted-elasticsearch-domain-in-the-future","title":"Will the solution support self-hosted Elasticsearch domain in the future?","text":"<p>There is no clear answer for this question yet. We have heard from customers that the AOS price is too high comparing  to self-hosted Elasticsearch domain. Because of this, we have added this feature in our backlog. However, we need more  information to prioritize our backlogs. Currently, it is in low priority.</p>"},{"location":"designs/assets/","title":"Assets","text":"<p>Here are some (not all) assets that help in building the Log Hub solution. We appreciate their great contribution to those blogs, open source products. </p>"},{"location":"designs/assets/#blogs","title":"Blogs","text":"<ul> <li>Configuring and authoring Kibana dashboards</li> <li>Query and analyze Amazon S3 data with the new Amazon Athena plugin for Grafana</li> <li>Query data in Amazon OpenSearch Service using SQL from Amazon Athena</li> <li>Build an observability solution using managed AWS services and the OpenTelemetry standard</li> <li>Deploy a dashboard for AWS WAF with minimal effort</li> <li>Log Analytics on AWS</li> <li>\u5728 AWS \u4e2d\u56fd\u533a\u5bf9 Amazon Elasticsearch Kibana \u8fdb\u884c\u8eab\u4efd\u8ba4\u8bc1\u7684\u89e3\u51b3\u65b9\u6848</li> </ul>"},{"location":"designs/assets/#github-projects","title":"GitHub projects","text":"<ul> <li>awslabs/centralized-logging</li> <li>siem-on-amazon-opensearch-service</li> <li>mingrammer/flog</li> <li>aws-samples/amazon-elasticsearch-service-with-cognito</li> <li>jkeczan/aws-api-gateway-elastic-search-proxy</li> </ul>"},{"location":"designs/concepts/","title":"Concepts","text":"<p>We created a couple of concepts to keep us at the same page. The following session will explain the definition of each concept.</p>"},{"location":"designs/concepts/#solution-components","title":"Solution Components","text":"<ul> <li> <p>Search Engine. The Search Engine is a bottom layer to index the log information and provide CRUD (Create, Read, Update,   Delete) capability to the other applications. In this system, the Search Engine represents Amazon Elasticsearch (AES)   cluster by default.</p> </li> <li> <p>Log Visual. It is a piece of software used to build up visuals, metrics and dashboard. In this system, the Log Visual   represents the Kibana associated with the AES cluster.</p> </li> <li> <p>Log Visual Template. A Log Visual Template is a standard configuration of commonly used applications (or AWS native   service) in Log Visual. For example, a standard dashboard template for Nginx, Apache, MySQL, or CloudFront.</p> </li> <li> <p>Log Buffer. A middle layer between Log Agent and AES. We introduce this layer to protect the engine layer from   being overwhelmed.</p> </li> <li> <p>Log Jobs. A Layer between Log Buffer Layer and Search Engine. In this layer, the users can perform some data cleanup   using some additional compute resource. For example, Lambda, Glue.</p> </li> <li> <p>Log Store. A storage media to keep log data. </p> </li> <li> <p>Instance Agent. An Instance Agent is a piece of software installed on EC2 instances (or on-premise instances) which can be   used to collect log and report to Amazon Elasticsearch (AES), Log Buffer or Log Storage.</p> </li> <li> <p>Container Agent. A daemon agent which being used on ECS/EKS cluster to collect log and report to Search Engine,   Cache Layer or Storage Layer.</p> </li> <li> <p>Mobile SDK. An SDK embedded in the mobile (iOS, Android, Web) client. Customers can use this SDK to authenticate against   AWS and send log, click stream to AWS.</p> </li> <li> <p>Configuration Center. It is a portal deployed in the customer's AWS account protected by Cognito User Pool or   OpenID Connect Provider. Customers can manage/import Search Engine, install/configure log agents, backup data and others.</p> </li> <li> <p>Log Format Config. A log format template is a configuration file for log agent to know the format of the   log, the location of the log file, the destination of the log, the local log processor, and others.</p> </li> <li> <p>Log Collector for Services. A component used to extract logs from AWS native services and send to   Log Buffer or Log Engine. For example, Log Collector for Lambda is a component used to extract data from CloudWatch   Logs and upload to AES or Kinesis. Log Collector for CloudFront is a component extract logs from S3 and upload to AES   or Kinesis. Log Collector for Lambda@Edge can extract logs from CloudWatch Logs in different regions and upload.</p> </li> <li> <p>Log Monitor. A component to create alarm and send notifications. In this system, we use the Kibana built-in feature   together with SNS. We also extend SNS notification target to support WeChat Enterprise account and DingTalk.</p> </li> <li> <p>Multi-tenant. A fine-grained access control to log data.</p> </li> <li> <p>Workload Simulator. A typical workload architect with application running on AWS. For example, a 3-tier web    application. This is used for customers to get sample experience of this solution.</p> </li> <li> <p>Traffic Generator. A piece of software used to generate web traffic, this will cause the simulated workload    to generate logs.</p> </li> </ul>"},{"location":"designs/concepts/#accounts","title":"Accounts","text":"<ul> <li> <p>Main Account. The main AWS account which contains the underlying infrastructure like Search Engine, Log Buffer,    Log Jobs, Log Storage, Configuration Center and others. All logs are sent to this account for further analysis and    storage.</p> </li> <li> <p>Sub Account. The other AWS accounts where your application are deployed and need to be sent to the centralized logging    platform.</p> </li> </ul>"},{"location":"designs/concepts/#solution-assets","title":"Solution Assets","text":"<ul> <li> <p>Deliverables. Amazon CloudFormation templates or AWS CDK package used to create a solution component or a combination    of solution components. The rest assets like Lambda code will be hosted in public S3 buckets owned by Solutions Builder team.</p> </li> <li> <p>Implementation Guide. A user manual gives guidance to the customers how to use the solution. It includes a step-by-step    Getting Started guide.</p> </li> <li> <p>Workshop Portal. A portal hosting the associated solution workshop. The workshop content can be used for the field    team to organize an offline customer facing workshop.</p> </li> </ul>"},{"location":"designs/log-analytics-pipeline-service/","title":"Service Log","text":""},{"location":"designs/log-analytics-pipeline-service/#amazon-s3-access-log","title":"Amazon S3 Access Log","text":""},{"location":"designs/log-analytics-pipeline-service/#amazon-cloudtrail-log","title":"Amazon CloudTrail Log","text":""},{"location":"designs/user-stories/","title":"User Stories","text":""},{"location":"designs/user-stories/#deploy-configuration-center","title":"Deploy Configuration Center","text":""},{"location":"designs/user-stories/#install-instance-agent","title":"Install Instance Agent","text":"<p>EC2 instances launched from EC2 Quick Start comes with SSM Agent installed. However, it is not associated with an appropriate instance profile which enable them to connect to the SSM control panel. Once the instance profile associated with EC2 instances, we can use the SSM Run Command to install Log Agent to report logs to Elasticsearch.</p>"},{"location":"designs/user-stories/#install-container-agent","title":"Install Container Agent","text":""},{"location":"designs/user-stories/#import-search-engine","title":"Import Search Engine","text":"<p>s</p>"},{"location":"designs/UI/help-panel/","title":"Help Panel","text":"<p>Help panel is a place to offer more information to customers. The Sketch file will not provide help panel information. We consolidate all help panel information in this PRD. The following is an example of Help Panel and its description.</p>"},{"location":"designs/UI/help-panel/#example","title":"Example","text":"Section Content Notes Title Help panel title Body This is a paragraph with some bold text and also some italic text.h4 section headerCode can be formatted as lines of code or blocks of code.<code>&lt;this is a block of code&gt;</code>h4 section headerCode can be formated as lines of code or blocks of code.<code>&lt;This is a block of code&gt;</code> Learn more First link to the documentationSecond link to the documentation"},{"location":"designs/UI/help-panel/#domain-detail","title":"Domain detail","text":""},{"location":"designs/UI/help-panel/#access-proxy","title":"Access Proxy","text":"Section Content Notes Title Access Proxy Body Access Proxy creates a Nginx based proxy (behind Application Load Balancer) which allows you to access the OpenSearch Dashboards through Internet.Prerequisites1. Domain name2. The domain associated SSL certificate in Amazon Certificate Manager (ACM)3. A EC2 public key Learn more Create a Access Proxy"},{"location":"designs/UI/help-panel/#alarms","title":"Alarms","text":"Section Content Notes Title Alarms Body Amazon OpenSearch provides a set of recommended CloudWatch alarms, Log Hub can help customers to create the alarms automatically, and sent notification to your email (or SMS) via SNS. Learn more Create OpenSearch Alarms"},{"location":"designs/UI/help-panel/#log-processing","title":"Log Processing","text":"Section Content Notes Title Log Processing Body Log Hub will provision Lambda (or other compute resource) to process logs using these networking configurations. You can specify the log processing networking layer when import OpenSearch domains. NoteThe log processing layer has access to the OpenSearch domain. Learn more Import OpenSearch domain"},{"location":"designs/UI/help-panel/#import-domain","title":"Import Domain","text":""},{"location":"designs/UI/help-panel/#networking-creation-creation-method","title":"Networking creation - Creation method","text":"Section Content Notes Title Creation method Body When import OpenSearch domains, you need to specify the networking configuration associated with the Log Processing Layer. Log Hub will automatically place Lambda (or other compute resource) in this layer. The Log Processing Layer must have access to the OpenSearch domain. AutomaticLog Hub will detect if there is a need to create a VPC Peering Connection. If needed, Log Hub will automatically create a VPC Peering Connection, update route table, and update the security group of OpenSearch domain.ManualManually specify the Log Processing Layer networking information. You may need to create VPC Peering Connection, update route table and security group of OpenSearch domain. Learn more Import OpenSearch domain"},{"location":"designs/UI/help-panel/#log-processing-network","title":"Log processing network","text":"Section Content Notes Title Log processing network Body When import OpenSearch domains, you need to specify the networking configuration associated with the Log Processing Layer. Log Hub will automatically place Lambda (or other compute resource) in this layer. The Log Processing Layer must have access to the OpenSearch domain. S3 Service accessBy default, Log Hub will output error logs to Amazon S3. Please guarantee the log processing layer has network access to S3. You can do it by place the log processing layer in public subnets, use AWS PrivateLink for Amazon S3 or via NAT Gateways.CloudWatch Logs accessMany AWS services output service logs to CloudWatch Logs. If you use Log Hub to ingest service logs. Please guarantee the log processing layer has network access to CloudWatch Logs. Kinesis Data Streams accessApplication logs are sent to Kinesis Data Streams in Log Hub. Please guarantee the log processing layer has networking access to Kinesis Data Streams. Learn more"},{"location":"designs/UI/help-panel/#service-log-ingestion","title":"Service Log ingestion","text":""},{"location":"designs/UI/help-panel/#creation-method","title":"Creation method","text":"Section Content Notes Title Log enabling Body Log Hub can automatically detect the log location, or you can specify the log location manually. AutomaticLog Hub will automatically detect the log location of the selected AWS service. If needed, it will enable the service log and save to a centralized log bucket.ManualManually input the AWS service source and its log location . Log Hub will read logs from the location you specified. Learn more"},{"location":"designs/UI/help-panel/#sample-dashboard","title":"Sample dashboard","text":"Section Content Notes Title Sample dashboard Body Log Hub will insert a preconfigured dashboard into the OpenSearch domain if Yes being selected. The dashboard name will be consist with your index name. Learn more"},{"location":"designs/UI/help-panel/#log-lifecycle","title":"Log lifecycle","text":"Section Content Notes Title Log lifecycle Body Log Hub will insert an Index State Management (ISM) into the OpenSearch domain. The life cycle will periodically move your indices in OpenSearch to save cost. Learn more Index State Management"},{"location":"designs/UI/help-panel/#log-config","title":"Log Config","text":""},{"location":"designs/UI/help-panel/#log-path","title":"Log Path","text":"Section Content Notes Title Log Path Body Specify the log file locations. If you have mutliple locations, please write all the locations and split using ','.  e.g. <code>/var/log/app1/*.log,/var/log/app2/*.log</code>. Learn more"},{"location":"designs/UI/help-panel/#nginx-log-format","title":"Nginx Log Format","text":"Section Content Notes Title Nginx Log Format Body Nginx capture detailed information about errors and request in log files. You can find the log format configuration in Nginx configuration file, such as the <code>/etc/nginx/nginx.conf</code> file.  The log format directive starts with <code>log_format</code>. Learn more Configuring Logging in Nginx"},{"location":"designs/UI/help-panel/#apache-log-format","title":"Apache Log Format","text":"Section Content Notes Title Apache HTTP Server Log Format Body Apache HTTP Server capture detailed information about errors and request in log files. You can find the log format configuration in Apache HTTP Server configuration file, such as the <code>/etc/httpd/conf/httpd.conf</code> file.  The log format directive starts with <code>LogFormat</code>. Learn more Apache HTTP Server Log Files"},{"location":"designs/UI/help-panel/#regular-expression","title":"Regular Expression","text":"Section Content Notes Title RegEx Log Format Body Log Hub uses custom Ruby Regular Expression to parse logs.  It supports both single-line log format and mutliple input format. Write the regular expression in Rubular to validate first and input the value here. Learn more Regular ExpressionRubular: A Rudy-based reular expression editorRegular Expression in Fluent Bit"},{"location":"designs/UI/help-panel/#application-pipeline","title":"Application Pipeline","text":""},{"location":"designs/UI/help-panel/#creation-method_1","title":"Creation Method","text":"Section Content Notes Title Instance Group Creation Body Create a new instance group, or choose an existing Instance Group created before. Learn more Instance Group"},{"location":"designs/app-log/api-design/","title":"Application Log API Design","text":""},{"location":"designs/app-log/api-design/#log-config-apis","title":"Log Config APIs","text":"<p>The following operations are available in the solution's Log Config APIs.</p>"},{"location":"designs/app-log/api-design/#create-log-config","title":"Create Log Config","text":"<p>Type: Mutation</p> <p>Description: Create a record in DynamoDB</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description confName String Yes The name of the log configuration. The name must be unique, and can only contains lower case letters and -. logType enum Yes JSON, Apache, Nginx, SingleLineText, MultiLineText. timeKey String No Time key in the log. timeOffset String No Timezone offset for the log multilineLogParser enum No JAVA_SPRING_BOOT. userLogFormat String No The log format configuration. For instance, the log format configuration of Apache. e.g. LogFormat \"%h %l %u %t \\\"%r\\\" %&gt;s %b\" common. userSampleLog String No Sampled log. regularExpression String No When the log type you select is SingleLineText, MultiLineText, you need to define a regular expression to parse the log. regularSpecs K-V No To be used to parse the log field type, we will create an index template for the search engine based on this. timeRegularExpression String No When the time key is specified, you need to define a regular expression to parse the time format. processorFilterRegex K-V No Filter details, such as filter key and condition etc. <p>Simple Request &amp; Response:</p> <pre><code>query example{\n  createLogConf(\n    confName: \"nginx-log\",\n    logType: \"Nginx\",\n    userSampleLog: \"127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \\\"GET / HTTP/1.1\\\" 200 3520 \\\"-\\\" \\\"curl/7.79.1\\\" \\\"-\\\"\",\n    userLogFormat: \"log_format%20%20main%20%20...*\",\n    regularSpecs: [],\n    timeRegularExpression: \"\",\n    processorFilterRegex: {\n      enable: true,\n      filters: [\n        {\n          key: \"status\",\n          condition: \"Include\",\n          value: \"200\"\n        }\n      ]\n    }\n  )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"createLogConf\": \"41848bb3-f48a-4cdd-b0af-861d4be768ca\"\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#update-log-config","title":"Update Log Config","text":"<p>Type: Mutation</p> <p>Description: Update a log configuration.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Config Unique ID (key in DynamoDB) confName String Yes The name of the log configuration. The name must be unique, and can only contains lower case letters and -. logType enum Yes JSON, Apache, Nginx, SingleLineText, MultiLineText. timeKey String No Time key in the log. timeOffset String No Timezone offset for the log multilineLogParser enum No JAVA_SPRING_BOOT. userLogFormat String No The log format configuration. For instance, the log format configuration of Apache. e.g. LogFormat \"%h %l %u %t \\\"%r\\\" %&gt;s %b\" common. userSampleLog String No Sampled log. regularExpression String No When the log type you select is SingleLineText, MultiLineText, you need to define a regular expression to parse the log. regularSpecs K-V No To be used to parse the log field type, we will create an index template for the search engine based on this. timeRegularExpression String No When the time key is specified, you need to define a regular expression to parse the time format. processorFilterRegex K-V No Filter details, such as filter key and condition etc. <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example{\n  updateLogConf(\n    id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\",\n    confName: \"my-nginx\",\n    logType: \"Nginx\",\n    userSampleLog: \"127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \\\"GET / HTTP/1.1\\\" 200 3520 \\\"-\\\" \\\"curl/7.79.1\\\" \\\"-\\\"\",\n    userLogFormat: \"log_format%20%20main%20%20...*\",\n    regularSpecs: [],\n    timeRegularExpression: \"\",\n    processorFilterRegex: {\n      enable: true,\n      filters: [\n        {\n          key: \"status\",\n          condition: \"Include\",\n          value: \"200\"\n        }\n      ]\n    }\n  )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"updateLogConf\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>confName already exists</li> </ul> <pre><code>{\n    \"data\": {\n        \"updateLogConf\": null\n    },\n    \"errors\": [{\n        \"path\": [\n            \"updateLogConf\"\n        ],\n        \"data\": null,\n        \"errorType\": \"Lambda:Unhandled\",\n        \"errorInfo\": null,\n        \"locations\": [{\n            \"line\": 3,\n            \"column\": 3,\n            \"sourceName\": null\n        }],\n        \"message\": \"confName already exists\"\n    }]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#delete-log-config","title":"Delete Log Config","text":"<p>Type: Mutation</p> <p>Description: We don't physically delete the record, we just set the state of the item to INACTIVE in DynamoDB Table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Config Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n    deleteLogConf(id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteLogConf\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"deleteLogConf\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"deleteLogConf\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 32,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Unknown exception, please check Lambda log for more details\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-log-configs","title":"List Log Configs","text":"<p>Type: Query</p> <p>Description: List all Log Configs</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    listLogConfs (page: 1, count: 10) {\n        logConfs {\n            id\n            confName\n            logType\n            syslogParser\n            createdDt\n            status\n        }\n        total\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listLogConfs\": {\n            \"logConfs\": [\n                {\n                    \"id\": \"b942da74-f755-499a-855e-12c43267a6c0\",\n                    \"confName\": \"my-nginx\",\n                    \"logType\": \"Nginx\",\n                    \"syslogParser\": null,\n                    \"createdDt\": \"2022-10-30T03:54:38Z\",\n                    \"status\": \"ACTIVE\"\n                },\n                ...\n            ],\n            \"total\": 4\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-log-config-details","title":"Get Log Config Details","text":"<p>Type: Query</p> <p>Description: Get details of a Log Config.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Config Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n  getLogConf (id: \"62c9a8c5-eb43-4d25-b94f-941848525645\") {\n        id\n        confName\n        logType\n        timeKey\n        timeOffset\n        createdDt\n        userLogFormat\n        userSampleLog\n        regularExpression\n        timeRegularExpression\n        regularSpecs {\n            key\n            type\n            format\n        }\n        processorFilterRegex {\n            enable\n            filters {\n                key\n                condition\n                value\n            }\n        }\n        status\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getLogConf\": {\n            \"id\": \"62c9a8c5-eb43-4d25-b94f-941848525645\",\n            \"confName\": \"myapp\",\n            \"logType\": \"SingleLineText\",\n            \"timeKey\": \"time_local\",\n            \"timeOffset\": \"+0000\",\n            \"createdDt\": \"2022-11-30T02:48:27Z\",\n            \"userLogFormat\": \"%28%3F%3Cremote_addr%3E%5CS%2B%29%5Cs*-%5Cs*%28%....*\",\n            \"userSampleLog\": \"...\",\n            \"regularExpression\": \"%28%3F%3Cremote_addr%3E%5CS%2B%29%5Cs*-%5Cs*%28%....*\",\n            \"timeRegularExpression\": \"\",\n            \"regularSpecs\": [\n                {\n                    \"key\": \"remote_addr\",\n                    \"type\": \"text\",\n                    \"format\": null\n                },\n                {\n                    \"key\": \"remote_user\",\n                    \"type\": \"text\",\n                    \"format\": null\n                },\n                {\n                    \"key\": \"time_local\",\n                    \"type\": \"text\",\n                    \"format\": \"%d/%b/%Y:%H:%M:%S\"\n                },\n                ...\n            ],\n            \"processorFilterRegex\": {\n                \"enable\": true,\n                \"filters\": [\n                    {\n                        \"key\": \"status\",\n                        \"condition\": \"Include\",\n                        \"value\": \"200\"\n                    }\n                ]\n            },\n            \"status\": \"ACTIVE\"\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#instance-group-apis","title":"Instance Group APIs","text":"<p>The following operations are available in the solution's Instance Group APIs.</p>"},{"location":"designs/app-log/api-design/#create-instance-group","title":"Create Instance Group","text":"<p>Type: Mutation</p> <p>Description: Create a record in DynamoDB</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description groupName String Yes The name of the log group. The name must be unique, and can only contains lower case letters and -. instanceSet String[] Yes EC2 Instance Id set <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n    createInstanceGroup(groupName: \"nginx-webgrp\", instanceSet: [\"web1\", \"web2\"])\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"createInstanceGroup\": \"2de27afe-d568-49cc-b7b5-86b161ce0662\"\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#delete-instance-group","title":"Delete Instance Group","text":"<p>Type: Mutation</p> <p>Description: We don't physically delete the record, we just set the state of the item to INACTIVE in DynamoDB Table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Group Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n    deleteInstanceGroup(id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteInstanceGroup\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"deleteInstanceGroup\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"deleteInstanceGroup\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 32,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Unknown exception, please check Lambda log for more details\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-instance-groups","title":"List Instance Groups","text":"<p>Type: Query</p> <p>Description: List all Instance Groups</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n    listInstanceGroups(count: 10, page: 1) {\n        total\n        instanceGroups {\n            id\n            accountId\n            region\n            groupName\n            groupType\n            instanceSet\n            createdDt\n            status\n        }\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listInstanceGroups\": {\n            \"total\": 1,\n            \"instanceGroups\": [{\n                \"createdDt\": \"2021-11-06T12:28:52.041408\",\n                \"groupName\": \"fsf1\",\n                \"groupType\": \"ASG\",\n                \"id\": \"1089057b-888b-4794-b797-fef943adccf0\",\n                \"instanceSet\": [\n                    \"1\",\n                    \"2\"\n                ]\n            }]\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-instance-group-details","title":"Get Instance Group Details","text":"<p>Type: Query</p> <p>Description: Get details of a Log Group.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Group Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n  getInstanceGroup(id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\") {\n    createdDt\n        groupName\n        id\n        instanceSet\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getInstanceGroup\": {\n    \"createdDt\": \"2021-11-06T12:28:52.041408\",\n        \"groupName\": \"fsf1\",\n        \"id\": \"1089057b-888b-4794-b797-fef943adccf0\",\n        \"instanceSet\": [\n                \"1\",\n                \"2\"\n        ]\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-instances","title":"List Instances","text":"<p>Type: Query</p> <p>Description: If you specify one or more managed node IDs, it returns information for those managed nodes.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description nextToken String No The token for the next set of items to return. (You received this token from a previous call.) maxResults Int No 10 The maximum number of items to return for this call. The call also returns a token that you can specify in a subsequent call to get the next set of results. instanceSet String[] No 1 The ID of the managed instance should be retrieved <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n        listInstances(maxResults: 10, instanceSet: [\"i-0bbf9209068ced7ed\"]) {\n        instances {\n            computerName\n            id\n            ipAddress\n            platformName\n            name\n        }\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listInstances\": {\n            \"instances\": [{\n                \"computerName\": \"ip-172-31-44-205.us-west-2.compute.internal\",\n                \"id\": \"i-0bbf9209068ced7ed\",\n                \"ipAddress\": \"172.31.44.205\",\n                \"platformName\": \"CentOS Linux\",\n                \"name\": \"Bastion\"\n            }]\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-log-agent-status","title":"Get Log Agent Status","text":"<p>Type: Query</p> <p>Description: Get Fluent Bit installation status.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description instanceId String Yes The ID of the managed instance should be retrieved region String No AWS region accountId String No AWS account ID <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n        getLogAgentStatus(instanceId: \"i-022c5110c4e3226bb\")\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getLogAgentStatus\": \"Online/Offline\"\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#application-log-pipelines-apis","title":"Application Log Pipelines APIs","text":"<p>The following operations are available in the solution's Application (App) Log Pipelines APIs.</p>"},{"location":"designs/app-log/api-design/#create-app-log-pipeline","title":"Create App Log Pipeline","text":"<p>Type: Mutation</p> <p>Description: Create an application log pipeline</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description aosParams K-V Yes Amazon OpenSearch related parameters. bufferType string Yes Type of buffer (e.g. S3, KDS etc). bufferParams List Yes Buffer related parameters. force boolean Yes Force to create pipeline when conflict detected. tags List No Custom tags. <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example{\n    createAppPipeline(\n        aosParams: {\n            coldLogTransition: 0, \n            failedLogBucket: \"backup-bucket\", \n            domainName: \"dev\", \n            engine: OpenSearch, \n            indexPrefix: \"my-index\", \n            logRetention: 180, \n            opensearchArn: \"arn:aws:es:us-west-2:123456789012:domain/dev\", \n            opensearchEndpoint: \"vpc-dev-xxx.us-west-2.es.amazonaws.com\", \n            replicaNumbers: 1, \n            shardNumbers: 5, \n            vpc: {\n                privateSubnetIds: \"subnet-1234,subnet-5678\", \n                publicSubnetIds: \"\", \n                securityGroupId: \"sg-1234\", \n                vpcId: \"vpc-0123\"\n                }, \n            warmLogTransition: 0\n        }, \n        bufferType: S3, \n        bufferParams: [\n            {paramKey: \"logBucketName\", paramValue: \"log-bucket\"}, \n            {paramKey: \"logBucketPrefix\", paramValue: \"AppLogs/my-index/year=%Y/month=%m/day=%d\"}, \n            {paramKey: \"defaultCmkArn\", paramValue: \"arn:aws:kms:us-west-2:123456789012:key/1dbbdae3-3448-4890-b956-2b9b36197784\"},\n            {paramKey: \"maxFileSize\", paramValue: \"50\"},\n            {paramKey: \"uploadTimeout\", paramValue: \"60\"},\n            {paramKey: \"compressionType\", paramValue: \"gzip\"}\n        ],\n        force: false, \n        tags: [{key: \"hello\", value: \"world\"}]\n    )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"createAppPipeline\": \"2de27afe-d568-49cc-b7b5-86b161ce0662\"\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#delete-app-log-pipeline","title":"Delete App Log Pipeline","text":"<p>Type: Mutation</p> <p>Description: We don't physically delete the record, we just set the state of the item to INACTIVE in DynamoDB Table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Group Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n    deleteAppPipeline(id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteAppPipeline\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"deleteAppPipeline\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"deleteAppPipeline\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 32,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Unknown exception, please check Lambda log for more details\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-app-log-pipelines","title":"List App Log Pipelines","text":"<p>Type: Query</p> <p>Description: List all application log pipelines</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n    listAppPipelines(count: 10, page: 1) {\n        appPipelines {\n            id\n            bufferType\n            bufferParams {\n                paramKey\n                paramValue\n            }\n            aosParams {\n                opensearchArn\n                domainName\n                indexPrefix\n                warmLogTransition\n                coldLogTransition\n                logRetention\n                shardNumbers\n                replicaNumbers\n                engine\n            }\n            createdDt\n            status\n            bufferAccessRoleArn\n            bufferAccessRoleName\n            bufferResourceName\n            bufferResourceArn\n            tags {\n                key\n                value\n            }\n        }\n        total\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listAppPipelines\": {\n            \"appPipelines\": [\n                {\n                    \"id\": \"67409286-2672-413f-b477-dc1f4d7966d4\",\n                    \"bufferType\": \"S3\",\n                    \"bufferParams\": [\n                        {\n                            \"paramKey\": \"logBucketName\",\n                            \"paramValue\": \"log-bucket\"\n                        },\n                        {\n                            \"paramKey\": \"logBucketPrefix\",\n                            \"paramValue\": \"AppLogs/my-index/year=%Y/month=%m/day=%d\"\n                        },\n                        {\n                            \"paramKey\": \"defaultCmkArn\",\n                            \"paramValue\": \"arn:aws:kms:eu-west-1:123456789012:key/9619ed02-b533-4d49-91de-3dd8efa11135\"\n                        },\n                        {\n                            \"paramKey\": \"maxFileSize\",\n                            \"paramValue\": \"50\"\n                        },\n                        {\n                            \"paramKey\": \"uploadTimeout\",\n                            \"paramValue\": \"60\"\n                        },\n                        {\n                            \"paramKey\": \"compressionType\",\n                            \"paramValue\": \"gzip\"\n                        }\n                    ],\n                    \"aosParams\": {\n                        \"opensearchArn\": \"arn:aws:es:eu-west-1:123456789012:domain/dev\",\n                        \"domainName\": \"dev\",\n                        \"indexPrefix\": \"my-index\",\n                        \"warmLogTransition\": 0,\n                        \"coldLogTransition\": 0,\n                        \"logRetention\": 180,\n                        \"shardNumbers\": 5,\n                        \"replicaNumbers\": 1,\n                        \"engine\": \"OpenSearch\"\n                    },\n                    \"createdDt\": \"2022-10-30T03:03:56Z\",\n                    \"status\": \"ACTIVE\",\n                    \"bufferAccessRoleArn\": \"arn:aws:iam::123456789012:role/LogHub-AppPipe-824f1-BufferAccessRoleDF53FD85-1ME7KUUVZVFTD\",\n                    \"bufferAccessRoleName\": \"LogHub-AppPipe-824f1-BufferAccessRoleDF53FD85-1ME7KUUVZVFTD\",\n                    \"bufferResourceName\": \"log-bucket\",\n                    \"bufferResourceArn\": \"arn:aws:s3:::log-bucket\",\n                    \"tags\": [\n                        {\n                            \"key\": \"hello\",\n                            \"value\": \"world\"\n                        }\n                    ]\n                },\n                ...\n            ],\n            \"total\": 3\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-app-log-pipeline-details","title":"Get App Log Pipeline Details","text":"<p>Type: Query</p> <p>Description: Get details of an application log pipeline.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes App Pipeline Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    getAppPipeline (id: \"67409286-2672-413f-b477-dc1f4d7966d4\") {\n        id\n        bufferType\n        bufferParams {\n            paramKey\n            paramValue\n        }\n        aosParams {\n            opensearchArn\n            domainName\n            indexPrefix\n            warmLogTransition\n            coldLogTransition\n            logRetention\n            shardNumbers\n            replicaNumbers\n            engine\n        }\n        createdDt\n        status\n        bufferAccessRoleArn\n        bufferAccessRoleName\n        bufferResourceName\n        bufferResourceArn\n        tags {\n            key\n            value\n        }\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getAppPipeline\": {\n            \"id\": \"67409286-2672-413f-b477-dc1f4d7966d4\",\n            \"bufferType\": \"S3\",\n            \"bufferParams\": [\n                {\n                    \"paramKey\": \"logBucketName\",\n                    \"paramValue\": \"log-bucket\"\n                },\n                {\n                    \"paramKey\": \"logBucketPrefix\",\n                    \"paramValue\": \"AppLogs/my-index/year=%Y/month=%m/day=%d\"\n                },\n                {\n                    \"paramKey\": \"defaultCmkArn\",\n                    \"paramValue\": \"arn:aws:kms:eu-west-1:123456789012:key/9619ed02-b533-4d49-91de-3dd8efa11135\"\n                },\n                {\n                    \"paramKey\": \"maxFileSize\",\n                    \"paramValue\": \"50\"\n                },\n                {\n                    \"paramKey\": \"uploadTimeout\",\n                    \"paramValue\": \"60\"\n                },\n                {\n                    \"paramKey\": \"compressionType\",\n                    \"paramValue\": \"gzip\"\n                }\n            ],\n            \"aosParams\": {\n                \"opensearchArn\": \"arn:aws:es:eu-west-1:123456789012:domain/dev\",\n                \"domainName\": \"dev\",\n                \"indexPrefix\": \"my-index\",\n                \"warmLogTransition\": 0,\n                \"coldLogTransition\": 0,\n                \"logRetention\": 180,\n                \"shardNumbers\": 5,\n                \"replicaNumbers\": 1,\n                \"engine\": \"OpenSearch\"\n            },\n            \"createdDt\": \"2022-11-30T03:03:56Z\",\n            \"status\": \"ACTIVE\",\n            \"bufferAccessRoleArn\": \"arn:aws:iam::123456789012:role/LogHub-AppPipe-67409-BufferAccessRoleDF53FD85-BTLM263CB8JI\",\n            \"bufferAccessRoleName\": \"LogHub-AppPipe-67409-BufferAccessRoleDF53FD85-BTLM263CB8JI\",\n            \"bufferResourceName\": \"log-bucket\",\n            \"bufferResourceArn\": \"arn:aws:s3:::log-bucket\",\n            \"tags\": [\n                {\n                    \"key\": \"hello\",\n                    \"value\": \"world\"\n                }\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#application-log-ingestion-apis","title":"Application Log Ingestion APIs","text":"<p>The following operations are available in the solution's Application (App) Log Ingestion APIs.</p>"},{"location":"designs/app-log/api-design/#create-app-log-ingestion","title":"Create App Log Ingestion","text":"<p>Type: Mutation</p> <p>Description: Create a record in DynamoDB</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description appPipelineId K-V Yes Selected Amazonn OpenSearch related parameters. confId K-V Yes Created Kinesis Data Stream related parameters. groupIds String[] Yes Created Kinesis Data Stream related parameters. stackId String Yes In the process of creating an application log pipeline, KDS and Lambda are created through the CloudFormation stack. This item can be obtained through the listAppLogIngestions API. stackName String Yes In the process of creating an application log pipeline, KDS and Lambda are created through the CloudFormation stack. This item can be obtained through the listAppLogIngestions API. <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example{\n    createAppLogIngestion(\n          appPipelineId: \"45851795-6401-41f7-8ded-6c6db14f375c\",\n          confId: \"01523e70-b571-4583-8882-56c877ec098c\",\n          groupIds: [\"afa6c23f-765c-4322-bb00-234525a5ff85\"],\n          stackId: \"\",\n          stackName: \"\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"createAppLogIngestion\": \"2de27afe-d568-49cc-b7b5-86b161ce0662\"\n    }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n    \"data\": {\n        \"createAppLogIngestion\": null\n    },\n    \"errors\": [{\n        \"path\": [\n            \"createAppLogIngestion\"\n        ],\n        \"data\": null,\n        \"errorType\": \"Lambda:Unhandled\",\n        \"errorInfo\": null,\n        \"locations\": [{\n            \"line\": 23,\n            \"column\": 3,\n            \"sourceName\": null\n        }],\n        \"message\": \"please check groupId afa6c23f-765c-4322-bb00-234525a5ff85 and conId 01523e70-b571-4583-8882-56c877ec098c, they already exist in applineId 45851795-6401-41f7-8ded-6c6db14f375c\"\n    }]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#delete-app-log-ingestion","title":"Delete App Log Ingestion","text":"<p>Type: Mutation</p> <p>Description: We don't physically delete the record, we just set the state of the item to INACTIVE in DynamoDB Table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description ids String[] Yes Log Ingestion ID Set (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n        deleteAppLogIngestion(\n              ids: [\"60779959-95e3-45b6-a433-225f5c57edcc\", \"86b02ebc-d952-4b37-ac17-f001150d3a16\"]\n              )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteAppLogIngestion\": \"OK\"\n  }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-app-log-ingestions","title":"List App Log Ingestions","text":"<p>Type: Query</p> <p>Description: List all Ingestion</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description appPipelineId String Yes 10 Application Pipeline Unique Id count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n    listAppLogIngestions(appPipelineId: \"45851795-6401-41f7-8ded-6c6db14f375c\", count: 10, page: 1) {\n        appLogIngestions {\n            appPipelineId\n            confId\n            confName\n            createdDt\n            groupId\n            groupName\n            stackName\n            stackId\n            id\n            tags [{\n                key\n                value\n            }]\n        }\n        total\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listAppLogIngestions\": {\n            \"appLogIngestions\": [{\n                    \"appPipelineId\": \"45851795-6401-41f7-8ded-6c6db14f375c\",\n                    \"confId\": \"01523e70-b571-4583-8882-56c877ec098c\",\n                    \"confName\": \"c2\",\n                    \"createdDt\": \"2021-11-16T11:26:35.509759\",\n                    \"groupId\": \"afa6c23f-765c-4322-bb00-234525a5ff85\",\n                    \"groupName\": \"g4\",\n                    \"stackName\": \"\",\n                    \"stackId\": \"\",\n                    \"id\": \"dd0eb789-6a33-4b51-873d-f5473ccdf144\",\n                    \"tags\": []\n                },\n                {\n                    \"appPipelineId\": \"45851795-6401-41f7-8ded-6c6db14f375c\",\n                    \"confId\": \"01523e70-b571-4583-8882-56c877ec098c\",\n                    \"confName\": \"c2\",\n                    \"createdDt\": \"2021-11-16T11:26:35.509716\",\n                    \"groupId\": \"8ef2debb-1c72-4821-9e61-ce89b6c6ed00\",\n                    \"groupName\": \"g3\",\n                    \"stackName\": \"\",\n                    \"stackId\": \"\",\n                    \"id\": \"af65c64b-7403-4e92-90f6-1ec13d655deb\",\n                    \"tags\": []\n                }\n            ],\n            \"total\": 2\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-app-log-ingestion-details","title":"Get App Log Ingestion Details","text":"<p>Type: Query</p> <p>Description: Get details of a Ingestion.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes App Log Ingestion Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    getAppLogIngestion(id: \"5051c5ce-f0fb-4b6e-be39-05490756b335\") {\n        appPipelineId\n        confId\n        createdDt\n        groupId\n        id\n        stackId\n        stackName\n        tags[{\n            key\n            value\n        }]\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getAppLogIngestion\": {\n            \"appPipelineId\": \"f45648b9-cfa8-4bfb-bf6b-f7a06a8fecf1\",\n            \"confId\": \"c1\",\n            \"createdDt\": \"2021-11-07T17:48:03.935902\",\n            \"groupId\": \"g1\",\n            \"id\": \"5051c5ce-f0fb-4b6e-be39-05490756b335\",\n            \"stackId\": \"s\",\n            \"stackName\": \"ss\",\n            \"tags\": []\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/architecture-design/","title":"Application Log Analytics Design","text":""},{"location":"designs/app-log/architecture-design/#overview","title":"Overview","text":"<p>Application Log Analytics, as one module of Log Hub solution, is used to collect logs for Application, process and ingest into Amazon OpenSearch Service (AOS). This document is to describe this module is designed.</p> <p>Currently, this solution supports JSON format, Nginx Format, Apache Format, Spring Boot Logs, Single-line text, Multi-line text.</p> <p>Info</p> <p>For more information about solution overall design, refer to Architecture Design.</p>"},{"location":"designs/app-log/architecture-design/#high-level-design","title":"High Level Design","text":"<p>Model Layer -The proxy of the back-end service encapsulates the model required by the view layer and defines the output content by the caller.</p> <p>Resources - Here we specifically refer to the Amazon Web Services created and invoked through APIs in the solution, such as EC2 instances that need to transmit log data, Systems Manager used by Fluent Bit installed, Amazon Kinesis Data Streams, Amazon Lambda, Amazon OpenSearch Service used for log storage and data analysis and presentation.</p> <p>Log Config Service - To used to describe log configuration information, including log location, log type, search engine field type, etc.</p> <p>Instance Group Service - An instance Group is a collection of instances. Currently, only instances in the same region as Log Hub are supported. This service is responsible for the logical classification of instances, the installation of Fluent Bit on the instance, and the status detection of Fluent Bit. For the installation of Fluent Bit, the system is processed through multi-threading.</p> <p>Application Log Pipeline Service - Responsible for asynchronous creation of data buffers, data buffer automatic scaling service, and log processor instance.</p> <p>Application Log Ingestion Service - Responsible for generating configuration files, distributing configuration through SSM, and scheduling Fluent Bit for data transmission, and creating OpenSearch templates according to the log field data types defined by Log config to ensure that the data written to OpenSearch conforms to the preset data types.</p> <p>Kinesis Data Streams Auto Scaling Service Amazon Kinesis Data Streams is automatically scaled by using Amazon CloudWatch and AWS Lambda if the user turns on autoscaling for the data buffer.</p>"},{"location":"designs/app-log/data-model-design/","title":"Application Log Analytics Data Model Design","text":""},{"location":"designs/app-log/data-model-design/#overview","title":"Overview","text":"<p>This part uses Amazon DynamoDB as the backend NoSQL database. This document is about the Data Model Design for Application Log Analytics module. </p>"},{"location":"designs/app-log/data-model-design/#entity-relationship-diagram","title":"Entity Relationship Diagram","text":""},{"location":"designs/app-log/data-model-design/#table-design","title":"Table Design","text":""},{"location":"designs/app-log/data-model-design/#logconf-table","title":"LogConf Table","text":"<p>LogConf table stores information about the Application log configuration by this solution, such as log type, log path.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a configuration Partition key confName String The name of the configuration name createdDt String creation time logPath String Log file path logType String Json, Regex, Nginx, Apache, MultiLineText multilineLogParser String JAVA_SPRING_BOOT regularExpression String Regular expressions regularSpec String Field type definition after regular parsing status String ACTIVE, INACTIVE INACTIVE means delete state userLogFormat String Log format updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#instancegroup-table","title":"InstanceGroup Table","text":"<p>InstanceGroup table stores information about the instance and grouping relationship information by this solution, such as log type.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a group Partition key groupName String The name of the log group. the name must be unique, and can only contains lower case letters and -. createdDt String Creation time instanceSet String List of instance ids status String ACTIVE, INACTIVE INACTIVE means delete state updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#instancemeta-table","title":"InstanceMeta Table","text":"<p>InstanceMeta table stores information about the instance ingestion by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a configuration Partition key createdDt String Creation time intanceId String The EC2 instance id appPipelineId String The Partition key of the AppPipeline table logAgent Map sub-field: agentName: FluentBit sub-field: version: 1.8.2 confId String The Partition key of the LogConf table groupId String The Partition key of the InstanceGroup table status String ACTIVE, INACTIVE updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#logagentstatus-table","title":"LogAgentStatus Table","text":"<p>LogAgentStatus table stores information about the status of Fluent Bit installation by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments intanceId String the EC2 instance Id Partition key createdDt String creation time id String the Command Id status String Not_Installed, Online, Offline updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#apppipeline-table","title":"AppPipeline Table","text":"<p>AppPipeline table stores information about Application Log Pipeline by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of the pipeline Partition key aosParams Map OpenSearch details (e.g. OpenSearch domain Arn, endpoint etc.) bufferParams List List of Map (paramKey, paramValue) bufferType String Type of buffer used in pipeline (such as KDS, S3) bufferResourceArn String Buffer Resource Arn (e.g. if buffer is S3, then it's S3 bucket ARN) bufferResourceName String Buffer Resource Name (e.g. if buffer is S3, then it's S3 bucket Name) osHelperFnArn String A helper Function ARN stackId String CloudFormation Stack ID error String CloudFormation Stack Error if any tags List List of Map (Key-Value) status String CREATING, DELETING, ERROR, INACTIVE, ACTIVE createdDt String creation time"},{"location":"designs/app-log/data-model-design/#applogingestion-table","title":"AppLogIngestion Table","text":"<p>AppLogIngestion table is used to the information about Application Log Ingestion by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a pipeline Partition key createdDt String creation time confId String The Partition key of the LogConf table sourceType String EC2,EKS, S3 sourceId String If EC2 then sourceId is groupId; If EKS then sourceId is EKSClusterId; If S3 then sourceId is S3LogSourceInfo; groupId String The Partition key of the InstanceGroup table stackId String The Cloudformation stack ID for ingesting application logs from the S3 bucket or K8s pod. stackName String The Cloudformation stack Name for ingesting application logs from the S3 bucket or K8s pod. appPipelineId String The Partition key of the AppPipeline table tags Map Sub-field: key-value, type:String status String CREATING, DELETING, ERROR, INACTIVE, ACTIVE updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#eksclusterlogsource-table","title":"EKSClusterLogSource Table","text":"<p>EKSClusterLogSource table stores information about imported EKS Cluster by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a pipeline Partition key createdDt String creation time aosDomainId String The Partition key of the Cluster table region String The region to which the imported EKS cluster belongs accountId String The account to which the imported EKS cluster belongs eksClusterName String The Partition key of the InstanceGroup table eksClusterArn String The imported EKS Cluster ARN. cri String The K8s Container runtime : containerd,docker. subnetIds String The EKS Cluster Subnets vpcId Map The EKS Cluster vpcId eksClusterSGId String The EKS Cluster security group oidcIssuer String OpenID Connect provider URL endpoint String The EKS Cluster API server endpoint deploymentKind String DaemonSet,Sidecar tags Map Sub-field: key-value, type:String logAgentRoleArn String The ARN of the role corresponding to the service account of K8s, this role attaches write-related permissions to KDS. status String CREATING, DELETING, ERROR, INACTIVE, ACTIVE updatedDt String The last time the data was updated"},{"location":"designs/app-log/log-analytics-pipeline-app/","title":"Application Log","text":"<p>A Log Pipeline includes the process of receiving, cleaning, enhancing, and writing data to AOS. Typically, a log pipeline only accepts logs in one format. A log analytics pipeline corresponds to an index pattern in AOS (e.g. <code>log-hub-nginx-log-index</code>)\u3002</p>"},{"location":"designs/app-log/log-analytics-pipeline-app/#system-architecture-design","title":"System Architecture design","text":""},{"location":"designs/app-log/log-analytics-pipeline-app/#concept","title":"Concept","text":""},{"location":"designs/app-log/log-analytics-pipeline-app/#log-config","title":"Log Config","text":"<p>Log Config contains</p> <ul> <li> <p>Config Name: The name of the log configuration, the name must be unique, and can only contains lower case letters and -.</p> </li> <li> <p>Log Path: Specify the log file locations. If you have multiple locations, please write all the locations and split using ' , '. e.g./var/log/app1/.log,/var/log/app2/.log. All files in the specified folder that match the file name will be monitored. The file name can be a full name or wildcard pattern matching is supported.</p> </li> <li> <p>Log Type: the Log Hub has built-in plugins to parse log data from log agents. Currently supported log types are as follows:</p> </li> <li> <p>JSON, Nginx Log, Apache Log, Spring Boot Log</p> </li> <li> <p>Additionally, we also point to custom parsing via regular expressions</p> </li> <li> <p>Log Format: the log format configuration in Nginx, Spring Boot, Apache configuration files. Such as Nginx, if you want to know more, you can refer to configuring logging in Nginx</p> </li> </ul>"},{"location":"designs/app-log/log-analytics-pipeline-app/#log-group","title":"Log Group","text":"<p>A Log Group is a collection of one or more Log Configs applied to a group of EC2 instances. The following figure can better understand the concepts of Log Group and Log Config. </p> <p>According to the configuration shown in the figure above, the configuration files of FluentBit in Instance A, B, and C are all different.</p> <ul> <li>Instance A: Collect Nginx type log a and JSON format log b.</li> <li>Instance B: Collect all four types of logs.</li> <li>Instance C: Collect Apache type logs c and JSON format logs d.</li> </ul>"},{"location":"designs/app-log/log-analytics-pipeline-app/#faq","title":"FAQ","text":"<p>Q. Why not use Firehose to collect and write to AOS?</p> <p>Because the minimum buffer interval of Firehose is 60s, it is difficult to meet the scene of real-time class analysis. Please refer to Amazon Kinesis Data Firehose Quota.</p>"},{"location":"designs/app-log/process-design/","title":"Application Log Pipeline Process","text":"<p>This document is about the Process Design for Log Agent Installation, Application Log Pipeline and Ingestion.</p>"},{"location":"designs/app-log/process-design/#overview","title":"Overview","text":""},{"location":"designs/app-log/process-design/#install-log-agent","title":"Install Log Agent","text":""},{"location":"designs/app-log/process-design/#create-an-application-log-ingestion","title":"Create an Application Log Ingestion","text":""},{"location":"designs/domain-management/api-design/","title":"Domain Management API Design","text":""},{"location":"designs/domain-management/api-design/#overview","title":"Overview","text":"<p>This document is about the API Design for Domain Management component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/domain-management/api-design/#domain-apis","title":"Domain APIs","text":"<p>Domain APIs are a list of operations on top of Amazon OpenSearch Service (AOS). </p> <p>The following operations are available in the solution's Domain APIs.</p>"},{"location":"designs/domain-management/api-design/#list-domain-names","title":"List Domain Names","text":"<p>Type: Query</p> <p>Description:  List all existing Amazon OpenSearch domains in a region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description region String No current region To support cross region listing (in the same account) <p>Simple Request &amp; Response:</p> <p>Request with region </p> <pre><code>query example{\n  listDomainNames(region: \"us-west-2\") {\n    domainNames\n  }\n}\n</code></pre> <p>Request without region</p> <pre><code>query example {\n  listDomainNames {\n    domainNames\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listDomainNames\": {\n      \"domainNames\": [\n        \"dev\",\n        \"test\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#import-domain","title":"Import Domain","text":"<p>Type: Mutation</p> <p>Description:  Import an Exisiting Amazon OpenSearch Domain,  store general info from DynamoDB table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description domainName String Yes Amazon OpenSearch Domain Name region String No current region To support cross region Amazon OpenSearch import vpc K-V Yes Log processing vpc tags K-V No Custom tags for the imported domain <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example{\n  importDomain(\n    domainName: \"dev\", \n    tags: {key: \"project\", value: \"Loghub\"},\n    vpc: {\n        securityGroupId: \"sg-1\", \n        vpcId: \"vpc-1\", \n        privateSubnetIds: \"subnet-a,subnet-b\", \n        publicSubnetIds: \"subnet-c,subnet-d\"\n    },\n    region: \"us-west-2\"\n  )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"importDomain\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Domain is already imported</li> <li>Elasticsearch Domain Not Found</li> <li>Public network type is not supported, only Amazon OpenSearch domain within VPC can be imported</li> <li>The domain to be imported must be active</li> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"importDomain\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"importDomain\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 8,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Domain is already imported\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#remove-domain","title":"Remove Domain","text":"<p>Type: Mutation</p> <p>Description:  Remove an Amazon OpenSearch Domain record from DynamoDB table. This will not remove the backend AOS domain.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  removeDomain(id: \"439239da8014f9a419c92b1b0c72a5fc\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"removeDomain\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"removeDomain\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"removeDomain\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 32,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Unknown exception, please check Lambda log for more details\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#list-imported-domains","title":"List Imported Domains","text":"<p>Type: Query</p> <p>Description:  List all existing Amazon OpenSearch domains in a region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description metrics Boolean No FALSE To decide wheather need to query domain metrix (additional request) <p>Simple Request &amp; Response:</p> <p>Request: </p> <pre><code>query example {\n  listImportedDomains(metrics: true) {\n    domainName\n    endpoint\n    id\n    metrics {\n      freeStorageSpace\n      health\n      searchableDocs\n    }\n    version\n    engine\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listImportedDomains\": [\n      {\n        \"id\": \"439239da8014f9a419c92b1b0c72a5fc\",\n        \"domainName\": \"dev\",\n        \"endpoint\": \"vpc-dev-3ze2yoxxxxxxxxx.us-west-2.es.amazonaws.com\",\n        \"metrics\": {\n          \"freeStorageSpace\": 16058.91,\n          \"health\": \"GREEN\",\n          \"searchableDocs\": 13159\n        },\n        \"version\": \"1.0\",\n        \"engine\": \"OpenSearch\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#get-domain-details","title":"Get Domain Details","text":"<p>Type: Query</p> <p>Description:  Get details of an imported domain.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn (key in DynamoDB) metrics Boolean No FALSE Whether to include metrics <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n  getDomainDetails(id: \"439239da8014f9a419c92b1b0c72a5fc\") {\n    domainName\n    endpoint\n    id\n    nodes {\n      coldEnabled\n      dedicatedMasterCount\n      dedicatedMasterEnabled\n      dedicatedMasterType\n      instanceCount\n      instanceType\n      warmCount\n      warmEnabled\n      warmType\n      zoneAwarenessEnabled\n    }\n    tags {\n      key\n      value\n    }\n    storageType\n    volume {\n      size\n      type\n    }\n    vpc {\n      privateSubnetIds\n      publicSubnetIds\n      securityGroupId\n      vpcId\n    }\n    metrics {\n      freeStorageSpace\n      health\n      searchableDocs\n    }\n    engine\n    version\n    proxyALB\n    proxyError\n    proxyInput {\n      certificateArn\n      cognitoEndpoint\n      customEndpoint\n      keyName\n      vpc {\n        privateSubnetIds\n        publicSubnetIds\n        securityGroupId\n        vpcId\n      }\n    }\n    proxyStatus\n    alarmError\n    alarmInput {\n      email\n      phone\n      alarms {\n        type\n        value\n      }\n    }\n    alarmStatus\n    cognito {\n      domain\n      enabled\n      identityPoolId\n      userPoolId\n      roleArn\n    }\n    accountId\n    domainArn\n    region\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getDomainDetails\": {\n      \"id\": \"439239da8014f9a419c92b1b0c72a5fc\",\n      \"domainName\": \"dev\",\n      \"endpoint\": \"vpc-dev-3ze2yoxxxxxxxxx.us-west-2.es.amazonaws.com\",\n      \"engine\": \"OpenSearch\",\n      \"version\": \"1.0\"\n      \"vpc\": {\n        \"privateSubnetIds\": \"subnet-1234\",\n        \"publicSubnetIds\": \"subnet-6789\",\n        \"securityGroupId\": \"sg-1\",\n        \"vpcId\": \"vpc-1\"\n      },\n      \"cognito\": {\n        \"domain\": \"\",\n        \"enabled\": false,\n        \"identityPoolId\": \"N/A\",\n        \"userPoolId\": \"N/A\",\n        \"roleArn\": \"N/A\"\n      }\n      \"nodes\": {\n        \"coldEnabled\": false,\n        \"dedicatedMasterCount\": 0,\n        \"dedicatedMasterEnabled\": false,\n        \"dedicatedMasterType\": \"N/A\",\n        \"instanceCount\": 1,\n        \"instanceType\": \"r6g.large.elasticsearch\",\n        \"warmCount\": 0,\n        \"warmEnabled\": false,\n        \"warmType\": \"N/A\",\n        \"zoneAwarenessEnabled\": false\n      },\n      \"tags\": [\n        {\n          \"key\": \"project\",\n          \"value\": \"Loghub\"\n        }\n      ],\n      \"storageType\": \"EBS\",\n      \"volume\": {\n        \"size\": 100,\n        \"type\": \"gp2\"\n      }\n      \"esVpc\": {\n        \"availabilityZones\": [\n          \"us-west-2b\"\n        ],\n        \"securityGroupIds\": [\n          \"sg-07cdfb011fba47e27\"\n        ],\n        \"subnetIds\": [\n          \"subnet-0f88a069\"\n        ],\n        \"vpcId\": \"vpc-538e702a\"\n      },  \n      \"metrics\": {\n        \"freeStorageSpace\": 1,\n        \"health\": \"GREEN\",\n        \"searchableDocs\": 1\n      },\n      \"alarmError\": \"\",\n      \"alarmInput\": {\n        \"email\": \"test@example.com\",\n        \"phone\": null,\n        \"alarms\": [\n          {\n            \"type\": \"CLUSTER_RED\",\n            \"value\": \"true\"\n          }\n        ]\n      },\n      \"alarmStatus\": \"ENABLED\",\n      \"proxyStatus\": \"ENABLED\",\n      \"proxyALB\": \"LogHu-LoadB-xxx.us-west-2.elb.amazonaws.com\",\n      \"proxyError\": \"\"\n      \"proxyInput\": {\n        \"certificateArn\": \"arn:aws:es:us-west-2:123456789012:domain/mycert\",\n        \"cognitoEndpoint\": \"\",\n        \"customEndpoint\": \"www.example.com\",\n        \"keyName\": \"my-key\",\n        \"vpc\": {\n          \"publicSubnetIds\": \"subnet-1234,subnet-1235\",\n          \"privateSubnetIds\": \"subnet-5678,subnet-5679\",\n          \"securityGroupId\": \"sg-1234\",\n          \"vpcId\": \"vpc-1234\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Cannot find domain in the imported list</li> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#get-domain-vpc","title":"Get Domain VPC","text":"<p>Type: Query</p> <p>Description:  Get VPC info of an Amazon OpenSearch domain in a region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description region String No current region To support cross region listing (in the same account) domainName String Yes Domain Name <p>Simple Request &amp; Response:</p> <p>Request with region </p> <pre><code>query example {\n  getDomainVpc(domainName: \"dev\", region: \"eu-west-1\") {\n    availabilityZones\n    securityGroupIds\n    subnetIds\n    vpcId\n  }\n}\n</code></pre> <p>Request without region</p> <pre><code>query example {\n  getDomainVpc(domainName: \"dev\") {\n    availabilityZones\n    securityGroupIds\n    subnetIds\n    vpcId\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getDomainVpc\": {\n      \"availabilityZones\": [\n        \"eu-west-1a\"\n      ],\n      \"securityGroupIds\": [\n        \"sg-07cdfb011fba47e27\"\n      ],\n      \"subnetIds\": [\n        \"subnet-0f88a069\"\n      ],\n      \"vpcId\": \"vpc-538e702a\"\n    }\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#create-proxy-for-opensearch","title":"Create Proxy For OpenSearch","text":"<p>Type: Mutation</p> <p>Description:  Create a Nginx Proxy for Amazon OpenSearch in vpc</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description customEndpoint String Yes Custom Domain to access Kibana cognitoEndpoint String No Cognito Domain for Amazon OpenSearch, blank if Amazon OpenSearch doesn't have cognito enabled id String Yes Amazon OpenSearch Domain Arn keyName String Yes? EC2 (nginx) key name vpc K-V Yes VPC for EC2 (nginx) certificateArn String Yes ACM certificate Arn for ELB <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  createProxyForOpenSearch(\n    nginx: {\n        vpc: {\n            securityGroupId: \"sg-1234\", \n            privateSubnetIds: \"subnet-1234,subnet-1235\",\n            publicSubnetIds: \"subnet-5678,subnet-5679\", \n            vpcId: \"vpc-1234\"\n        }, \n        certificateArn: \"arn:aws:es:us-west-2:123456789012:domain/mycert\", \n        keyName: \"my-key\",\n        cognitoEndpoint: \"hello.auth.us-west-2.amazoncognito.com\", \n        customEndpoint: \"www.example.com\",\n    }, \n\n    id: \"439239da8014f9a419c92b1b0c72a5fc\"\n  )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"createNginxProxyForOpenSearch\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#delete-proxy-for-opensearch","title":"Delete Proxy For OpenSearch","text":"<p>Type: Mutation</p> <p>Description:  Remove an Amazon OpenSearch Nginx Proxy Stack</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn (key in DynamoDB) <p>Request:</p> <pre><code>mutation example {\n  deleteProxyForOpenSearch(id: \"439239da8014f9a419c92b1b0c72a5fc\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteProxyForOpenSearch\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#create-alarm-for-opensearch","title":"Create Alarm For OpenSearch","text":"<p>Type: Mutation</p> <p>Description:  Create an Alarm for opensearch domain</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn email String Yes? Email to receive notification alarms List Yes List of k-v for alarm parameters phone String No Phone number to receive notification <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  createAlarmForOpenSearch(\n    id: \"439239da8014f9a419c92b1b0c72a5fc\", \n    input: {\n        email: \"test@example.com\", \n        alarms: [\n            {Type: CLUSTER_RED, value: \"true\"},\n            {Type: FREE_STORAGE_SPACE, value: \"20\"}\n            ...\n    })\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"createAlarmForOpenSearch\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#delete-alarm-for-opensearch","title":"Delete Alarm For OpenSearch","text":"<p>Type: Mutation</p> <p>Description:  Remove an Alarm Stack</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  deleteAlarmForOpenSearch(id: \"439239da8014f9a419c92b1b0c72a5fc\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteAlarmForOpenSearch\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#resource-apis","title":"Resource APIs","text":"<p>Resource APIs are a list of helper functions for AWS Resources that are used in the solution, such as listing VPCs etc.</p>"},{"location":"designs/domain-management/api-design/#list-resources","title":"List Resources","text":"<p>Type: Query</p> <p>Description:  List AWS Resources (Services) in current region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description type String Yes Available List: S3Bucket, VPC, Subnet, SecurityGroup, Certificate, KeyName, Trail parentId String No To filter by parent Id if any, if not provided, all are returned <p>Simple Request &amp; Response:</p> <p>Request for list S3Bucket</p> <pre><code>query example {\n  listResources(Type:S3Bucket) {\n    id\n    name\n    parentId\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"id\": \"bucketa\",\n        \"name\": \"bucketa\",\n        \"parentId\": null\n      },\n      {\n        \"id\": \"bucketb\",\n        \"name\": \"bucketb\",\n        \"parentId\": null\n      }\n    ]\n  }\n}\n</code></pre> <p>Request for list VPC Id</p> <pre><code>query example {\n  listResources(Type:VPC) {\n    id\n    name\n    parentId\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"id\": \"vpc-040e5096a29a457db\",\n        \"name\": \"test-vpc\",\n        \"parentId\": null\n      },\n      {\n        \"id\": \"vpc-538e702a\",\n        \"name\": \"default-vpc\",\n        \"parentId\": null\n      },\n      {\n        \"id\": \"vpc-1112456\",\n        \"name\": \"-\",\n        \"parentId\": null\n      }\n    ]\n  }\n}\n</code></pre> <p>Request for list Subnet Id</p> <pre><code>query example {\n  listResources(Type:Subnet, parentId: \"vpc-088c09a3e0b797406\") {\n    id\n    description\n    name\n    parentId\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"id\": \"subnet-00a5510951c6b4bad\",\n        \"description\": \"eu-west-1a\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/publicSubnet1\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-066f81646e30f0e48\",\n        \"description\": \"eu-west-1b\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/publicSubnet2\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-06c232cfb88789980\",\n        \"description\": \"eu-west-1a\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/privateSubnet1\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-04324df4484d33cfb\",\n        \"description\": \"eu-west-1b\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/privateSubnet2\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-065c2b45471b08568\",\n        \"description\": \"eu-west-1b\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/isolatedSubnet2\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-0934357dfce96eba3\",\n        \"description\": \"eu-west-1a\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/isolatedSubnet1\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      }\n    ]\n  }\n} \n</code></pre> <p>Request for list lambda functions</p> <pre><code>query example {\n  listResources(Type:Lambda) {\n    description\n    id\n    name\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"description\": \"Log Hub - Helper function to handle CloudFormation deployment\",\n        \"id\": \"LogHub-LogHubCfnFlowCfnHelperD9302B91-VZNJXLqIZjVu\",\n        \"name\": \"LogHub-LogHubCfnFlowCfnHelperD9302B91-VZNJXLqIZjVu-$LATEST\"\n      },\n      ...\n    ]\n  }\n}\n</code></pre> <p>Request for list RDS instances</p> <pre><code>query example {\n  listResources(Type:RDS) {\n    description\n    id\n    name\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"description\": \"/aws/rds/instance/database-1\",\n        \"id\": \"database-1\",\n        \"name\": \"database-1 (mysql)\"\n      },\n      {\n        \"description\": \"/aws/rds/cluster/demodb\",\n        \"id\": \"demodb-instance-1\",\n        \"name\": \"demodb-instance-1 (aurora-mysql)\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#put-resource-logging-bucket","title":"Put Resource Logging Bucket","text":"<p>Type: Mutation</p> <p>Description:  Put Logging bucket for resource in current region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description type String Yes Available List: S3Bucket, CloudFront resourceName String Yes The resource name or ID <p>Simple Request &amp; Response:</p> <p>Request</p> <pre><code>query example {\n  putResourceLoggingBucket(resourceName: \"test-bucket\", Type: S3Bucket) {\n    bucket\n    prefix\n    enabled\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"putResourceLoggingBucket\": {\n      \"bucket\": \"loghub-loghubloggingbucket0fa53b76-mkvj68ix2ufo\",\n      \"prefix\": \"s3/test-bucket/\",\n      \"enabled\": true\n    }\n  }\n} \n</code></pre>"},{"location":"designs/domain-management/api-design/#get-resource-logging-bucket","title":"Get Resource Logging Bucket","text":"<p>Type: Query</p> <p>Description:  Get Logging bucket for resource in current region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description type String Yes Available List: S3Bucket, Trail resourceName String Yes The resource name or ID <p>Simple Request &amp; Response:</p> <p>Request</p> <pre><code>query example {\n  getResourceLoggingBucket(Type: Trail, resourceName: \"testtrail\") {\n    bucket\n    prefix\n    enabled\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getResourceLoggingBucket\": {\n      \"bucket\": \"aws-cloudtrail-logs-123456789012-222dcf7b\",\n      \"prefix\": \"AWSLogs/123456789012/CloudTrail/\",\n      \"enabled: true\n    }\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/component-design/","title":"Domain Management Component Design","text":""},{"location":"designs/domain-management/component-design/#overview","title":"Overview","text":"<p>Log Hub solution uses Amazon OpenSearch service (AOS) as the underlying engine to store and analyze logs. This component consists a list of operations on top of AOS domains.</p> <p>This document is to describe this component is designed.</p> <p>Info</p> <p>For more information about solution overall design, refer to High Level Design.</p>"},{"location":"designs/domain-management/component-design/#component-design","title":"Component Design","text":""},{"location":"designs/domain-management/component-design/#high-level-architecture","title":"High-Level Architecture","text":"<p>This component contains two sub components.</p> <ul> <li> <p>Proxy for AOS</p> <p>As OpenSearch Dashboards is within VPC and has no public accesses. Customer can choose to deploy a proxy stack to access the OpenSearch Dashboards from internet with a custom domain.</p> <p>Below is the high level architecture diagram:</p> <p></p> <p>The process is described as below:</p> <ol> <li> <p>Customer accesses custom domain for the proxy, the domain needs to be resolved via DNS service (for example, using Route 53 on AWS)</p> </li> <li> <p>The DNS service routes the traffic to internet-facing Application Load Balancer (ALB)</p> </li> <li> <p>The ALB distributes web traffic to backend Nginx server running on Amazon EC2 within Auto Scaling Group. </p> </li> <li> <p>The Nginx server redirects the requests to OpenSearch Dashboards.</p> </li> <li> <p>(optional) VPC peering is required if the VPC for the proxy is not the same one as the OpenSearch service.</p> </li> </ol> <p>Info</p> <p>This stack can be deployed independently without the UI, check more details about the CloudFormation Design</p> </li> <li> <p>Alarm for AOS</p> <p>There are a list of recommended CloudWatch alarms to be set up for Amazon OpenSearch Service. For example, to sent an email if the cluster health status is red for longer than one minute. Customer can choose to deploy an Alarm stack with one click to set up alarms in AWS.</p> <p>Below is the high level architecture diagram:</p> <p></p> <p>Info</p> <p>This stack can be deployed indepentdantly without the UI, check more details about the CloudFormation Design</p> <p>The process is described as below:</p> <ol> <li> <p>CloudWatch Alarm to monitor Amazon OpenSearch service and send state change event to Amazon EventBridge</p> </li> <li> <p>Amazon EventBridge rule to trigger and send information to Amazon SNS as target</p> </li> <li> <p>Amazon SNS uses Email as subscription and notifies Administrators</p> </li> </ol> </li> </ul>"},{"location":"designs/domain-management/component-design/#process-design","title":"Process Design","text":"<p>This components includes a list of processes for domain management.</p> <p>For details about how the processes are designed, please refer to Process Design</p>"},{"location":"designs/domain-management/component-design/#api-design","title":"API Design","text":"<p>This solution uses GraphQL APIs built on AWS Appsync service.</p> <p>For details about how the backend APIs are designed, please refer to API Design</p>"},{"location":"designs/domain-management/component-design/#data-model-design","title":"Data Model Design","text":"<p>This component uses Amazon DynamoDB as the backend NoSQL database to store the information about AOS domains.</p> <p>To learn more information about how the data model is designed, please refer to Data Model Design</p>"},{"location":"designs/domain-management/component-design/#cloudformation-design","title":"CloudFormation Design","text":"<ul> <li>Proxy for AOS CloudFormation Design</li> </ul> <p>The parameters in the CloudFormation template are listed as below:</p> Parameter Default Description VPCId <code>&lt;Requires input&gt;</code> The VPC to deploy the Nginx proxy resources, for example, <code>vpc-bef13dc7</code>. PublicSubnetIds <code>&lt;Requires input&gt;</code> The public subnets where ELB are deployed. You need to select at least two public subnets, for example, <code>subnet-12345abc, subnet-54321cba</code>. ELBSecurityGroupId <code>&lt;Requires input&gt;</code> The Security group being associated with the ELB, for example, <code>sg-123456</code>. ELBDomain <code>&lt;Requires input&gt;</code> The custom domain name of the ELB, for example, <code>dashboard.example.com</code>. ELBDomainCertificateArn <code>&lt;Requires input&gt;</code> The SSL certificate ARN associated with the ELBDomain. The certificate must be created from [Amazon Certificate Manager (ACM)][acm]. PrivateSubnetIds <code>&lt;Requires input&gt;</code> The private subnets where Nginx instances are deployed. You need to select at least two private subnets, for example, <code>subnet-12345abc, subnet-54321cba</code>. NginxSecurityGroupId <code>&lt;Requires input&gt;</code> The Security group associated with the Nginx instances. The security group must allow access from ELB security group. KeyName <code>&lt;Requires input&gt;</code> The PEM key name of the Nginx instances. EngineType OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint, for example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code>. CognitoEndpoint <code>&lt;Optional&gt;</code> The Cognito User Pool endpoint URL of the OpenSearch domain, for example, <code>mydomain.auth.us-east-1.amazoncognito.com</code>. Leave empty if your OpenSearch domain is not authenticated through Cognito User Pool. <ul> <li>Alarm for AOS CloudFormation Design</li> </ul> <p>The parameters in the CloudFormation template are listed as below:</p> Parameter Default Description Endpoint <code>&lt;Requires input&gt;</code> The endpoint of the OpenSearch domain, for example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code>. DomainName <code>&lt;Requires input&gt;</code> The name of the OpenSearch domain. Email <code>&lt;Requires input&gt;</code> The notification email address. Alarms will be sent to this email address via SNS. ClusterStatusRed <code>Yes</code> Whether to enable alarm when at least one primary shard and its replicas are not allocated to a node. ClusterStatusYellow <code>Yes</code> Whether to enable alarm when at least one replica shard is not allocated to a node. FreeStorageSpace <code>10</code> Whether to enable alarm when a node in your cluster is down to the free storage space you typed in GiB. We recommend setting it to 25% of the storage space for each node. <code>0</code> means the alarm is disabled. ClusterIndexWritesBlocked <code>1</code> Index writes blocked error occurs for &gt;= x times in 5 minutes, 1 consecutive time. Input <code>0</code> to disable this alarm. UnreachableNodeNumber <code>3</code> Nodes minimum is &lt; x for 1 day, 1 consecutive time. <code>0</code> means the alarm is disabled. AutomatedSnapshotFailure <code>Yes</code> Whether to enable alarm when automated snapshot failed. AutomatedSnapshotFailure maximum is &gt;= 1 for 1 minute, 1 consecutive time. CPUUtilization <code>Yes</code> Whether to enable alarm when sustained high usage of CPU occurred. CPUUtilization or WarmCPUUtilization maximum is &gt;= 80% for 15 minutes, 3 consecutive times. JVMMemoryPressure <code>Yes</code> Whether to enable alarm when JVM RAM usage peak occurred. JVMMemoryPressure or WarmJVMMemoryPressure maximum is &gt;= 80% for 5 minutes, 3 consecutive times. MasterCPUUtilization <code>Yes</code> Whether to enable alarm when sustained high usage of CPU occurred in master nodes. MasterCPUUtilization maximum is &gt;= 50% for 15 minutes, 3 consecutive times. MasterJVMMemoryPressure <code>Yes</code> Whether to enable alarm when JVM RAM usage peak occurred in master nodes. MasterJVMMemoryPressure maximum is &gt;= 80% for 15 minutes, 1 consecutive time. KMSKeyError <code>Yes</code> Whether to enable alarm when KMS encryption key is disabled. KMSKeyError is &gt;= 1 for 1 minute, 1 consecutive time. KMSKeyInaccessible <code>Yes</code> Whether to enable alarm when KMS encryption key has been deleted or has revoked its grants to OpenSearch Service. KMSKeyInaccessible is &gt;= 1 for 1 minute, 1 consecutive time."},{"location":"designs/domain-management/data-model-design/","title":"Domain Management Data Model Design","text":""},{"location":"designs/domain-management/data-model-design/#overview","title":"Overview","text":"<p>This component uses Amazon DynamoDB as the backend NoSQL database. This document is about the Data Model Design for Domain Management component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/domain-management/data-model-design/#cluster-table","title":"Cluster Table","text":"<p>Cluster table is used to store basic information on imported AOS domain.</p> <p>The data attributes are listed as below:</p> Attribute name Type Example Description Comments id String 439239da8014f9a419c92b1b0c72a5fc MD5 of OpenSearch domain ARN Partition key version String 1.0 OpenSearch Version engine String OpenSearch Either OpenSearch or Elasticsearch region String us-east-1 AWS region endpoint String vpc-dev-i5jwvhie5lzhsfvnxapny.us-east-1.es.amazonaws.com OpenSearch Endpoint domainArn String arn:aws:es:us-east-1:123456789012:domain/dev OpenSearch domain ARN domainName String dev OpenSearch domain name importedDt String 2021-12-20T05:37:20.523951 Date of import proxyStatus String ENABLED OpenSearch Proxy stack status proxyALB String LogHu-LoadB-1T8YLOO675OCN-1782845820.us-east-1.elb.amazonaws.com ELB url for OpenSearch Proxy stack proxyStackId String arn:aws:cloudformation:us-east-1:123456789012:stack/LogHub-Proxy-14682/1b492000-615e-11ec-b5e4-1213fdb3e837 OpenSearch Proxy stack ID proxyInput Map {...} Parameters used when deploy a proxy stack for OpenSearch proxyError String Error messages when deploy a proxy stack for OpenSearch alarmStatus String DISABLED OpenSearch Alarm stack status vpc Map {...} Processing layer VPC when importing domain tags List [{...}] Custom Tags"},{"location":"designs/domain-management/eks-auto-import/","title":"Automatically Import AOS Domain Process","text":""},{"location":"designs/domain-management/eks-auto-import/#overview","title":"Overview","text":"<p>This document is about the process design for Automatically Import AOS Domain method.</p> <p></p>"},{"location":"designs/domain-management/process-design/","title":"Domain Management Process Design","text":""},{"location":"designs/domain-management/process-design/#overview","title":"Overview","text":"<p>This document is about the Process Design for Domain Management component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/domain-management/process-design/#domain-management-process","title":"Domain Management Process","text":""},{"location":"designs/domain-management/process-design/#import-domain","title":"Import Domain","text":"<p>This operation is to import an existing AOS domain into Log Hub solution to ingest logs to.</p> <p>The process to import domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Store AOS information in DynamoDB.</p> <p>When a domain is imported, basic domain information is stored in Cluster table in DynamoDB. </p> <p>Consider only information that can not/might not be changed, such as endpoint, vpc etc. Other information such as EBS volume, Node information can be resized hence is derived via OpenSearch SDK on demand.</p> <p>Solution specified information such as processing layer vpc, tags is also stored in Cluster table.</p> <p>The domain metrics such as free storage are also not stored in Cluster table, as they are changing all the time.</p> </li> <li> <p>Domain ID design</p> <p>Considering that we will support cross account and cross region log ingestion in future, the unique domain ID (partition key in DynamoDB table) must support this.</p> <p>In this solution, we use MD5 of the domain ARN as the domain ID (Assumption is that it's unlikely that two different domain ARNs can have the same MD5 string)</p> </li> <li> <p>Exception handling</p> <p>When a domain is deleting or creating, or a domain is with public network, the import must fail with expection.</p> <p>When importing a domain that is already imported, the import should fail. To avoid such case happening, from frontend, when customer choose from a drop down list of OpenSearch domains, imported domains are excluded from the list.</p> <p>When a domain is already imported and then deleted. the list should not fail.</p> </li> </ol> <p>References:</p> <ul> <li>Import Domain API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#remove-domain","title":"Remove Domain","text":"<p>This operation is to remove an imported AOS domain from Log Hub rather than deleting the AOS domain.</p> <p>The process to remove domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Delete process</p> <p>Deleting an imported domain only removes the item from Cluster table in DynamoDB. The backend OpenSearch domain will not be affected.</p> <p>Also deleting domain will not impact any existing log ingestion pipelines.</p> <p>There is no need to implement soft delete for this process. Customer can easily re-import the domain as needed.</p> </li> </ol> <p>References:</p> <ul> <li>Remove Domain API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#list-imported-domains","title":"List Imported Domains","text":"<p>This operation is to support listing of all the imported domain along with key metrics (Cluster Health, Free Storage Space, Searchable documents).</p> <p>The process to import domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Where to get AOS metrics</p> <p>There are two ways of getting AOS domain metrics such as free storage space, domain health etc. One is to use AOS REST API such as <code>GET _cluster/health</code>, the other is to query in CloudWatch metrics.</p> <p>In this design, CloudWatch metrics are chosen to get domain metrics for two reasons:</p> <p>a) The API backend Lambda doesn't need to have VPC access.</p> <p>b) The CloudWatch metric data is same as what is shown in the AWS Management console.</p> </li> <li> <p>Not all calls need metrics</p> <p>Getting metrics from CloudWatch takes time and bring extra costs. But not all the listing scenerios requires metrics, For example, when customer is choosing an imported domain as a destionation from a list.</p> <p>So an option to choose whether to include metrics is provided. </p> </li> </ol> <p>References:</p> <ul> <li>List Imported Domains API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#get-domain-details","title":"Get Domain Details","text":"<p>This operation is to provide more details about an imported AOS domain, including nodes, volumes, networks etc.</p> <p>The process to Get domain details is described in below diagram:</p> <p></p> <p>Design consideration:</p> <p>Same as List Imported Domain</p> <p>References:</p> <ul> <li>Get Domain Details API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#proxy-for-aos","title":"Proxy for AOS","text":"<p>This operation is to provide a proxy to access OpenSearch dashboards which is within VPC.</p> <p>The process to Create/Delete Proxy for AOS Domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Step Functions Design</p> <p>Use an independent CloudFormation stack to provision resources (such as EC2, ELB etc.) for proxy stacks. Create a reusable Child step function flow for orchestrating the deployment/delete of the sub-stack. When create/delete proxy is triggerred, a parent flow will trigger the child flow to run, and once child flow is completed, the parent flow will be informed will the result and update the status in Cluster table.</p> </li> <li> <p>Store Proxy information in DynamoDB.</p> <p>Store the related parameter key-values of proxy stack in cluster table in DynamoDB. When the proxy is created, the stack id is stored in the table as the stack id is required in order to delete the proxy stack.</p> </li> </ol> <p>References:</p> <ul> <li>Create Proxy for AOS API</li> <li>Delete Proxy for AOS API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#alarm-for-aos","title":"Alarm for AOS","text":"<p>This operation is to quicly create recommended CloudAlarms to monitor AOS. An email notification will be triggered for alarm.</p> <p>The process to Create/Delete Alarm for AOS Domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Step Functions Design</p> <p>Use an independent CloudFormation stack to provision resources (such as CloudWatch alarms, SNS topic etc.) for alarm stacks. Create a reusable Child step function flow for orchestrating the deployment/delete of the sub-stack. When create/delete alarm is triggerred, a parent flow will trigger the child flow to run, and once child flow is completed, the parent flow will be informed will the result and update the status in Cluster table.</p> </li> <li> <p>Store Proxy information in DynamoDB.</p> <p>Store the related parameter key-values of proxy stack in cluster table in DynamoDB. When the alarm is created, the stack id is stored in the table as the stack id is required in order to delete the alarm stack.</p> </li> </ol> <p>References:</p> <ul> <li>Create Alarm for AOS API</li> <li>Delete Alarm for AOS API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/eks-log/api-design/","title":"EKS Log API Design","text":""},{"location":"designs/eks-log/api-design/#eks-log-source-apis","title":"EKS Log Source APIs","text":"<p>This document is about API design of EKS cluster as log source component.</p>"},{"location":"designs/eks-log/api-design/#list-eks-cluster-names","title":"list EKS cluster names","text":"<p>Type: Query</p> <p>Description: Display cluster names for all regions</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description nextToken String No The token for pagination isListAll Boolean No false Whether to show EKS clusters in all regions <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n  listEKSClusterNames(nextToken: \"\", isListAll: false) {\n    clusters\n    nextToken\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listEKSClusterNames\": {\n      \"clusters\": [\n        \"eks-demo\",\n        \"loghub\"\n      ],\n      \"nextToken\": null\n    }\n  }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#import-an-eks-cluster","title":"Import an EKS cluster","text":"<p>Type: Mutation</p> <p>Description: Import an EKS cluster used as log source in solution</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description aosDomainId String Yes The imported AOS domain id. deploymentKind String Yes The deployment type fo the log agent, DaemonSet or SideCar eksClusterName enum Yes The imported EKS cluster name. accountId enum No The account id corresponding to the imported EKS cluster. cri String No K8s container runtime. region String No The region name corresponding to the imported EKS cluster. tags K-V No Tag the EKS Cluster log source. <p>Simple Request &amp; Response:</p> <pre><code>query example{\n  importEKSCluster(\n      aosDomainId: \"7d43abe07ebb4c90af0d8619328054\",\n      deploymentKind: \"DaemonSet\",\n      eksClusterName: \"eks-demo-cluster\",\n      accountId: \"20599832\",\n      cri: \"containerd\",\n      region: \"us-west-2\",\n      tags: {\n          key: \"evn\",\n          value: \"Testing\"\n        }\n    )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"importEKSCluster\": \"OK\"\n  }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#list-imported-eks-clusters","title":"list imported EKS clusters","text":"<p>Type: Query</p> <p>Description: List imported EKS cluster</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    listImportedEKSClusters (page: 1, count: 10) {\n        eksClusterLogSourceList {\n            id\n            aosDomain {\n                id\n                domainName\n                engine\n                version\n                endpoint\n                metrics {\n                    searchableDocs\n                    freeStorageSpace\n                    health\n                }\n            }\n            eksClusterName\n            eksClusterArn\n            cri\n            vpcId\n            eksClusterSGId\n            subnetIds\n            oidcIssuer\n            endpoint\n            createdDt\n            accountId\n            region\n            logAgentRoleArn\n            deploymentKind\n            tags {\n                key\n                value\n            }\n        }\n        total\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listImportedEKSClusters\": {\n            \"eksClusterLogSourceList\": [\n                {\n                    \"id\": \"e83aa65ef40e4a8b883dff33af483e36\",\n                    \"aosDomain\": {\n                        \"id\": \"439239da8014f9a419c92b1b0c72a5fc\",\n                        \"domainName\": \"dev\",\n                        \"engine\": \"OpenSearch\",\n                        \"version\": \"1.0\",\n                        \"endpoint\": \"vpc-dev-xxx.eu-west-1.es.amazonaws.com\",\n                        \"metrics\": null\n                    },\n                    \"eksClusterName\": \"test\",\n                    \"eksClusterArn\": \"arn:aws:eks:eu-west-1:123456789012:cluster/test\",\n                    \"cri\": \"docker\",\n                    \"vpcId\": \"vpc-0123\",\n                    \"eksClusterSGId\": \"sg-1234\",\n                    \"subnetIds\": [\n                        \"subnet-1234\",\n                        \"subnet-5678\",\n                        ...\n                    ],\n                    \"oidcIssuer\": \"https://oidc.eks.eu-west-1.amazonaws.com/id/ABC\",\n                    \"endpoint\": \"https://ABC.sk1.eu-west-1.eks.amazonaws.com\",\n                    \"createdDt\": \"2022-10-29T07:52:48Z\",\n                    \"accountId\": \"123456789012\",\n                    \"region\": \"eu-west-1\",\n                    \"logAgentRoleArn\": \"arn:aws:iam::123456789012:role/LogHub-EKS-LogAgent-Role-3623ec2044264a2189416bcaaa7ee948\",\n                    \"deploymentKind\": \"DaemonSet\",\n                    \"tags\": []\n                },\n                ...\n            ],\n            \"total\": 2\n        }\n    }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#get-imported-eks-cluster-details","title":"Get imported EKS cluster details","text":"<p>Type: Query</p> <p>Description: Display details of an imported eks cluster</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description eksClusterId String Yes log source id for the imported EKS Cluster <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    getEKSClusterDetails (eksClusterId: \"e83aa65ef40e4a8b883dff33af483e36\") {\n        id\n        aosDomain {\n            id\n            domainName\n            engine\n            version\n            endpoint\n            metrics {\n                searchableDocs\n                freeStorageSpace\n                health\n            }\n        }\n        eksClusterName\n        eksClusterArn\n        cri\n        vpcId\n        eksClusterSGId\n        subnetIds\n        oidcIssuer\n        endpoint\n        createdDt\n        accountId\n        region\n        logAgentRoleArn\n        deploymentKind\n        tags {\n            key\n            value\n        }\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getEKSClusterDetails\": {\n            \"id\": \"e83aa65ef40e4a8b883dff33af483e36\",\n            \"aosDomain\": {\n                \"id\": \"439239da8014f9a419c92b1b0c72a5fc\",\n                \"domainName\": \"dev\",\n                \"engine\": \"OpenSearch\",\n                \"version\": \"1.0\",\n                \"endpoint\": \"vpc-dev-xxx.eu-west-1.es.amazonaws.com\",\n                \"metrics\": null\n            },\n            \"eksClusterName\": \"test\",\n            \"eksClusterArn\": \"arn:aws:eks:eu-west-1:123456789012:cluster/test\",\n            \"cri\": \"docker\",\n            \"vpcId\": \"vpc-0123\",\n            \"eksClusterSGId\": \"sg-1234\",\n            \"subnetIds\": [\n                \"subnet-1234\",\n                \"subnet-5678\",\n                ...\n            ],\n            \"oidcIssuer\": \"https://oidc.eks.eu-west-1.amazonaws.com/id/ABC\",\n            \"endpoint\": \"https://ABC.sk1.eu-west-1.eks.amazonaws.com\",\n            \"createdDt\": \"2022-10-29T07:52:48Z\",\n            \"accountId\": \"123456789012\",\n            \"region\": \"eu-west-1\",\n            \"logAgentRoleArn\": \"arn:aws:iam::123456789012:role/LogHub-EKS-LogAgent-Role-3623ec2044264a2189416bcaaa7ee948\",\n            \"deploymentKind\": \"DaemonSet\",\n            \"tags\": []\n        }\n    }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#eks-log-deployment-apis","title":"EKS Log Deployment APIs","text":""},{"location":"designs/eks-log/api-design/#deploy-as-daemonset","title":"Deploy as DaemonSet","text":"<p>Type: Query</p> <p>Description: Display kubernetes deployment details in yaml for DaemonSet</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description eksClusterId String Yes log source id for the imported EKS Cluster <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example  {\n    getEKSDaemonSetConf (eksClusterId: \"a\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getEKSDaemonSetConf\": \"...\"\n    }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#deploy-as-side-car","title":"Deploy as Side Car","text":"<p>Type: Query</p> <p>Description: Display kubernetes deployment details in yaml for Side Car</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description eksClusterId String Yes log source id for the imported EKS Cluster ingestionId String Yes the Id for application log ingestion <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n  getEKSDeploymentConfig(\n      eksClusterId: \"1d431ccd7caa4c90af0d86193bf78f9a\",\n      ingestionId: \"2d43abe07caa4c90af0d8619323064\"\n      )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getEKSDeploymentConfig\": \"...\"\n  }\n}\n</code></pre>"},{"location":"designs/eks-log/architecture-design/","title":"EKS Cluster Log Analytics Design","text":""},{"location":"designs/eks-log/architecture-design/#overview","title":"Overview","text":"<p>Considering that there are many types of logs in EKS, and different types of logs have different collection sources. But in general, it can be divided into three ways:</p> <ul> <li> <p>The first type of logs is Control plane logging, which includes Audit logs, API server logs, interfacing with CloudWatch, and Controller manager logs. Such logs need to be subscribed in the log group (/aws/eks//cluster) , transfer the newly generated log data to the KDS generated by Log Hub EKS Cluster log Ingestion. We are currently working on Control plane logging. <li> <p>The second is the ELB type Ingress access log. If your ELB type is ALB, this part has been implemented in the Service Log component</p> </li> <li> <p>The last one is the application log in the EKS cluster and the system log on the Node (kube-proxy-XXX.log, aws-load-balancer-controller, aws-node-_kube-system_aws-vpc-cni- init-XXX.log, aws-node-_kube-system_aws-node-XXX.log, coredns-XXX.log). Regarding the use of Nginx as an Ingress, the Ingress access log generated is essentially an application log. Regarding such logs, we will collect the logs by deploying Fluent Bit as a log agent and send the logs to the data buffer created by Log Hub."},{"location":"designs/eks-log/architecture-design/#system-architecture","title":"System Architecture","text":""},{"location":"designs/eks-log/architecture-design/#faq","title":"FAQ","text":"<p>Q. What is the difference between the log pipeline of an EKS cluster and the log pipeline of an application?</p> <p>Considering whether the application is deployed in EC2 or EKS Cluster, there is usually no difference in the application log format, but the analysis methods for logs from different sources are still different. When we designed EKS Cluster log analysis, we abstracted the concept of ==log source== in the application log ingestion component. Therefore, EKS Cluster's log pipeline still uses the application log pipeline. When an EKS Cluster log pipeline is created, a log ingestion is created together</p> <p>Q. Which deployment mode does the log agent support?</p> <p>Whether you choose the DaemonSet type or SideCar, the Log Hub supports\u3002</p> <p>Q. Is the deployment of the log agent performed automatically by the system?</p> <p>We do not support fully automated deployment yet. Currently, the Log Hub will generate a Yaml file for deployment, so that you can make changes according to your actual scenarios. In the future we will do this through the Lambda with a Kubectl environment.</p>"},{"location":"designs/eks-log/process-design/","title":"EKS Cluster Log Analytics Process","text":"<p>This document is about the Process Design for Import EKS Cluster, Create EKS Cluster Log Pipeline and Ingestion. </p>"},{"location":"designs/eks-log/process-design/#overview","title":"Overview","text":""},{"location":"designs/eks-log/process-design/#import-an-eks-cluster","title":"Import an EKS Cluster","text":""},{"location":"designs/eks-log/process-design/#collect-control-plane-logging","title":"Collect Control plane logging","text":""},{"location":"designs/eks-log/process-design/#request-to-create-pipeline-and-ingestion-for-collecting-eks-cluster-application-logs","title":"Request to create pipeline and ingestion for collecting EKS cluster application logs","text":"<p>Sequence diagram: Request to create pipeline</p> <p></p> <p>Sequence diagram: The StepFunction Process</p> <p></p> <p>Sequence diagram: Create an ingested  from an existing pipeline</p> <p></p>"},{"location":"designs/service-log/api-design/","title":"Service Log Pipeline API Design","text":""},{"location":"designs/service-log/api-design/#overview","title":"Overview","text":"<p>This document is about the API Design for Service Log Pipeline component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/service-log/api-design/#service-pipeline-apis","title":"Service Pipeline APIs","text":"<p>Service Pipeline APIs are a list of operations to manage end to end Log analytics pipelines for AWS services.</p>"},{"location":"designs/service-log/api-design/#create-service-pipeline","title":"Create Service Pipeline","text":"<p>Type: Mutation</p> <p>Description:  Create a record in DynamoDB, start an execution of Step function,  trigger CloudFormation template to run</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description type String Yes Allowed values: S3AccessLog, CloudTrail, CloudFront parameters K-V Yes Source info (such as S3 bucket, prefix) tags K-V No Tag the pipeline <p>Request:</p> <pre><code>{\n  createServicePipeline(\n    Type: S3, \n    source: \"aws-lambda-12843845950\", \n    target: \"dev\", \n    tags: [{key: \"Hello\", value: \"World\"}], \n    parameters: [\n    {parameterKey: \"engineType\", parameterValue: \"OpenSearch\"},\n    {parameterKey: \"logBucketName\", parameterValue: \"loghub-loghubloggingbucket0fa53b76-1cf5iuchzpbz8\"},\n    {parameterKey: \"logBucketPrefix\", parameterValue: \"AWSLogs/347283850106/s3/aws-lambda-12843845950\"},\n    {parameterKey: \"endpoint\", parameterValue: \"vpc-dev-ardonphnbg327lwqncuj2vps3q.eu-west-1.es.amazonaws.com\"}\n    {parameterKey: \"domainName\", parameterValue: \"dev\"},\n    {parameterKey: \"indexPrefix\", parameterValue: \"aws-lambda-12843845950\"},\n    {parameterKey: \"createDashboard\", parameterValue: \"Yes\"},\n    {parameterKey: \"vpcId\", parameterValue: \"vpc-0e172e182aa53806b\"},\n    {parameterKey: \"subnetIds\", parameterValue: \"subnet-09f0654b6db09eb23,subnet-0b873d0b6e73c2f9c\"},\n    {parameterKey: \"securityGroupId\", parameterValue: \"sg-0a55e5364049a5b1d\"},\n    {parameterKey: \"backupBucketName\", parameterValue: \"loghub-loghubloggingbucket0fa53b76-1cf5iuchzpbz8\"},\n    {parameterKey: \"daysToWarm\", parameterValue: \"0\"},\n    {parameterKey: \"daysToCold\", parameterValue: \"0\"},\n    {parameterKey: \"daysToRetain\", parameterValue: \"0\"}\n    ])\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"createServicePipeline\": \"24483703-41b6-43ba-aae3-19318bdb1b4e\"\n  }\n}\n</code></pre>"},{"location":"designs/service-log/api-design/#delete-service-pipeline","title":"Delete Service Pipeline","text":"<p>Type: Mutation</p> <p>Description:  mask the record in DynamoDB as Inactive, start an execution of Step function,  trigger CloudFormation template to delete</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Pipeline Unique ID in DynamodB <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  deleteServicePipeline(id: \"24483703-41b6-43ba-aae3-19318bdb1b4e\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteServicePipeline\": \"OK\"\n  }\n}\n</code></pre>"},{"location":"designs/service-log/api-design/#list-service-pipelines","title":"List Service Pipelines","text":"<p>Type: Query</p> <p>Description:  List all pipelines</p> <p>Resolver: DynamoDB</p> <p>Parameters:</p> Name Type Required Default Description page Int No 1 page number, start from 1 count String No 20 number of records per page <p>Simple Request &amp; Response:</p> <p>Request</p> <pre><code>query example {\n  listServicePipelines(page: 1, count: 20) {\n    pipelines {\n      createdDt\n      id\n      source\n      status\n      target\n      type\n    }\n    total\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listServicePipelines\": {\n      \"pipelines\": [\n        {\n          \"createdDt\": \"2021-09-16T04:12:06.288536\",\n          \"id\": \"f2272d96-5cb5-4eed-9d1e-bbe545cfa181\",\n          \"source\": \"abc-bucket\",\n          \"status\": \"ACTIVE\",\n          \"target\": \"dev\",\n          \"type\": \"S3\"\n        },\n        {\n          \"createdDt\": \"2021-09-16T04:12:04.216817\",\n          \"id\": \"f2b845fa-be44-4b94-9912-e692d2bc270d\",\n          \"source\": \"bcd-bucket\",\n          \"status\": \"ERROR\",\n          \"target\": \"dev\",\n          \"type\": \"S3\"\n        },\n        ...\n      ],\n      \"total\": 166\n    }\n  }\n}\n</code></pre>"},{"location":"designs/service-log/api-design/#get-service-pipeline","title":"Get Service Pipeline","text":"<p>Type: Query</p> <p>Description:  Get service pipeline detail by ID</p> <p>Resolver: DynamoDB</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Unique pipeline ID <p>Simple Request &amp; Response:</p> <p>Request</p> <pre><code>query example {\n  getServicePipeline(id: \"d3b88a26-38ab-430a-9b20-1dd009586e22\") {\n    createdDt\n    id\n    parameters {\n      parameterKey\n      parameterValue\n    }\n    status\n    tags {\n      key\n      value\n    }\n    type\n    source\n    target\n    error\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getServicePipeline\": {\n      \"createdDt\": \"21-12-09T07:54:17Z\",\n      \"id\": \"d3b88a26-38ab-430a-9b20-1dd009586e22\",\n      \"parameters\": [\n        {\n          \"parameterKey\": \"engineType\",\n          \"parameterValue\": \"OpenSearch\"\n        },\n        {\n          \"parameterKey\": \"logBucketName\",\n          \"parameterValue\": \"loghub-loghubloggingbucket0fa53b76-1cf5iuchzpbz8\"\n        },\n        {\n          \"parameterKey\": \"logBucketPrefix\",\n          \"parameterValue\": \"AWSLogs/347283850106/s3/aws-lambda-12843845950\"\n        },\n        {\n          \"parameterKey\": \"endpoint\",\n          \"parameterValue\": \"vpc-dev-ardonphnbg327lwqncuj2vps3q.eu-west-1.es.amazonaws.com\"\n        },\n        {\n          \"parameterKey\": \"domainName\",\n          \"parameterValue\": \"dev\"\n        },\n        {\n          \"parameterKey\": \"indexPrefix\",\n          \"parameterValue\": \"aws-lambda-12843845950\"\n        },\n        {\n          \"parameterKey\": \"createDashboard\",\n          \"parameterValue\": \"Yes\"\n        },\n        {\n          \"parameterKey\": \"vpcId\",\n          \"parameterValue\": \"vpc-0e172e182aa53806b\"\n        },\n        {\n          \"parameterKey\": \"subnetIds\",\n          \"parameterValue\": \"subnet-09f0654b6db09eb23,subnet-0b873d0b6e73c2f9c\"\n        },\n        {\n          \"parameterKey\": \"securityGroupId\",\n          \"parameterValue\": \"sg-0a55e5364049a5b1d\"\n        },\n        {\n          \"parameterKey\": \"backupLogBucketName\",\n          \"parameterValue\": \"loghub-loghubloggingbucket0fa53b76-1cf5iuchzpbz8\"\n        },\n        {\n          \"parameterKey\": \"daysToWarm\",\n          \"parameterValue\": \"0\"\n        },\n        {\n          \"parameterKey\": \"daysToCold\",\n          \"parameterValue\": \"0\"\n        },\n        {\n          \"parameterKey\": \"daysToRetain\",\n          \"parameterValue\": \"0\"\n        }\n      ],\n      \"status\": \"ERROR\",\n      \"tags\": [\n        {\n          \"key\": \"Hello\",\n          \"value\": \"World\"\n        }\n      ],\n      \"type\": \"S3\",\n      \"source\": \"aws-lambda-12843845950\",\n      \"target\": \"dev\",\n      \"error\": \"An error occurred (ValidationError) when calling the CreateStack operation: Parameters: [failedLogBucket] must have values\"\n    }\n  }\n}\n</code></pre>"},{"location":"designs/service-log/component-design/","title":"Service Log Pipeline Component Design","text":""},{"location":"designs/service-log/component-design/#overview","title":"Overview","text":"<p>Service Log Analytics Pipeline, as one component of Log Hub solution, is used to collect logs for AWS services, process and ingest into Amazon OpenSearch Service (AOS). This document is to describe this component is designed.</p> <p>Currently, this solution supports S3 Access Logs, CloudTrail Logs, ELB Logs, CloudFront Logs, RDS Logs, WAF Logs, Lambda Logs. To learn how to extend this solution to support more service logs, please check Tutorial: Extend Service Logs</p> <p>Info</p> <p>For more information about solution overall design, refer to High Level Design.</p>"},{"location":"designs/service-log/component-design/#component-design","title":"Component Design","text":""},{"location":"designs/service-log/component-design/#high-level-architecture","title":"High-Level Architecture","text":"<p>Based on different log destinations, Different architectures are used. </p> <ul> <li> <p>Destination on Amazon S3</p> <p>Normally the logs on Amazon S3 are not for real-time analysis. Currently, this solution supports CloudTrail logs, CloudFront Standard logs, Amazon S3 Access logs, Elastic Load Balancing (ELB) logs, VPC Flow logs, and Amazon Config logs.</p> <p></p> <p>The process is described as below:</p> <ol> <li> <p>AWS Services store logs on Amazon S3 bucket</p> </li> <li> <p>A notification is sent to Amazon SQS when new log file is created</p> </li> <li> <p>Amazon SQS triggers the Lambda (Log processor) to run</p> </li> <li> <p>The Log processor read and processes the log file and ingest the logs into Amazon OpenSearch service.</p> </li> </ol> <p>For cross-account log ingestion, the AWS Services store logs on Amazon S3 bucket in one account, and other resources remain in Log Hub's Account:</p> <p></p> </li> <li> <p>Destination on CloudWatch Logs</p> <p>Some services can only choose Amazon CloudWatch Log Group as destination. Currently, this solution supports RDS logs and Lambda Logs</p> <p></p> <p>The process is described as below:</p> <ol> <li> <p>AWS Services store logs on Amazon CloudWatch log group</p> </li> <li> <p>The CloudWatch logs is streaming to Amazon Kinesis Data Stream (KDS) via subscription. </p> </li> <li> <p>KDS triggers the Lambda (Log processor) to run</p> </li> <li> <p>The Log processor read and processes the log records and ingest the logs into Amazon OpenSearch service.</p> </li> </ol> <p>For cross-account log ingestion, the AWS Services store logs on Amazon CloudWatch log group in one account, and other resources remain in Log Hub's Account:</p> <p></p> </li> </ul>"},{"location":"designs/service-log/component-design/#process-design","title":"Process Design","text":"<p>To learn more information about how the detail process are designed, please refer to Process Design</p>"},{"location":"designs/service-log/component-design/#api-design","title":"API Design","text":"<p>A list of GraphQL APIs are built on AWS Appsync service to support service pipeline management from Log Hub Web Console.</p> <p>To learn more information about how the backend APIs are designed, please refer to API Design</p>"},{"location":"designs/service-log/component-design/#data-model-design","title":"Data Model Design","text":"<p>This component uses Amazon DynamoDB as the backend NoSQL database to store information about the service log pipelines.</p> <p>To learn more information about how the data model is designed, please refer to Data Model Design</p>"},{"location":"designs/service-log/component-design/#cloudformation-design","title":"CloudFormation Design","text":"<p>This component can be launched independently via CloudFormation without the Solution Web Console (UI).</p> <p>The parameters in the CloudFormation template are listed as below:</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the service logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the service logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. e.g. <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be -elb-. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <code>&lt;requires input&gt;</code> Select at leasts two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <code>&lt;requires input&gt;</code> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes, keep the size of each shard between 10-50 GiB. Number of Replicas 1 The number of days required to move the index into warm storage, this is only effecitve when the value is &gt;0 and warm storage is enabled in OpenSearch. Days to Warm Storage 0 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Cold Storage 0 The number of days required to move the index into cold storage, this is only effecitve when the value is &gt;0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index, if value is 0, the index will not be deleted."},{"location":"designs/service-log/component-design/#appendix","title":"Appendix","text":""},{"location":"designs/service-log/component-design/#service-log-output-destination","title":"Service Log Output Destination","text":"<p>Most of AWS Services output logs to Amazon CloudWatch Logs or Amazon S3, and some output to Kinesis Data Streams or Kinesis Firehose. The following table is a sample list of AWS services and their log destinations.</p> Service log destination AWS Services Amazon S3 CloudTrail, S3 Access Log, CloudFront Standard Logs, ELB Access Log, VPC Flow Logs, WAF Log, Config Log Amazon CloudWatch Logs RDS, Lambda, Lambda@Edge, VPC Flow Logs, AppSync, API Gateway, WAF Log Kinesis Firehose WAF Log Kinesis Data Streams CloudFront Real-time logs, Amazon Pinpoint events"},{"location":"designs/service-log/data-model-design/","title":"Service Log Pipeline Data Model Design","text":""},{"location":"designs/service-log/data-model-design/#overview","title":"Overview","text":"<p>This component uses Amazon DynamoDB as the backend NoSQL database. This document is about the Data Model Design for Service Log Pipeline component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/service-log/data-model-design/#service-pipeline-table","title":"Service Pipeline Table","text":"<p>Service pipeline table stores information about the service log pipelines managed by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Example Description Comments id String 06e3e64d-0958-43b1-b426-fe52ac55738f Unique ID of a Pipeline Partition key stackName String LogHub-Pipe-06e3e Service Pipeline CloudFormation Stack Name stackId String arn:aws:cloudformation:us-east-1:123456789012:stack/LogHub-Pipe-06e3e/a3d66790-6300-11ec-9ef5-0a829481a42d Service Pipeline CloudFormation Stack ID source String test-bucket Source of the Log target String dev OpenSearch domain status String ACTIVE Status of the pipeline error String Error message of the pipeline type String S3 AWS service type parameters List [{...}] Parameter key-value pairs to deploy the pipeline createdDt String 2021-12-22T08:24:53Z Creation date of the pipeline tags List [{...}] Custom tags"},{"location":"designs/service-log/process-design/","title":"Service Log Pipeline Process Design","text":""},{"location":"designs/service-log/process-design/#overview","title":"Overview","text":"<p>This document is about the Process Design for Service Log Pipeline component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/service-log/process-design/#service-log-pipeline-process","title":"Service Log Pipeline Process","text":"<p>At a high level, an end to end log analytics pipeline consists the 4 following stages.</p> <ol> <li>Collect </li> <li>Buffer</li> <li>Process</li> <li>Visualize</li> </ol> <p>Different services are used in different stages.</p> <p></p>"},{"location":"designs/service-log/process-design/#collect","title":"Collect","text":"<p>Customers can enable the service logs from AWS Management Console or API calls. The log is stored in different destinations.</p> <p>The main services in this stage are:</p> <ul> <li>Amazon S3</li> <li>CloudWatch Log Group</li> </ul> <p>Info</p> <p>Log Hub Solution also supports automatically enable the logs for some services, such as S3 Access Logs. Check API Design</p>"},{"location":"designs/service-log/process-design/#buffer","title":"Buffer","text":"<p>The buffering layers involves different services based on various cases.</p> <ul> <li> <p>Amazon SQS</p> <p>For service logs that is stored in Amazon S3, SQS is used as the buffering layer to receive S3 Events.</p> </li> <li> <p>Kinesis Data Stream (KDS)</p> <p>KDS can be used to subscribe the log streams from CloudWatch Logs. Also, CloudFront real-time logs can only be sent to KDS.</p> </li> <li> <p>Kinesis Data Firehose (KDF)</p> <p>KDF can be used as a bufferring layer and sink the logs to destination such as S3 buckets or directly to OpenSearch. </p> </li> </ul>"},{"location":"designs/service-log/process-design/#process","title":"Process","text":"<p>This stage uses AWS Lambda as the core service.</p> <p>The general purpose of a Log Processor is to parse the raw logs, filter and enrich the log info before ingest that to OpenSearch.</p> <p>The process includes below four steps:</p> <ol> <li>Decompress: This is only required if the source log file is compressed.</li> <li>Parse: This is to parse the raw log records such as using regex.</li> <li>Filter: This is to filter on the logs based on certain conditions.</li> <li>Enrich: This is to enrich the original log messages with extra information, such as IP to Location.</li> </ol> <p>Warning</p> <p>Currently, Filter and Enrich are not yet supported.</p> <p>The processed logs can then be ingested into AOS. There are several actions to be taken in OpenSearch for Log Ingestion.</p> <ul> <li> <p>Create Index Template</p> <p>Index templates let you initialize new indices with predefined mappings and settings. For example, if you continuously index log data, you can define an index template so that all of these indices have the same number of shards and replicas.</p> <p>This is very important as once the data is loaded into OpenSearch, the mapping and the number of shards can't be changed.</p> <p>Use below OpenSearch REST API to create index template <pre><code>PUT _index_template/&lt;index-template-name&gt;\n</code></pre></p> <p>Info</p> <p>This is just an one time action that is executed during the start of the CloudFormation deployment.</p> </li> <li> <p>Create Index State Management (ISM) Policy</p> <p>ISM lets you automate periodic, administrative operations by triggering them based on changes in the index age, index size, or number of documents. Using the ISM, you can define policies that automatically handle index transition (such as hot to warm, warm to cold) or deletions to fit your use case.</p> <p>Below OpenSearch REST API is used to check if index template exists <pre><code># for OpenSearch\nPUT _opendistro/_ism/policies/{policy_id}\n\n# for Elasticsearch\nPUT _plugins/_ism/policies/{policy_id}\n</code></pre></p> <p>Info</p> <p>This is just an one time action that is executed during the start of the CloudFormation deployment.</p> </li> <li> <p>Check if index template exists</p> <p>It's a good practice to have a check whether index template already exists or not before loading data into OpenSearch.  Otherwise, the logs could be loaded as dirty data and it took more time to delete and reprocess the logs.</p> <p>Below OpenSearch REST API is used to check if index template exists <pre><code>HEAD _index_template/&lt;index-template-name&gt;\n</code></pre></p> <p>Info</p> <p>This action is executed everytime before loading data.</p> </li> <li> <p>Bulk Load</p> <p>Once the index template is created, and the data is ready to load, normally the log is loaded in batches via the Bulk load API.</p> <p>The default batch size is <code>10000</code> records. Too many records in one single batch can result in <code>513 Payload too large</code> error.</p> <p>Below OpenSearch REST API is used to load data in batches <pre><code>PUT &lt;index-name&gt;/_bulk\n</code></pre></p> </li> </ul>"},{"location":"designs/service-log/process-design/#visualize","title":"Visualize","text":"<p>Once the log data is ingested into OpenSearch, customer can then analyze and visulize the logs in OpenSearch Dashboards.</p> <p>This solution is shipped with simple dashboards for each services.</p> <ul> <li> <p>Import pre-built dashboards</p> <p>Below OpenSearch Dashboards REST API is used to import pre-built dashboards. <pre><code># for OpenSearch\nPOST _dashboards/api/saved_objects/_import?createNewCopies=true\n\n# for OpenSearch\nPOST _plugin/kibana/api/saved_objects/_import?createNewCopies=true\n</code></pre></p> </li> <li> <p>Dashboard Design</p> <p>The dashboard should contains valuable data insights. </p> <p>Take CloudFront Log as an example, the dashboards contains information such as:</p> <ul> <li>PV/UV count</li> <li>Cache Hit/Miss Rate</li> <li>Bandwidth</li> <li>Health Status (2xx, 3xx, 4xx, 5xx)</li> <li>Top Request URIs</li> <li>Top Client IPs</li> </ul> </li> </ul>"},{"location":"designs/service-log/process-design/#pipeline-orchestration-process","title":"Pipeline Orchestration Process","text":"<p>Customer can manage Service Log pipeline from Log Hub Web Console.  Below is the high-level process that is used to orchestrate service log pipeline flow.</p>"},{"location":"designs/service-log/process-design/#create-service-pipeline","title":"Create Service Pipeline","text":"<p>The process to create service log pipeline is described in below diagram:</p> <p></p> <p>The UML diagram is shown in below:</p> <p></p> <ol> <li>Create Service Pipeline API call is sent to Appsync</li> <li>Appsync invoke Lambda (pipeline handler) as resolver</li> <li>Lambda generate a UUID and create a new item in DynamoDB</li> <li>Lambda trigger Pipeline Step function to flow</li> <li>Pipeline Step Function execute CfnFlow step function as a Child Step</li> <li>CfnFlow use CloudFormation createStack api to start deployment of sub stack template</li> <li>CfnFlow use CloudFormation describe api to query the status of the sub stack</li> <li>CfnFlow check the status and repeat step 7 until the status is completed</li> <li>CfnFlow notify the result to parent Pipeline Step function flow</li> <li>Pipeline Step Function update the status to DynamoDB</li> </ol> <p>References:</p> <ul> <li>Create Service Pipeline API</li> <li>Service Pipeline Table</li> </ul>"},{"location":"designs/service-log/process-design/#delete-service-pipeline","title":"Delete Service Pipeline","text":"<p>The process to delete service log pipeline is described in below diagram:</p> <p></p> <p>The UML diagram is shown in below:</p> <p></p> <ol> <li>Delete Service Pipeline API call is sent to Appsync</li> <li>GraphQL invoke Lambda (pipeline handler) as resolver</li> <li>Lambda query item in DynamoDB to get sub stack ID</li> <li>Lambda update item in DynamoDB with status \u2018DELETING\u2019</li> <li>Lambda trigger Pipeline Step function to flow</li> <li>Pipeline Step Function execute CfnFlow step function as a Child Step</li> <li>CfnFlow use CloudFormation deleteStack api to start deletion of sub stack template</li> <li>CfnFlow use CloudFormation describe api to query the status of the sub stack</li> <li>CfnFlow check the status and repeat step 8 until the status is completed</li> <li>CfnFlow notify the result to parent Pipeline Step function flow</li> <li>Pipeline Step Function update the status to DynamoDB</li> </ol> <p>References:</p> <ul> <li>Delete Service Pipeline API</li> <li>Service Pipeline Table</li> </ul>"},{"location":"designs/service-log/tutorial-extend-service-log/","title":"Tutorial: Extend Service Logs","text":""},{"location":"designs/service-log/tutorial-extend-service-log/#overview","title":"Overview","text":"<p>The purpose of this document is to describe how to extend current Log Hub solution to support more types of AWS service logs.</p> <p>In this guide, we will work you through a tutorial of how to add support for a new type of logs using WAF Logs as an example. If you are not from CSDC solution team, we also welcome your contributions. You can jump to How to Contribute for more details.</p> <p>Notice</p> <p>So far, this guide is only for Log Destination as S3. Check more information about AWS Service Log Destination.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Clone the repo</li> <li>Provision an OpenSearch domain in VPC with private subnets for testing</li> </ul> <p>Info</p> <p>Currently, clone the code repo from code.amazon.com, all changes need be submitted to develop branch for code review.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#how-to-extend","title":"How to Extend","text":"<p>Follow below step by step guide to learn how to extend service logs analysis for WAF in Log Hub solution.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-1-update-log-processor","title":"Step 1: Update Log Processor","text":"<p>To add a new log type for WAF Logs to Log Processor Lambda,  open <code>source/constructs/lambda/pipeline/service/log-processor/util/log_parser.py</code></p> <p>Add an implemetation of LogType for WAF logs, which basically is just to implement the method <code>parse(line)</code> and return a processed record(s) in Json format.</p> <pre><code>class WAF(LogType):\n\"\"\"An implementation of LogType for WAF Logs\"\"\"\n\n    _format = \"json\"\n\n    def parse(self, line: str):\n        json_record = json.loads(line)\n\n        # Extract web acl name, host and user agent\n        json_record[\"webaclName\"] = re.search(\n            \"[^/]/webacl/([^/]*)\", json_record[\"webaclId\"]\n        ).group(1)\n        headers = json_record[\"httpRequest\"][\"headers\"]\n        for header in headers:\n            if header[\"name\"].lower() == \"host\":\n                json_record[\"host\"] = header[\"value\"]\n            elif header[\"name\"].lower() == \"user-agent\":\n                json_record[\"userAgent\"] = header[\"value\"]\n            else:\n                continue\n        return json_record\n</code></pre> <p>Info</p> <p>WAF Logs are in Json format, check Appendix 1: Log Type Implementation for another example for flat log files.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-2-add-cdk-stack","title":"Step 2: Add CDK Stack","text":"<p>To generate a new CloudFormation Template for Waf Logs,  Open <code>source/constructs/bin/main.ts</code> , add a new line as below:</p> <pre><code>new ServiceLogPipelineStack(app, 'WAFLog', { logType: 'WAF' });\n</code></pre> <p>For WAF, simply reuse the common CloudFormation parameters. Open <code>Log-hub/source/constructs/lib/pipeline/service/service-log-pipeline-stack.ts</code>, and add \u2018WAF\u2019 to below line (This is required for all logs that is with log destination as S3):</p> <pre><code>if (['S3', 'CloudTrail', 'ELB', 'CloudFront', 'WAF'].includes(props.logType)) {\nconst logBucketName = new CfnParameter(this, 'logBucketName', {\n...\n</code></pre> <p>For some other log types, you can add more CloudFormation parameters if needed. For example, add an extra parameter of <code>Log Format</code> for VPC Flow Logs since the log format can be customized.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-3-add-index-template","title":"Step 3: Add Index Template","text":"<p>Index templates let you initialize new indices with predefined mappings and settings. Before logs are ingested to OpenSearch, the index template/mapping must exist, otherwise, once data is loaded, the index mapping can't be changed.</p> <p>Log Type is used to identify which template json file to use, Make sure the file name is same as the log type in lower case. </p> <p>For example, if the log Type is \u2018WAF\u2019,  create a waf.json in folder <code>source/constructs/lambda/pipeline/common/opensearch-helper/assets/index_template</code></p> <p>The template must be in a format of : <pre><code>{\n\"aliases\": {},\n\"mappings\": {\n\"properties\": {\n\"...\": {\n\"type\": \"...\",\n...\n},\n...\n}\n},\n\"settings\": {\n\"index\": {\n\"number_of_shards\": \"5\",\n\"number_of_replicas\": \"1\"\n}\n}\n}\n</code></pre></p> <p>Keep aliases as blank. Make sure at least number_of_shards and number_of_replicas are in the setting section (keep 5 and 1 as the default value). The alias and setting will be overrided during deployment.</p> <p>Info</p> <p>To learn more about index template, check Official OpenSearch Document about Index Template</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-4-cdk-deploy","title":"Step 4: CDK Deploy","text":"<p>Run cdk deploy, for example:</p> <pre><code>cdk deploy WAFLog \\                                       \n--parameters vpcId=vpc-0e172e182aa53806b \\\n--parameters subnetIds=subnet-06548d0c4ee34da59,subnet-0b873d0b6e73c2f9c \\\n--parameters securityGroupId=sg-04b03782612cb485f \\\n--parameters endpoint=vpc-dev-ardonphnbg327lwqncuj2vps3q.eu-west-1.es.amazonaws.com \\\n--parameters domainName=dev \\\n--parameters logBucketName=loghub-alb-loggingbucket0fa53b76 \\\n--parameters logBucketPrefix=waf \\\n--parameters backupBucketName=loghub-alb-loggingbucket0fa53b76 \\\n--parameters createDashboard=No \\\n--parameters indexPrefix=xxxxxx \\\n--parameters engineType=OpenSearch\n</code></pre> <p>To learn more about what the parameters are, check CloudFormation Design</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-5-test-and-verify","title":"Step 5:  Test and Verify","text":"<p>Generate some log data by making some requests to WAF (For example, if WAF is for cloudfront, simply access the cloudfront link). Check S3 bucket to see if the log files are generated.</p> <p>Then open Lambda in AWS management console, find and open the function <code>WAFLog-WAFLogPipelineLogProcessorFn</code> and check the cloudwatch logs. If the ingestion is successful, you should see logs as the following:</p> <pre><code>...\n[INFO]  2022-01-14T05:45:14.397Z    947eb9f3-a2c6-4d59-bf63-870be02dd20a    --&gt; bulk_load response code 200\n[INFO]  2022-01-14T05:45:15.165Z    947eb9f3-a2c6-4d59-bf63-870be02dd20a    PUT xxxxxx-waf-2022-01-14/_bulk\n[INFO]  2022-01-14T05:45:16.163Z    947eb9f3-a2c6-4d59-bf63-870be02dd20a    --&gt; bulk_load response code 200\n[INFO]  2022-01-14T05:45:16.229Z    947eb9f3-a2c6-4d59-bf63-870be02dd20a    --&gt; Total: 278126 Success: 278126 Fail: 0\nEND RequestId: 947eb9f3-a2c6-4d59-bf63-870be02dd20a\n</code></pre> <p>You can also see error messages if the ingestion failed, check and then fix the error accordingly.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-6-export-sample-dashboards","title":"Step 6: Export Sample Dashboards","text":"<p>Once data is ingested into OpenSearch, you can then create dashboard and visualize for the logs. Follow the same naming standard.</p> <p>For example,  if the index prefix is <code>xxxxxx-waf-*</code>,  create everything with same prefix <code>xxxxxx-waf-</code></p> <p></p> <p>Once dashboard is completed,  export the dashboard with related objects, the exported file is with file extention <code>.ndjson</code>. Open the export ndjson, and replace all <code>xxxxxx-waf</code> to <code>%%INDEX%%</code>, then save the file to project folder <code>source/constructs/lambda/pipeline/common/opensearch-helper/assets/saved_objects</code></p>"},{"location":"designs/service-log/tutorial-extend-service-log/#how-to-contribute","title":"How to Contribute","text":"<p>If you are not one of the solution team members, and you want to contribute for new service logs, please follow the section How to Extend and get it built and tested in your account.</p> <p>After that, you can contact one of our team members, and submit us with below artifacts:</p> <ul> <li>Log Type implementation in Python</li> <li>Index Template file (.json)</li> <li>Sample Dashboard file (.ndjson)</li> <li>Some sample log file that we can test (Nice to have)</li> </ul>"},{"location":"designs/service-log/tutorial-extend-service-log/#appendix","title":"Appendix","text":""},{"location":"designs/service-log/tutorial-extend-service-log/#log-type-implementation","title":"Log Type Implementation","text":"<p>The Log Type definition</p> <pre><code>class LogType(ABC):\n\"\"\"An abstract class represents one type of Logs.\n\n    Each AWS service has its own log format.\n    Create a class for each service with an implementation of `parse(line)` to parse its service logs\n    \"\"\"\n\n    _fields = []  # list of fields\n    _format = \"text\"  # log file format, such as json, text, etc.\n\n    @abstractmethod\n    def parse(self, line: str):\n\"\"\"Parse the original raw log record, and return processed json record(s).\n\n        This should be implemented in each service class.\n        \"\"\"\n        pass\n\n    ...\n</code></pre> <p>Below is an example of implementation for ELB (ALB) Logs</p> <pre><code>class ELB(LogType):\n\"\"\"An implementation of LogType for ELB Logs\"\"\"\n\n    _fields = [\n        \"type\",\n        \"timestamp\",\n        \"elb\",\n        \"client_ip\",\n        \"client_port\",\n        \"target_ip\",\n        ...\n    ]\n\n    def parse(self, line: str) -&gt; dict:\n        json_record = {}\n        pattern = (\n            \"([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) \"\n            '([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \"([^ ]*) ([^ ]*) '\n            '(- |[^ ]*)\" \"([^\"]*)\" ([A-Z0-9-]+) ([A-Za-z0-9.-]*) ([^ ]*) \"([^\"]*)\" \"([^\"]*)\" '\n            '\"([^\"]*)\" ([-.0-9]*) ([^ ]*) \"([^\"]*)\" \"([^\"]*)\" \"([^ ]*)\" \"([^ ]+?)\" '\n            '\"([^ ]+)\" \"([^ ]*)\" \"([^ ]*)\"'\n        )\n        result = re.match(pattern, line)\n        if result:\n            for i, attr in enumerate(self._fields):\n                # print(f'{attr} = {result.group(i+1)}')\n                json_record[attr] = result.group(i + 1)\n        else:\n            logger.error(\"Unable to parse line: %s\", line)\n        return json_record\n</code></pre>"},{"location":"designs/solution/high-level-design/","title":"Log Hub Solution High Level Design","text":""},{"location":"designs/solution/high-level-design/#overview","title":"Overview","text":"<p>Log Hub is a solution that enables customer to easliy and quickly build end-to-end log analytics pipelines on top of Amazon OpenSearch Service (AOS). A log pipeline includes a series of log processing steps, including collecting logs from source, processing and sending them to OpenSearch as destination for further analysis.</p> <p>This solution provides a web management console from which customer can easliy manage log analytics pipelines and gain valuable data insights for both AWS service logs and application logs with out of the box visualization dashboards without worring about the underling technical complexity. </p> <p>The purpose of this document is to descibe how Log Hub solution is technical designed from high level perspective.</p>"},{"location":"designs/solution/high-level-design/#requirements","title":"Requirements","text":""},{"location":"designs/solution/high-level-design/#functional-requirements","title":"Functional Requirements","text":"<p>This solution is designed with below functional requirements:</p> <ul> <li>A centralized web console for customer to manage all the tasks to reduce operational complexities.</li> <li>Supports both AWS service logs and application logs</li> <li>Supports automatically creating visualization dashboards</li> <li>Supports log lifecycle management</li> <li>Supports cross accounts/cross regions log management</li> <li>Supports both China and Global regions</li> </ul>"},{"location":"designs/solution/high-level-design/#non-functional-requirements","title":"Non-Functional Requirements","text":"<p>The design must meet below non-functional requirements:</p> <ul> <li> <p>Security: Authentication and Authorization is required to protect unexpected access.</p> </li> <li> <p>Scalability: The design must be able to support different scales of logs.</p> </li> <li> <p>Stability: The design must support auto-retries for recoverable errors.</p> </li> <li> <p>Cost Effective: Use serverless architecture design whenever possible, and support log life-cycle management to reduce the overall costs.</p> </li> </ul>"},{"location":"designs/solution/high-level-design/#assumptions","title":"Assumptions","text":"<p>The design is based on below assumptions:</p> <ul> <li> <p>Customers who use this solution must be with administrator access on AWS to be able to perform related functions, as this solution will provision different resources such as Lambda, DynamoDB, etc in the AWS account.</p> </li> <li> <p>Customers must understand their business requirments for log analysis, such as the volume of the logs, the days to retain logs etc.</p> </li> </ul>"},{"location":"designs/solution/high-level-design/#high-level-architecture","title":"High-Level Architecture","text":"<p>Below is the high level architecture diagram:</p> <p></p> <p>This solution deploys the following infrastructure in your AWS Cloud account:</p> <ol> <li> <p>Amazon CloudFront to distribute the frontend web UI assets hosted in Amazon S3 bucket.</p> </li> <li> <p>AWS AppSync to provide the backend GraphQL APIs.</p> </li> <li> <p>Amazon Cognito user pool to provide authentication and authorization for frontend and backend.</p> </li> <li> <p>Amazon DynamoDB as backend database to store the solution related information.</p> </li> <li> <p>AWS Lambda to interact with other AWS Services to execute core logic including managing log pipelines or managing log agents and get the information updated in DynamoDB tables.</p> </li> <li> <p>AWS Step Functions to orchestrate on-demand AWS CloudFormation deployment of a set of predefined stacks for log pipeline management. The log pipeline stacks deploys separate AWS resources and are used to collect and process logs and ingest them into Amazon OpenSearch Service for further analysis and visualiztion.</p> </li> <li> <p>AWS Systems Manager and Amazon EventBridge to manage log agent for collecting logs from Application Servers, such as installing log agents (fluentbit) to Application servers and monitoring the health status of the agents.</p> </li> </ol>"},{"location":"designs/solution/high-level-design/#component-definition","title":"Component Definition","text":"<p>This solution consists of below main components:</p> <ul> <li> <p>Domain Management</p> <p>This solution uses Amazon OpenSearch Service as the underlying engine to store and analyze logs. This Domain Management component consists a list of operations on top of existing AOS domains, such as importing an existing AOS domain for log ingestion, providing a proxy for access the AOS dashboards which is within VPC, etc.</p> <p>Info</p> <p>To learn more about how this component is designed, please refer to Domain Management Component Design</p> <p>Warning</p> <p>Provision of AOS domain is not in scope of this component. Customer needs to create AOS domain before using this component.</p> </li> <li> <p>Service Log Pipeline</p> <p>This solution supports out of the box log analysis for many AWS service logs, such as Amazon S3 access logs, ELB access logs, etc.  This Service Log Pipeline component is designed to reduce the complexisities of building log analytics pipelines for different AWS services with different formats. Customer can collect and process AWS service logs without writing any codes, as well as gain data insights using out of the box visualization dashboards.</p> <p>Info</p> <p>To learn more about how this component is designed, please refer to Service Log Pipeline Component Design</p> </li> <li> <p>Application Log Pipeline</p> <p>This solution supports out of the box log analysis for application logs, such as Nginx/Apache logs or general application logs via regex parser. This Application Log Pipeline component uses Fluent Bit as the underlying log agent to collect logs from the application servers, and allow customers to easily install log agent and monitor the healthy of the agent via System Manager.</p> </li> </ul>"},{"location":"implementation-guide/architecture/","title":"Architecture Overview","text":"<p>Deploying this solution with the default parameters builds the following environment in the AWS Cloud.</p> <p> Figure 1: Log Hub on AWS architecture</p> <p>This solution deploys the AWS CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li> <p>Amazon CloudFront distributes the frontend web UI assets hosted in Amazon S3 bucket.</p> </li> <li> <p>Amazon Cognito user pool or OpenID Connector (OIDC) can be used for authentication.</p> </li> <li> <p>AWS AppSync provides the backend GraphQL APIs.</p> </li> <li> <p>Amazon DynamoDB stores the solution related information as backend database.</p> </li> <li> <p>AWS Lambda interacts with other AWS Services to process core logic of managing log pipelines or log agents, and obtains information updated in DynamoDB tables.</p> </li> <li> <p>AWS Step Functions orchestrates on-demand AWS CloudFormation deployment of a set of predefined stacks for log pipeline management. The log pipeline stacks deploy separate AWS resources and are used to collect and process logs and ingest them into Amazon OpenSearch Service for further analysis and visualization.</p> </li> <li> <p>Service Log Pipeline or Application Log Pipeline are provisioned on demand via Log Hub console.</p> </li> <li> <p>AWS Systems Manager and Amazon EventBridge manage log agents for collecting logs from Application Servers, such as installing log agents (Fluent Bit) for Application servers and monitoring the health status of the agents.</p> </li> <li> <p>Amazon EC2 or Amazon EKS installs Fluent Bit agents, and uploads log data to Application Log Pipeline.</p> </li> <li> <p>Application Log Pipelines read, parse, process application logs and ingest them into Amazon OpenSearch.</p> </li> <li> <p>Service Log Pipelines read, parse, process AWS service logs and ingest them into Amazon OpenSearch.</p> </li> </ol> <p>This solution supports two types of log pipelines: Service Log Analytics Pipeline and Application Log Analytics Pipeline.</p>"},{"location":"implementation-guide/architecture/#service-log-analytics-pipeline","title":"Service Log Analytics Pipeline","text":"<p>Log Hub supports log analysis for AWS services, such as Amazon S3 access logs, and Application Load Balancer access logs. For a complete list of supported AWS services, refer to Supported AWS Services.</p> <p>AWS services output logs to different destinations, including Amazon S3 bucket, CloudWatch log groups, Kinesis Data Streams, and Kinesis Firehose. The solution ingests those logs using different workflows.</p> <p>Note</p> <p>Log Hub supports cross-account log ingestion. If you want to ingest the logs from another AWS account, the resources in the Sources group in the architecture diagram will be in another account.</p>"},{"location":"implementation-guide/architecture/#logs-in-amazon-s3","title":"Logs in Amazon S3","text":"<p>Some AWS services use Amazon S3 as the destination, and the logs in Amazon S3 are generally not for real-time analysis. </p> <p> Figure 2: Amazon S3 based service log pipeline architecture</p> <p>The log pipeline runs the following workflow:</p> <ol> <li> <p>AWS services store logs in Amazon S3 bucket (Log Bucket).</p> </li> <li> <p>An S3 Event Notification is sent to Amazon SQS when a new log file is created.</p> </li> <li> <p>Amazon SQS triggers the Lambda (Log Processor) to run.</p> </li> <li> <p>The log processor reads and processes the log files.</p> </li> <li> <p>The log processor ingest the logs into the Amazon OpenSearch Service.</p> </li> <li> <p>The logs failed to be processed are exported to Amazon S3 bucket (Backup Bucket).</p> </li> </ol> <p>For cross-account ingestion, the AWS Services store logs in Amazon S3 bucket in the member account, and other resources remain in Log Hub.</p>"},{"location":"implementation-guide/architecture/#logs-in-amazon-cloudwatch","title":"Logs in Amazon CloudWatch","text":"<p>Some services use Amazon CloudWatch log group as the destination. </p> <p> Figure 3: Amazon CloudWatch based service log pipeline architecture</p> <p>The log pipeline runs the following workflow:</p> <ol> <li> <p>AWS Services store logs in Amazon CloudWatch log group.</p> </li> <li> <p>The CloudWatch logs is streaming to Amazon Kinesis Data Firehose via subscription.</p> </li> <li> <p>Amazon Kinesis Data Firehose saves logs to the Amazon S3 bucket (Log Bucket).</p> </li> <li> <p>Notifications are sent to Amazon SQS via S3 Event Notifications.</p> </li> <li> <p>SQS triggers the Lambda (Log Processor) to run.</p> </li> <li> <p>The log processor reads and processes the log files.</p> </li> <li> <p>The log processor ingest the logs into the Amazon OpenSearch Service.</p> </li> <li> <p>The logs failed to be processed are exported to Amazon S3 bucket (Backup Bucket).</p> </li> </ol> <p>For cross-account ingestion, the AWS Services store logs on Amazon CloudWatch log group in the member account, and other resources remain in Log Hub.</p>"},{"location":"implementation-guide/architecture/#application-log-analytics-pipeline","title":"Application Log Analytics Pipeline","text":"<p>Log Hub supports log analysis for application logs, such as Nginx/Apache HTTP Server logs or custom application logs. </p> <p>Note</p> <p>Log Hub supports cross-account log ingestion. If you want to ingest logs from the same account, the resources in the Sources group will be in the same account as your Log Hub account. Otherwise, they will be in another AWS account.</p>"},{"location":"implementation-guide/architecture/#logs-from-amazon-ec2-amazon-eks","title":"Logs from Amazon EC2 / Amazon EKS","text":"<p> Figure 4: Application log pipeline architecture for EC2</p> <p>The log pipeline runs the following workflow:</p> <ol> <li> <p>Fluent Bit works as the underlying log agent to collect logs from application servers and send them to an optional Log Buffer, or ingest into OpenSearch domain directly. </p> </li> <li> <p>The Log Buffer triggers the Lambda (Log Processor) to run.</p> </li> <li> <p>The log processor reads and processes the log records and ingests the logs into the OpenSearch domain.</p> </li> <li> <p>The logs failed to be processed are exported to an Amazon S3 bucket (Backup Bucket).</p> </li> </ol>"},{"location":"implementation-guide/architecture/#logs-from-syslog-client","title":"Logs from Syslog Client","text":"<p>Important</p> <ol> <li>Please make sure your Syslog generator/sender's subnet is connected to Log Hub' two private subnets so that log can be ingested, you need to use VPC Peering Connection or Transit Gateway to connect these VPCs.</li> <li>The NLB together with the ECS containers in the architecture diagram will be only provisioned when you create a syslog ingestion and be automated deleted when there is no syslog ingestion.</li> </ol> <p>  Figure 5: Application log pipeline architecture for Syslog</p> <ol> <li> <p>Syslog client (like Rsyslog) send logs to a Network Load Balancer (NLB) in Log Hub's private subnets, and NLB routes to the ECS containers running Syslog servers.</p> </li> <li> <p>Fluent Bit works as the underlying log agent in the ECS Service to parse logs, and send them to an optional Log Buffer, or ingest into OpenSearch domain directly.</p> </li> <li> <p>The Log Buffer triggers the Lambda (Log Processor) to run.</p> </li> <li> <p>The log processor reads and processes the log records and ingests the logs into the OpenSearch domain.</p> </li> <li> <p>The logs failed to be processed are exported to an Amazon S3 bucket (Backup Bucket).</p> </li> </ol>"},{"location":"implementation-guide/considerations/","title":"Considerations","text":""},{"location":"implementation-guide/considerations/#regional-deployments","title":"Regional deployments","text":"<p>This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List. </p> <p>Log Hub provides two types of authentication, Cognito User Pool and OpenID Connect (OIDC) Provider. You need to choose to launch the solution with OpenID Connect if one of the following cases occurs:</p> <ul> <li>Cognito User Pool is not available in your AWS region.</li> <li>You already have an OpenID Connect Provider.</li> </ul> <p>Supported regions for deployment</p> Region Name Launch with Cognito User Pool Launch with OpenID Connect US East (N. Virginia) US East (Ohio) US West (N. California) US West (Oregon) Africa (Cape Town) Asia Pacific (Hong Kong) Asia Pacific (Mumbai) Asia Pacific (Osaka) Asia Pacific (Seoul) Asia Pacific (Singapore) Asia Pacific (Sydney) Asia Pacific (Tokyo) Canada (Central) Europe (Frankfurt) Europe (Ireland) Europe (London) Europe (Milan) Europe (Paris) Europe (Stockholm) Middle East (Bahrain) South America (Sao Paulo) China (Beijing) Region Operated by Sinnet China (Ningxia) Regions operated by NWCD"},{"location":"implementation-guide/considerations/#restrictions","title":"Restrictions","text":"<ul> <li>You can have only one active Log Hub solution stack in one region. If your deployment failed, make sure you have deleted the failed stack before retrying the deployment.</li> </ul>"},{"location":"implementation-guide/cost/","title":"Cost Estimation","text":"<p>Important</p> <p>The cost estimation described in this section are just examples, which are calculated based on assumptions, and may vary in your environment. </p> <p>You will be responsible for the cost of the AWS services used when running the solution. As of October 2022, the main factors affecting the solution cost include:</p> <ul> <li>Type of logs to be ingested</li> <li>Volume of logs to be ingested/processed</li> <li>Size of the log message</li> <li>Location of logs</li> <li>Additional features</li> </ul> <p>The following examples will demonstrate the cost estimation of 10/100/1000 GB daily log ingestion. The total cost is composed of Amazon OpenSearch Cost, Processing Cost, and Additional Features Cost. </p> <p>Note</p> <p>All the cost estimation is based on the AWS service price in AWS N. Virginia Region (us-east-1). </p>"},{"location":"implementation-guide/cost/#amazon-opensearch-cost","title":"Amazon OpenSearch Cost","text":"<ul> <li>OD: On Demand</li> <li>AURI_1: All Upfront Reserved Instance 1 Year</li> <li>Tiering: The days stored in each tier. For example, 7H + 23W + 60C indicates that the log is stored in hot tier for 7 days, warm tier for 23 days, and cold tier for 60 days.</li> <li>Replica: The number of shard replicas.</li> </ul> Daily log Volume (GB) Retention (days) Tiering Replica OD Monthly ($) AURI_1 Monthly  ($) Dedicated Master Data Node EBS (GB) UltraWarm Nodes UltraWarm/Cold S3 Storage (GB) OD cost per GB ($) AURI_1 cost per GB ($) 10 30 30H 0 216.28 158.54 N/A c6g.large[2] 380 N/A 0 0.72093 0.52847 10 30 30H 1 289.35 223.94 N/A m6g.large[2] 760 N/A 0 0.9645 0.74647 100 30 7H + 23W 0 989.49 825.97 m6g.large[3] m6g.large[2] 886 medium[2] 0 0.32983 0.27532 100 30 7H + 23W 1 1295.85 1066.92 m6g.large[3] m6g.large[4] 1772 medium[2] 0 0.43195 0.35564 100 90 7H + 23W + 60C 0 1133.49 969.97 m6g.large[3] m6g.large[2] 886 medium[2] 8300 0.12594 0.10777 100 90 7H + 23W + 60C 1 1439.85 1210.92 m6g.large[3] m6g.large[4] 1772 medium[2] 8300 0.15998 0.13455 100 180 7H + 23W + 150C 0 1349.49 1185.97 m6g.large[3] m6g.large[2] 886 medium[2] 17300 0.07497 0.06589 100 180 7H + 23W + 150C 1 1655.85 1426.92 m6g.large[3] m6g.large[4] 1772 medium[2] 17300 0.09199 0.07927 1000 30 7H + 23W 0 6101.15 5489.48 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 23000 0.20337 0.18298 1000 30 7H + 23W 1 8759.49 7635.8 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 23000 0.29198 0.25453 1000 90 7H + 23W + 60C 0 8027.33 7245.45 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 83000 0.08919 0.0805 1000 90 7H + 23W + 60C 1 10199.49 9075.8 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 83000 0.11333 0.10084 1000 180 7H + 23W + 150C 0 9701.15 9089.48 m6g.large[3] r6g.xlarge[6] 8856 medium[15] 173000 0.0539 0.0505 1000 180 7H + 23W + 150C 1 12644.19 11420.86 m6g.large[3] r6g.2xlarge[6] 17712 medium[15] 173000 0.07025 0.06345"},{"location":"implementation-guide/cost/#processing-cost","title":"Processing Cost","text":""},{"location":"implementation-guide/cost/#aws-service-logs","title":"AWS Service Logs","text":"<p>Depending on the log location, the cost of ingesting and processing service logs may vary. </p>"},{"location":"implementation-guide/cost/#logs-in-amazon-s3","title":"Logs in Amazon S3","text":"<p>Note</p> <p>Ingesting AWS service logs from S3 will incur SQS and S3 request fees which are very low, usually within the AWS Free Tier.</p> <p>Here are the assumptions:</p> <ul> <li>AWS Services save logs to Amazon S3 every 5 minutes in gzip format (most of AWS services output logs in gzip). </li> <li>A 4MiB compressed log file in S3 is roughly 100MiB in raw log size.</li> <li>A Lambda with 1GB memory takes about 26 seconds to process a 4MiB log file, namely 260 ms per MiB raw logs. </li> <li>The maximum compressed log file size is 5MiB.</li> </ul> <p>You have <code>N</code> GB raw log per day, and the daily cost estimation is as follows: </p> <ul> <li>Lambda Cost = 260 ms per MiB x 1024 MiB x <code>N</code> GB/day x $0.0000000167 per ms</li> <li>S3 Storage Cost = $0.023 per GB x <code>N</code>GB/day x 4% (compression)</li> </ul> <p>The total monthly cost for ingesting AWS service logs is:</p> <p>Total Monthly Cost = (Lambda Cost + S3 Storage Cost) x 30 days</p> Daily Log Volume Daily Lambda Cost ($) Daily S3 Storage Cost ($) Monthly Cost ($) 10 0.044 0.009 1.610 100 0.445 0.092 16.099 1000 4.446 0.920 160.986"},{"location":"implementation-guide/cost/#application-logs","title":"Application Logs","text":"<p>Important</p> <p>If you have multiple log formats (index), you need to make cost estimation for each of them.</p> <p>Depending on the log location, and the Log Buffer, the cost of ingesting and  processing application logs may vary. </p> <p>There is no additional cost when you ingest data from log agents to the OpenSearch domain directly. However,  it is recommended to use Log Buffer because it can help to aggregate logs and protect OpenSearch clusters from overwhelming during peak hours. If you have small volume of logs and you are very confident that the size of the OpenSearch clusters have the capacity to ingest logs during peak hours, you can go without the Log Buffer.</p>"},{"location":"implementation-guide/cost/#log-ingestion-through-amazon-s3","title":"Log ingestion through Amazon S3","text":"<p>The cost estimation is based on the following assumptions and facts:</p> <ul> <li>The average log message size is 1 KB.</li> <li>The daily log volume is <code>L</code> GB.</li> <li>The Lambda processor memory is 1024 MiB.</li> <li>The log agent deliveries logs to Amazon S3 when it hits the Buffer size (50 MiB) or Buffer interval (60 seconds) thresholds. The condition satisfied first triggers data delivery to Amazon S3.</li> <li>The compression rate of Gzip format is 20%. </li> <li>The Lambda runtime to process log less than 10 MiB is 1000ms.</li> <li>Each log file will encounter one S3 PUT request to upload the log file, and one S3 GET request to read the log file.</li> <li>The daily number of files <code>N</code> saved on Amazon S3 = Max(<code>L</code> GB per day x 1024 / 50 MiB, 24 hours x 60 seconds/hour)</li> <li>The daily volume of files <code>V</code> saved on Amazon S3 = <code>L</code> GB per day x 20% compression rate</li> </ul> <p>Base on the above assumptions, here is the daily cost estimation formula:</p> <ul> <li>Amazon S3 storage Cost = $0.023 per GB x daily volume of files <code>V</code></li> <li>Amazon S3 request Cost = ($0.005 per 1000 requests + $0.0004 per 1000 requests) x daily number of files <code>N</code> / 1000</li> <li>Lambda Cost = $0.0000000167 per 1ms x 1000 ms per invocation x daily number of files <code>N</code></li> </ul> <p>Total Monthly Cost = (Lambda Cost + S3 Storage Cost + S3 request Cost) x 30 days</p> Daily Log Volume (GB) Daily Number of files Daily S3 Storage Cost ($) Daily S3 Request Cost ($) Daily Lambda Cost ($) Monthly Cost ($) 10 1,440 0.046 0.008 0.024 2.335 100 2,048 0.460 0.011 0.034 15.158 1000 20,480 4.600 0.111 0.342 151.578"},{"location":"implementation-guide/cost/#log-ingestion-through-amazon-kinesis-data-streams","title":"Log ingestion through Amazon Kinesis Data Streams","text":"<p>The cost estimation is based on the following assumptions and facts:</p> <ul> <li>The average log message size is 1 KB.</li> <li>The daily log volume is <code>L</code> GB.</li> <li>The Lambda processor memory is 1024 MiB.</li> <li>Every Lambda invocation processes 1 MiB logs.</li> <li>One Lambda invocation processes one shard of Kinesis, and Lambda can scale up to more concurrent innovations to process multiple shards. </li> <li>The Lambda runtime to process log less than 5 MiB is 500ms.</li> <li>30% additional shards are provided to handle traffic jitter.</li> <li>One Kinesis shard intake log size is =  1 MiB /second x 3600 seconds per hour x 24 hours x 0.7 = 60.48 GB/day.</li> <li>The desired Kinesis Shard number <code>S</code> is = Round_up_to_next_integer(Daily log volume <code>L</code> / 60.48).</li> </ul> <p>Based on the above assumptions, here is the daily cost estimation formula:</p> <ul> <li>Kinesis Shard Hour Cost = $0.015 / shard hour x 24 hours per day x <code>S</code> shards</li> <li>Kinesis PUT Payload Unit Cost =  $0.014 per million units x 1 millions per GB x <code>L</code> GB per day</li> <li>Lambda Cost = $0.0000000167 per 1ms x 500 ms per invocation x 1,000 invocations per GB x <code>L</code> GB per day</li> </ul> <p>Total Monthly Cost = (Kinesis Shard Hour Cost + Kinesis PUT Payload Unit Cost + Lambda Cost) x 30 days</p> Daily Log Volume (GB) Shards Daily Kinesis Shard Hour Cost ($) Daily Kinesis PUT Payload Unit Cost ($) Daily Lambda Cost ($) Monthly Cost ($) 10 1 0.36 0.14 0.0835 17.505 100 2 0.72 1.4 0.835 88.65 1000 17 6.12 14 8.35 854.1"},{"location":"implementation-guide/cost/#additional-features-cost","title":"Additional Features Cost","text":"<p>Note</p> <p>You will not be charged if you choose not to use the additional features in the Log Hub console.</p>"},{"location":"implementation-guide/cost/#access-proxy","title":"Access Proxy","text":"<p>If you deploy the Access Proxy through Log Hub, the following charges will apply.  </p> <p>The cost estimation is based on the following assumptions and facts:</p> <ul> <li>Proxy module provisioned 2 x t3.large instances.</li> <li>1 year all upfront reserved instances pricing model applies.</li> <li>The 1 year all upfront reserved instances for t3.large instance is $426.612/year.</li> <li>The EBS attached to the EC2 instance is 8 GB.</li> </ul> <p>Based on the above assumptions, here is the monthly cost estimation formula:</p> <ul> <li>EC2 Cost = EC2 $426.612 x 2 / 12 months  = $71.1/month</li> <li>EBS Cost = EBS $0.1 GB/month x 8 GB x 2 = $1.6/month</li> <li>Elastic Load Balancer Cost = $0.0225 per ALB-hour x 720 hours/month = $16.2/month</li> </ul> <p>Total Monthly Cost = $71.1 EC2 Cost + $1.6 EBS Cost + $16.2 Elastic Load Balancer Cost = $88.9</p>"},{"location":"implementation-guide/cost/#alarms","title":"Alarms","text":"<p>If you deploy the Alarms through Log Hub, the CloudWatch Price will apply.</p>"},{"location":"implementation-guide/faq/","title":"Frequently Asked Questions","text":""},{"location":"implementation-guide/faq/#general","title":"General","text":"<p>Q:  What is Log Hub solution? Log Hub is an AWS Solution that simplifies the building of log analytics pipelines. It provides to customers, as complementary of Amazon OpenSearch Service, capabilities to ingest and process both application logs and AWS service logs without writing code, and create visualization dashboards from out-of-box templates. Log Hub automatically assembles the underlying AWS services, and provides you a web console to manage log analytics pipelines.</p> <p>Q: What are the supported logs in this solution? Log Hub supports both AWS service logs and EC2/EKS application logs. Refer to the supported AWS services, and the supported application log formats and sources for more details.</p> <p>Q: Does Log Hub support ingesting logs from multiple AWS accounts? Yes. Starting from v1.1.0, Log Hub supports ingesting AWS service logs and application logs from a different AWS account  in the same region. For more information, see cross-account ingestion.</p> <p>Q: Does Log Hub support ingesting logs from multiple AWS Regions? Currently, Log Hub does not automate the log ingestion from a different AWS Region. You need to ingest logs from other  regions into pipelines provisioned by Log Hub. For AWS services which store the logs in S3 bucket, you can leverage  the S3 Cross-Region Replication to copy the logs to the Log Hub deployed region, and import incremental logs using the manual mode by specifying the  log location in the S3 bucket. For application logs on EC2 and EKS, you need to set up the networking (for example, Kinesis VPC endpoint, VPC Peering),  install agents, and configure the agents to ingest logs to Log Hub pipelines.</p> <p>Q: What is the license of this solution? This solution is provided under the Apache-2.0 license. It is a permissive free software license written by the Apache Software Foundation. It allows users to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software under the terms of the license, without concern for royalties.</p> <p>Q: How can I find the roadmap of this solution? This solution uses GitHub project to manage the roadmap. You can find the roadmap here.</p> <p>Q: How can I submit a feature request or bug report? You can submit feature requests and bug report through the GitHub issues. Here are the templates for feature request, bug report.</p>"},{"location":"implementation-guide/faq/#setup-and-configuration","title":"Setup and configuration","text":"<p>Q: Can I deploy Log Hub on AWS in any AWS Region? Log Hub provides two deployment options: option 1 with Cognito User Pool, and option 2 with OpenID Connect. For  option 1, customers can deploy the solution in AWS Regions where Amazon Cognito User Pool, AWS AppSync, Amazon Kinesis Data Firehose (optional) are available.  For option 2, customers can deploy the solution in AWS Regions where AWS AppSync, Amazon Kinesis Data Firehose (optional) are available. Refer to supported regions for deployment for more information.</p> <p>Q: What are the prerequisites of deploying this solution? Log Hub does not provision Amazon OpenSearch clusters, and you need to import existing OpenSearch clusters through the web console. The cluster must meet the requirements specified in prerequisites.</p> <p>Q: Why do I need a domain name with ICP recordal when deploy the solution in AWS China Regions? The Log Hub console is served via CloudFront distribution which is considered as an Internet information service. According to the local regulations, any Internet information service must bind to a domain name with ICP recordal.</p> <p>Q: What versions of OpenSearch does the solution work with? Log Hub supports Amazon OpenSearch Service, with engine version Elasticsearch 7.10 and later, Amazon OpenSearch 1.0 and later.</p> <p>Q: Can I deploy the solution in an existing VPC? Yes. You can either launch the solution with a new VPC or launch the solution with an existing VPC. When using an existing VPC, you need to select the VPC and the corresponding subnets. Refer to launch with Cognito User Pool or launch with OpenID Connect for more details.</p> <p>Q: I did not receive the email containing the temporary password when launch with Cognito User Pool. How can I resend the password? Your account is managed by the Cognito User Pool. To resend the first time temporary password, you can find the user pool  created by the solution, delete and recreate the user using the same email address. If you still have the same issue,  try with another email address.</p> <p>Q: How can I create more users for this solution? If you launched the solution with Cognito User Pool, go to the AWS console, find the user pool created by the solution, and you can create more users. If you launched the solution with OpenID Connect (OIDC), you should add more users in the user pool managed by the OIDC provider. Please note, all users have the same privileges. </p>"},{"location":"implementation-guide/faq/#pricing","title":"Pricing","text":"<p>Q: How will I be charged and billed for the use of this solution? The solution is free to use, and you are responsible for the cost of AWS services used while running this solution.  You pay only for what you use, and there are no minimum or setup fees. Refer to the Log Hub Cost section for detailed cost estimation. </p> <p>Q: Will there be additional cost for cross-account ingestion? No. The cost will be same as ingesting logs within the same AWS account.</p>"},{"location":"implementation-guide/faq/#log-ingestion","title":"Log Ingestion","text":"<p>Q: What is the log agent used in the Log Hub solution? Log Hub uses AWS for Fluent Bit, a distribution of Fluent Bit maintained by AWS. The solution uses this distribution to ingest logs from Amazon EC2 and Amazon EKS.</p> <p>Q: I have already stored the AWS service logs of member accounts in a centralized logging account. How should I create service log ingestion for member accounts? In this case, you need to deploy the Log Hub solution in the centralized logging account, and ingest AWS service logs  using the Manual mode from the logging account. Refer to this guide for ingesting Application  Load Balancer logs with Manual mode. You can do the same with other supported AWS services which output logs to S3.</p> <p>Q: Why there are some duplicated records in OpenSearch when ingesting logs via Kinesis Data Streams? This is usually because there is no enough Kinesis Shards to handle the incoming requests. When threshold error occurs in Kinesis, the Fluent Bit agent will retry  that chunk. To avoid this issue, you need to estimate your log throughput and set a proper Kinesis shard number. Please refer to the  Kinesis Data Streams quotas and limits. Log Hub provides a built-in feature to scale-out and scale-in the Kinesis shards, and it would take a couple of minutes  to scale out to the desired number.</p>"},{"location":"implementation-guide/faq/#log-visualization","title":"Log Visualization","text":"<p>Q. How can I find the built-in dashboards in OpenSearch? Please refer to the AWS Service Logs and Application Logs to  find out if there is a built-in dashboard supported. You also need to turn on the Sample Dashboard option when creating a log analytics pipeline. The dashboard will be inserted into the AOS under Global Tenant. You can switch to the  Global Tenant from the top right coder of the OpenSearch Dashboards.</p>"},{"location":"implementation-guide/faq/#upgrades","title":"Upgrades","text":"<p>Q: How can I upgrade the solution? You can use the latest CloudFormation template link to upgrade the Log Hub. Follow the upgrade steps here.</p> <p>Q: Will I lose any data during the upgrading? No. Upgrading will only update the Log Hub console, and it will not affect any existing log ingestion pipelines.</p> <p>Q: How long does the upgrade take? It depends on the Log Hub versions. In most cases, the upgrade will take less than 30 minutes to complete.</p> <p>Q: Can I upgrade to the latest version from any version? You can upgrade to the latest version from last two versions without changes. For example, you can upgrade from <code>v1.0.X</code> or <code>v1.1.X</code> to <code>v1.2.X</code>. If you are not able to upgrade to the latest version, you may need to upgrade to some intermediate versions first.</p>"},{"location":"implementation-guide/release-notes/","title":"Release Notes","text":""},{"location":"implementation-guide/release-notes/#120","title":"1.2.0","text":"<p>November 30, 2022</p> <p>What's New</p> <ul> <li>Your now have two more options to buffer logs when you create application pipeline. One is to use S3 as buffer layer that supports large throughput and minute-level latency, another is to send logs to your AOS domain directly without buffering. #43</li> <li>Support ingesting syslog from servers that could forward syslog in TCP or UDP protocol. We provide log parsers for RFC5424 and RFC3164 format, also allow you to define custom parser. #41</li> <li>Support filtering log before ingestion, you can define filters on the UI directly. #38</li> <li>Support creating instance group based on Amazon EC2 Auto Scaling Group(ASG), any auto-scaled instance will be automatically included into instance group. #44</li> <li>Support editing existing instance group, you can add or remove instance in an instance group. #45</li> <li>You now have the flexibility to specify a time zone for your logs. #42</li> <li>You now can choose a field parsed from your log to be the time key for your logs. #54</li> <li>You now can deploy the solution in Africa (Cape Town), Asia Pacific (Hong Kong), Europe (Milan), Middle East (Bahrain) regions. #37</li> </ul> <p>Update</p> <ul> <li>Update the log agent Fluent Bit from v1.9.3 to v1.9.9.</li> </ul> <p>Bug Fixes</p> <ul> <li>Fix the Fluent-Bit K8S filter crash issue when deployed in Amazon EKS.</li> <li>Fix the S3 prefix issue for VPC Flow logs.</li> <li>Fix the automatic VPC Peering Connection issue between the solution's private subnets and other subnets without explicit route table. #55</li> </ul>"},{"location":"implementation-guide/release-notes/#110","title":"1.1.0","text":"<p>August 26, 2022</p> <p>What's New</p> <ul> <li>Support the ingestion of VPC Flow logs with an optional dashboard template. #14</li> <li>Support the ingestion of AWS Config logs with an optional dashboard template. #15</li> <li>Support the ingestion of sampled AWS WAF logs. #16</li> <li>Support ingesting AWS service logs from another AWS account in the same region. #17</li> <li>Support ingesting application logs from another AWS account in the same region. #18</li> <li>Launch the solution in an existing VPC. #19</li> <li>Add an article of how to deploy the solution with other region's Cognito User Pool. #20</li> <li>Add an article of how to deploy the solution with ADFS. #21</li> <li>Support ingesting EKS pod logs directly into OpenSearch. #25</li> </ul> <p>Update</p> <ul> <li>Upgrade the Lambda runtime from nodejs12.X to newer version. You will not receive the Lambda runtime deprecation warning emails anymore.</li> <li>Upgrade the Lambda runtime from python3.6 to newer version.</li> <li>Upgrade the CDK version to CDK 2.36.0</li> </ul> <p>Bug Fixes</p> <ul> <li>OpenSearch cluster details page now can display information for instances with gp3 type of EBS.</li> </ul>"},{"location":"implementation-guide/release-notes/#100","title":"1.0.0","text":"<p>June 6, 2022</p> <p>What's New</p> <ul> <li>Support of Amazon CloudTrail logs.</li> <li>Support of Amazon S3 Access logs.</li> <li>Support of Amazon RDS/Aurora MySQL logs (audit, general, error, slow query).</li> <li>Support of Amazon CloudFront standard logs.</li> <li>Support of AWS Lambda logs.</li> <li>Support of Application Load Balancer access logs.</li> <li>Support of AWS WAF logs.</li> <li>Support ingesting logs from EC2 and EKS, including Apache HTTP Server, Nginx, Single-line Text, Multi-line Text, JSON format.</li> <li>Support ingesting incremental logs from S3, including JSON and single-line text format.</li> </ul>"},{"location":"implementation-guide/security/","title":"Security","text":"<p>When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared model  reduces your operational burden because AWS operates, manages, and controls the components including the host  operating system, the virtualization layer, and the physical security of the facilities in which the services operate.  For more information about AWS security, see AWS Cloud Security.</p>"},{"location":"implementation-guide/security/#iam-roles","title":"IAM Roles","text":"<p>AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions  to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions,  AWS AppSync and Amazon Cognito access to create regional resources.</p>"},{"location":"implementation-guide/security/#security-groups","title":"Security Groups","text":"<p>The security groups created in this solution are designed to control and isolate network traffic between the solution  components. We recommend that you review the security groups and further restrict access as needed once the deployment  is up and running.</p>"},{"location":"implementation-guide/security/#amazon-cloudfront","title":"Amazon CloudFront","text":"<p>This solution deploys a web console hosted in an Amazon Simple Storage Service (Amazon S3) bucket. To help reduce  latency and improve security, this solution includes an Amazon CloudFront distribution with an origin access identity,  which is a CloudFront user that provides public access to the solution\u2019s website bucket contents.  For more information, refer to Restricting Access to Amazon S3 Content by Using an Origin Access Identity in the  Amazon CloudFront Developer Guide.</p>"},{"location":"implementation-guide/solution-components/","title":"Solution components","text":"<p>The solution consists of the following components:</p>"},{"location":"implementation-guide/solution-components/#domain-management","title":"Domain Management","text":"<p>This solution uses Amazon OpenSearch Service (AOS) as the underlying engine to store and analyze logs. You can import an existing AOS domain for log ingestion, and provide an access proxy to the AOS dashboards within VPC. Moreover, you can set up recommended CloudWatch alarms for AOS.</p>"},{"location":"implementation-guide/solution-components/#analytics-pipelines","title":"Analytics Pipelines","text":"<p>A log pipeline includes a series of log processing steps, including collecting logs from sources, processing and sending them to Amazon OpenSearch Service for further analysis. Log Hub supports AWS Service log ingestion and server-side application log ingestion.</p>"},{"location":"implementation-guide/solution-components/#service-log-pipeline","title":"Service Log Pipeline","text":"<p>This solution supports out of the box log analysis for AWS service logs, such as Amazon S3 access logs, and ELB access logs. The component is designed to reduce the complexities of building log analytics pipelines for different AWS services with different formats. </p>"},{"location":"implementation-guide/solution-components/#application-log-pipeline","title":"Application Log Pipeline","text":"<p>This solution supports out of the box log analysis for application logs, such as Nginx/Apache logs or general application logs via regex parser. The component uses Fluent Bit as the underlying log agent to collect logs from application servers, and allows you to easily install log agent and monitor the agent health via System Manager.</p>"},{"location":"implementation-guide/trouble-shooting/","title":"Troubleshooting Errors in Log Hub","text":"<p>The following help you to fix errors or problems that you might encounter when using Log Hub.</p>"},{"location":"implementation-guide/trouble-shooting/#error-failed-to-assume-service-linked-role-arnxxxawsserviceroleforappsync","title":"Error: Failed to assume service-linked role <code>arn:x:x:x:/AWSServiceRoleForAppSync</code>","text":"<p>The reason for this error is that the account has never used the AWS AppSync service. You can deploy the solution's CloudFormation template again. AWS has already created the role automatically when you encountered the error. </p> <p>You can also go to AWS CloudShell or the local terminal and run the following AWS CLI command to Link AppSync Role</p> <pre><code>aws iam create-service-linked-role --aws-service-name appsync.amazonaws.com\n</code></pre>"},{"location":"implementation-guide/trouble-shooting/#error-unable-to-add-backend-role","title":"Error: Unable to add backend role","text":"<p>Log Hub only supports AOS domain with Fine-grained access control enabled. You need to go to AOS console, and edit the Access policy for the AOS domain.</p>"},{"location":"implementation-guide/trouble-shooting/#erroruser-xxx-is-not-authorized-to-perform-stsassumerole-on-resource","title":"Error\uff1aUser xxx is not authorized to perform sts:AssumeRole on resource","text":"<p>If you see this error, please make sure you have entered the correct information during cross account setup, and then please wait for several minutes.</p> <p>Log Hub uses AssumeRole for cross-account access. This is the best practice to temporary access the AWS resources in your sub-account.  However, these roles created during cross account setup take seconds or minutes to be affective.</p>"},{"location":"implementation-guide/trouble-shooting/#error-putrecords-api-responded-with-errorinvalidsignatureexception","title":"Error: PutRecords API responded with error='InvalidSignatureException'","text":"<p>Fluent-bit agent reports PutRecords API responded with error='InvalidSignatureException', message='The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.'</p> <p>Please restart the fluent-bit agent. Eg. on EC2 with Amazon Linux2, run command: <pre><code>sudo service fluent-bit restart\n</code></pre></p>"},{"location":"implementation-guide/trouble-shooting/#error-putrecords-api-responded-with-erroraccessdeniedexception","title":"Error: PutRecords API responded with error='AccessDeniedException'","text":"<p>Fluent-bit agent deployed on EKS Cluster reports \"AccessDeniedException\" when sending records to Kinesis. Verify that  the IAM role trust relations are correctly set. With the Log Hub console:</p> <ol> <li>Open the Log Hub console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Choose the EKS Cluster that you want to check.</li> <li>Click the IAM Role ARN which will open the IAM Role in AWS Console.</li> <li>Choose the Trust relationships to verify that the OIDC Provider, the service account namespace and conditions are correctly set.</li> </ol> <p>You can get more information from Amazon EKS IAM role configuration</p>"},{"location":"implementation-guide/trouble-shooting/#my-cloudformation-stack-is-stuck-on-deleting-an-awslambdafunction-resource-when-i-update-the-stack-how-to-resolve-it","title":"My CloudFormation stack is stuck on deleting an <code>AWS::Lambda::Function</code> resource when I update the stack. How to resolve it?","text":"<p> The Lambda function resides in a VPC, and you need to wait for the associated ENI resource to be deleted.</p>"},{"location":"implementation-guide/trouble-shooting/#the-agent-status-is-offline-after-i-restart-the-ec2-instance-how-can-i-make-it-auto-start-on-instance-restart","title":"The agent status is offline after I restart the EC2 instance, how can I make it auto start on instance restart?","text":"<p>This usually happens if you have installed the log agent, but restart the instance before you create any Log Ingestion. The Logging Agent will auto restart if there is at least one Log Ingestion. If you have a log ingestion, but the problem still exists, you can use <code>systemctl status fluent-bit</code> to check its status inside the instance.</p>"},{"location":"implementation-guide/trouble-shooting/#i-have-switched-to-global-tenant-however-i-still-cannot-find-the-dashboard-in-opensearch","title":"I have switched to Global tenant. However, I still cannot find the dashboard in OpenSearch.","text":"<p>This is usually because Log Hub received 403 error from OpenSearch when creating the index template and dashboard. This  can be fixed by re-run the Lambda function manually by following the steps below:</p> <p>With the Log Hub console:</p> <ol> <li>Open the Log Hub console, and find the AWS Service Log pipeline which has this issue.</li> <li>Copy the first 5 characters from the ID section. Eg. you should copy <code>c169c</code> from ID <code>c169cb23-88f3-4a7e-90d7-4ab4bc18982c</code></li> <li>Go to AWS Console &gt; Lambda. Paste in function filters. This will filter in all the lambda function created for this AWS Service Log ingestion.</li> <li>Click the Lambda function whose name contains \"OpenSearchHelperFn\".</li> <li>In the Test tab, create a new event with any Event name.</li> <li>Click the Test button to trigger the Lambda, and wait the lambda function to complete.</li> <li>The dashboard should be available in OpenSearch</li> </ol>"},{"location":"implementation-guide/trouble-shooting/#how-to-install-log-agent-on-centos-7","title":"How to install log agent on CentOS 7","text":"<ol> <li> <p>Log in to your CentOS 7 machine and install SSM Agent manually.</p> <p><pre><code>sudo yum install -y http://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\nsudo systemctl enable amazon-ssm-agent\nsudo systemctl start amazon-ssm-agent\n</code></pre> 2. Go to the Instance Group panel of Log Hub console, create Instance Group, select the CentOS 7 machine, click Install log agent and wait for its status to be offline. 3. Login to CentOS 7 and install fluent-bit 1.9.3 manually.</p> <p><pre><code>export RELEASE_URL=${FLUENT_BIT_PACKAGES_URL:-https://packages.fluentbit.io}\nexport RELEASE_KEY=${FLUENT_BIT_PACKAGES_KEY:-https://packages.fluentbit.io/fluentbit.key}\n\nsudo rpm --import $RELEASE_KEY\ncat &lt;&lt; EOF | sudo tee /etc/yum.repos.d/fluent-bit.repo\n[fluent-bit]\nname = Fluent Bit\nbaseurl = $RELEASE_URL/centos/VERSION_ARCH_SUBSTR\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=$RELEASE_KEY\nenabled=1\nEOF\nsudo sed -i 's|VERSION_ARCH_SUBSTR|\\$releasever/\\$basearch/|g' /etc/yum.repos.d/fluent-bit.repo\nsudo yum install -y fluent-bit-1.9.3-1\n\n# Modify the configuration file\nsudo sed -i 's/ExecStart.*/ExecStart=\\/opt\\/fluent-bit\\/bin\\/fluent-bit -c \\/opt\\/fluent-bit\\/etc\\/fluent-bit.conf/g' /usr/lib/systemd/system/fluent-bit.service\nsudo systemctl daemon-reload\nsudo systemctl enable fluent-bit\nsudo systemctl start fluent-bit\n</code></pre> 4. Go back to the Instance Groups panel of the Log Hub console and wait for the CentOS 7 machine status to be Online and proceed to create the instance group.</p> </li> </ol>"},{"location":"implementation-guide/uninstall/","title":"Uninstall the Log Hub","text":"<p>Warning</p> <p>You will encounter IAM role missing error if you delete the Log Hub main stack before you delete the log pipelines. Log Hub console launches additional CloudFormation stacks to ingest logs. If you want to uninstall the Log Hub solution.  We recommend you to delete log pipelines (incl. AWS Service log pipelines and application log pipelines) before uninstall the solution. </p>"},{"location":"implementation-guide/uninstall/#step-1-delete-application-log-pipelines","title":"Step 1. Delete Application Log Pipelines","text":"<p>Important</p> <p>Please delete all the log ingestion before deleting an application log pipeline.</p> <ol> <li>Go to the Log Hub console, in the left sidebar, choose Application Log.</li> <li>Click the application log pipeline to view details.</li> <li>In the ingestion tab, delete all the application log ingestion in the pipeline.</li> <li>Uninstall/Disable the Fluent Bit agent.<ul> <li>EC2 (Optional): after removing the log ingestion from EC2 instance group. Fluent Bit will automatically stop ship logs, it is optional for you to stop the Fluent Bit in your instances. Here are the command for stopping Fluent Bit agent.       <pre><code>   sudo service fluent-bit stop\n   sudo systemctl disable fluent-bit.service\n</code></pre></li> <li>EKS DaemonSet (Mandatory): if you have chosen to deploy the Fluent Bit agent using DaemonSet, you need to delete your Fluent Bit agent. Otherwise, the agent will continue ship logs to Log Hub pipelines.       <pre><code>   kubectl delete -f ~/fluent-bit-logging.yaml\n</code></pre></li> <li>EKS SideCar (Mandatory): please remove the fluent-bit agent in your <code>.yaml</code> file, and restart your pod.</li> </ul> </li> <li>Delete the Application Log pipeline.</li> <li>Repeat step 2 to Step 5 to delete all your application log pipelines.</li> </ol>"},{"location":"implementation-guide/uninstall/#step-2-delete-aws-service-log-pipelines","title":"Step 2. Delete AWS Service Log Pipelines","text":"<ol> <li>Go to the Log Hub console, in the left sidebar, choose AWS Service Log.</li> <li>Select and delete the AWS Service Log Pipeline one by one.</li> </ol>"},{"location":"implementation-guide/uninstall/#step-3-clean-up-imported-opensearch-domains","title":"Step 3. Clean up imported OpenSearch domains","text":"<ol> <li>Delete Access Proxy, if you have created the proxy using Log Hub console.</li> <li>Delete Alarms, if you have created alarms using Log Hub console.</li> <li>Delete VPC peering Connection between Log Hub's VPC and OpenSearch's VPC.<ul> <li>Go to AWS VPC Console</li> <li>Click Peering connections in left sidebar.</li> <li>Find and delete the VPC peering connection between the Log Hub's VPC and OpenSearch's VPC. You may not have Peering Connections if you did not use the \"Automatic\" mode when importing OpenSearch domains.</li> </ul> </li> <li>(Optional) Remove imported OpenSearch Domains. (This will not delete the Amazon OpenSearch domain in the AWS account.)</li> </ol>"},{"location":"implementation-guide/uninstall/#step-4-delete-log-hub-stack","title":"Step 4. Delete Log Hub stack","text":"<ol> <li>Go to the CloudFormation console.</li> <li>Find CloudFormation Stack of the Log Hub solution.</li> <li> <p>(Optional) Delete S3 buckets created by Log Hub.</p> <p>Important</p> <p>The S3 bucket whose name contains LoggingBucket is the centralized bucket for your AWS service log. You might have enabled AWS Services to send logs to this S3 bucket. Deleting this bucket will cause AWS Services failed to send logs.</p> <ul> <li>Click the CloudFormation stack of the Log Hub solution, and click Resources tab.</li> <li>In search bar, enter <code>AWS::S3::Bucket</code>. This will show all the S3 buckets created by Log Hub solution, and the Physical ID field is the S3 bucket name</li> <li>Go to S3 console, and find the S3 bucket using the bucket name. Empty and Delete the S3 bucket.</li> </ul> </li> <li> <p>Delete the CloudFormation Stack of the Log Hub solution</p> </li> </ol>"},{"location":"implementation-guide/upgrade/","title":"Upgrade Log Hub","text":"<p>Time to upgrade: Approximately 20 minutes</p>"},{"location":"implementation-guide/upgrade/#upgrade-overview","title":"Upgrade Overview","text":"<p>Use the following steps to upgrade the solution on AWS console. </p> <ul> <li>Step 1. Update the CloudFormation Stack</li> <li>Step 2. Trigger Lambda to refresh web config</li> <li>Step 3. Create an invalidation on CloudFront</li> <li>Step 4. Refresh the web console</li> </ul>"},{"location":"implementation-guide/upgrade/#step-1-update-the-cloudformation-stack","title":"Step 1. Update the CloudFormation stack","text":"<ol> <li> <p>Go to the AWS CloudFormation console.</p> </li> <li> <p>Select the Log Hub main stack, and click the Update button.</p> </li> <li> <p>Choose Replace current template, and enter the specific Amazon S3 URL according to your initial deployment type. Refer to Deployment Overview for more details.</p> Type Link Launch with Cogito User Pool &amp; New VPC <code>https://aws-gcr-solutions.s3.amazonaws.com/log-hub/latest/LogHub.template</code> Launch with Cognito User Pool &amp; Existing VPC <code>https://aws-gcr-solutions.s3.amazonaws.com/log-hub/latest/LogHubFromExistingVPC.template</code> Launch with OpenID Connect &amp; New VPC <code>https://aws-gcr-solutions.s3.amazonaws.com/log-hub/latest/LogHubWithOIDC.template</code> Launch with OpenID Connect &amp; Existing VPC <code>https://aws-gcr-solutions.s3.amazonaws.com/log-hub/latest/LogHubFromExistingVPCWithOIDC.template</code> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>On Configure stack options page, choose Next.</p> </li> <li> <p>On Review page, review and confirm the settings. Check the box I acknowledge that AWS CloudFormation might create IAM resources.</p> </li> <li> <p>Choose Update stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a UPDATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/upgrade/#step-2-create-an-invalidation-on-cloudfront","title":"Step 2. Create an invalidation on CloudFront","text":"<p>CloudFront has cached an old version of Log Hub console at its pop locations. We need to create an invalidation on the CloudFront console to  force the deletion of cache. You must do this after thew console configuration file being generated.</p> <ol> <li> <p>Go to the AWS CloudFront console.</p> </li> <li> <p>Choose the Distribution of Log Hub. The Description is like <code>SolutionName - Web Console Distribution (RegionName)</code>.</p> </li> <li> <p>On the Invalidation page, click Create invalidation, and create an invalidation with <code>/*</code>.</p> </li> </ol>"},{"location":"implementation-guide/upgrade/#step-3-refresh-the-web-console","title":"Step 3. Refresh the web console","text":"<p>Now you have completed all the upgrade steps. Please click the refresh button in your browser. You can check the new version number in the bottom right corner of the Log Hub console.</p>"},{"location":"implementation-guide/upgrade/#upgrade-notice","title":"Upgrade Notice","text":""},{"location":"implementation-guide/upgrade/#application-logs-from-ec2","title":"Application Logs from EC2","text":"<p>Log Hub has an updated IAM policy after v1.1.0. If you have created an Application Log Pipeline  in Log Hub V1.0.X, and want to create a new Application Log Ingestion in v1.1.0 or later versions, you will receive an upgrade notice popup:</p> <p></p> <p>Click the Upgrade button to upgrade your Application Log Pipeline to the current version,  This upgrade will not affect your existing log ingestion which were created in Log Hub V1.0.X. However, please make sure you have updated IAM Policy to the EC2 instance profile before creating a new ingestion.</p>"},{"location":"implementation-guide/upgrade/#application-logs-from-eks","title":"Application Logs from EKS","text":"<p>Log Hub has updated the default architecture for ingesting application logs from EKS. In Log Hub V1.1.0 or later version, by default, Log Hub ingests EKS pod logs directly into Amazon OpenSearch. This upgrade will not affect your existing log ingestion created in Log Hub V1.0.x. </p> <p>For example, if you have log ingestion created in Log Hub V1.0.x, and have created a new log ingestion in Log Hub V1.1.0. Log Hub will send the logs ingested in V1.0.x to OpenSearch through Amazon Kinesis Data Stream, and send the logs ingested in V1.1.0 to OpenSearch directly.</p>"},{"location":"implementation-guide/applications/","title":"Application Log Analytics Pipelines","text":"<p>Log Hub supports ingesting application logs from EC2 instances, EKS clusters, and Syslog.</p> <ul> <li>For EC2 instances, Log Hub will automatically install log agent (Fluent Bit 1.9), collect application logs on EC2 instances and then send logs into Amazon OpenSearch.</li> <li>For EKS clusters, Log Hub will generate all-in-one configuration file for customers to deploy the log agent (Fluent Bit 1.9) as a DaemonSet or Sidecar. After log agent is deployed, Log Hub will start collecting pod logs and send to Amazon OpenSearch.</li> <li>For Syslog, Log Hub will collect syslog logs through UDP or TCP protocol.</li> </ul>"},{"location":"implementation-guide/applications/#supported-log-formats-and-sources","title":"Supported Log Formats and Sources","text":"Log Format EC2 Instance Group EKS Cluster Syslog Nginx Yes Yes No Apache HTTP Server Yes Yes No JSON Yes Yes Yes Single-line Text Yes Yes Yes Multi-line Text Yes Yes No Multi-line Text (Spring Boot) Yes Yes No Syslog RFC5424/RFC3164 No No Yes Syslog Custom No No Yes <p>In this chapter, you will learn how to create log ingestion for the following log formats:</p> <ul> <li>Apache HTTP server logs</li> <li>Nginx logs</li> <li>Single-line Text logs</li> <li>Multi-line Text logs</li> <li>JSON logs</li> <li>Syslog logs</li> </ul> <p>Before creating log ingestion, you need to:</p> <ul> <li>Create a log source (not applicable for S3 bucket and Syslog)</li> <li>Create an application log pipeline</li> </ul>"},{"location":"implementation-guide/applications/#concepts","title":"Concepts","text":"<p>The following introduce concepts that help you to understand how the application log ingestion works.</p>"},{"location":"implementation-guide/applications/#application-log-analytics-pipeline","title":"Application Log Analytics Pipeline","text":"<p>To collect application logs, a data pipeline is needed. The pipeline not only buffers the data in transmit but also cleans or pre-processes data. For example, transforming IP to Geo location. Currently, Kinesis Data Stream is used as data buffering for EC2 log source.</p>"},{"location":"implementation-guide/applications/#log-ingestion","title":"Log Ingestion","text":"<p>A log ingestion configures the Log Source, Log Type and the Application Log Analytics Pipeline for the log agent used by Log Hub. After that, Log Hub will start collecting certain type of logs from the log source and sending them to Amazon OpenSearch.</p>"},{"location":"implementation-guide/applications/#log-agent","title":"Log Agent","text":"<p>A log agent is a program that reads logs from one location and sends them to another location (for example, OpenSearch).  Currently, Log Hub only supports Fluent Bit 1.9 log agent which is installed automatically. The Fluent Bit agent has a dependency of OpenSSL 1.1. To learn how to install OpenSSL on Linux instances, refer to OpenSSL installation. Please find the supported platforms by Fluent Bit in this link.</p>"},{"location":"implementation-guide/applications/#log-buffer","title":"Log Buffer","text":"<p>Log Buffer is a buffer layer between the Log Agent and OpenSearch clusters. The agent uploads logs into the buffer layer, and then be processed and delivered into the OpenSearch clusters. A buffer layer is a way to protect OpenSearch clusters from overwhelming. This solution provides the following types of buffer layers.</p> <ul> <li> <p>Amazon S3. The log agent periodically uploads logs to an Amazon S3 bucket. The frequency of data delivery to  Amazon S3 is determined by Buffer size (default value is 50 MiB) and Buffer interval (default value is 60 seconds) value  that you configured when creating the application log analytics pipelines. The condition satisfied first triggers data delivery to Amazon S3.  Use this option if you can bear minutes-level latency for log ingestion.</p> </li> <li> <p>Amazon Kinesis Data Streams. The log agent uploads logs to Amazon Kinesis Data Stream in seconds. The frequency  of data delivery to Kinesis Data Streams is determined by Buffer size (10 MiB) and Buffer interval (5 seconds). The  condition satisfied first triggers data delivery to Kinesis Data Streams. Use this option if you need real-time log ingestion.</p> </li> </ul> <p>Log Buffer is optional when creating an application log analytics pipeline. For all types of application logs, this  solution also provides a way to ingest logs without any buffer layers. However, we only recommend this option when you have small log volume, and you are confident that will not encounter thresholds at the OpenSearch side.</p>"},{"location":"implementation-guide/applications/#log-source","title":"Log Source","text":"<p>A Log Source refers to a location where you want Log Hub to collect application logs from. Supported log sources includes:</p> <ul> <li>Instance Group</li> <li>EKS Cluster</li> <li>Syslog</li> </ul>"},{"location":"implementation-guide/applications/#instance-group","title":"Instance Group","text":"<p>An instance group is a collection of EC2 instances from which you want to collect application logs.  Log Hub can help you install the log agent in each instance within a group. You can select arbitrary instances through the user interface, or choose an EC2 Auto Scaling Group.</p>"},{"location":"implementation-guide/applications/#eks-cluster","title":"EKS Cluster","text":"<p>The EKS Cluster in Log Hub refers to the Amazon EKS from which you want to collect pod logs. Log Hub  will guide you to deploy the log agent as a DaemonSet or Sidecar in the EKS Cluster.</p>"},{"location":"implementation-guide/applications/#syslog","title":"Syslog","text":"<p>Log Hub supports collecting syslog logs through UDP or TCP protocol.</p>"},{"location":"implementation-guide/applications/#log-config","title":"Log Config","text":"<p>A Log Config is a configuration that is telling Log Hub where the logs had been stored on Log Source, which types of logs you want to collect, what fields a line of log contains, and types of each field. </p>"},{"location":"implementation-guide/applications/apache/","title":"Apache HTTP server logs","text":"<p>Apache HTTP Server (httpd) is capable of writing error and access log files to a local directory. You can configure Log Hub to ingest Apache HTTP server logs.</p>"},{"location":"implementation-guide/applications/apache/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an AOS domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/apache/#step-1-create-an-apache-http-server-log-config","title":"Step 1: Create an Apache HTTP server log config","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose Apache HTTP server in the log type dropdown menu.</li> <li> <p>In the Apache Log Format section, paste your Apache HTTP server log format configuration. It is in the format of <code>/etc/httpd/conf/httpd.conf</code> and starts with <code>LogFormat</code>.</p> <p>For example: <pre><code>LogFormat \"%h %l %u %t \\\"%r\\\" %&gt;s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined\n</code></pre></p> </li> <li> <p>(Optional) In the Sample log parsing section, paste a sample Apache HTTP server log to verify if the log parsing is successful.</p> <p>For example: <pre><code>127.0.0.1 - - [22/Dec/2021:06:48:57 +0000] \"GET /xxx HTTP/1.1\" 404 196 \"-\" \"curl/7.79.1\"\n</code></pre></p> </li> <li> <p>Choose Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/apache/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":""},{"location":"implementation-guide/applications/apache/#instance-group-as-log-source","title":"Instance Group as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Click on the application pipeline that has been created during the Prerequisites.</li> <li>Go to Permission tab and copy the provided JSON policy.</li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column, and </p> <ul> <li>Click Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your account id.</li> <li>Choose Next, Next, then enter the name for this policy. For example: <code>loghub-ec2-policy</code>.</li> <li>Attach the policy to your EC2 instance profile to allow the log agent have permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group (depends on your Auto Scaling Group's setup, please follow the AWS documentation to update [Launch Template][launch-template] or [Launch Configuration][launch-configuration]).</li> </ul> </li> <li> <p>Click the Create an Ingestion dropdown menu, and select From Instance Group.</p> </li> <li>Select Choose exists and choose Next.</li> <li>Select the instance group you have created during the Prerequisites and choose Next.</li> <li>(Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template.</li> <li>Select Choose exists and select the log config created in previous setup.</li> <li>Choose Next, then choose Create.</li> </ol>"},{"location":"implementation-guide/applications/apache/#eks-cluster-as-log-source","title":"EKS Cluster as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Click the EKS Cluster that has been imported as Log Source during the Prerequisites.</li> <li>Go to App Log Ingestion tab and click Create an Ingestion.<ul> <li>Select Choose exists and choose the application pipeline that has been created during the Prerequisites. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ul> </li> <li>Deploy Fluent-bit log agent following the guide generated by Log Hub. <ul> <li>Select the App Log Ingestion just created.</li> <li>Follow DaemonSet or Sidecar Guide to deploy the log agent.</li> </ul> </li> </ol>"},{"location":"implementation-guide/applications/apache/#step-3-check-built-in-apache-http-server-dashboard-in-opensearch","title":"Step 3: Check built-in Apache HTTP server dashboard in OpenSearch","text":"<p>For Apache HTTP server logs, Log Hub will create a built-in sample dashboard.</p> <ol> <li>Open OpenSearch dashboard in your browser.</li> <li>Go to Dashboard section in the left sidebar.</li> <li>Find the dashboard whose name starts with <code>&lt;the application pipeline&gt;</code>.</li> </ol>"},{"location":"implementation-guide/applications/apache/#sample-dashboard","title":"Sample Dashboard","text":""},{"location":"implementation-guide/applications/create-applog-pipeline/","title":"Create an application pipeline","text":"<ol> <li> <p>Sign in to the Log Hub Console.</p> </li> <li> <p>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</p> </li> <li> <p>Click the Create a pipeline.</p> </li> <li> <p>Specify Index name in lowercase.</p> </li> <li> <p>In the Buffer section, choose S3 or Kinesis Data Streams. If you don't want the buffer layer, choose None. Refer to the Log Buffer for more information about choosing the appropriate buffer layer.</p> <ul> <li>S3 buffer parameters</li> </ul> Parameter Default Description S3 Bucket A log bucket created by the solution Select a bucket to store the log data. S3 Bucket Prefix <code>AppLogs/&lt;index-prefix&gt;/year=%Y/month=%m/day=%d</code> The log agent appends the prefix when delivers the log files to the S3 bucket. Buffer size 50 MiB The maximum size of log data cached at the log agent side before delivering to S3. The size may be smaller if the Buffer interval triggers first. Buffer interval 60 seconds The interval of the log agent to deliver logs to S3. The interval may be shorter, if the log size triggers first. Compression for data records <code>Gzip</code> The log agent compress records before delivering them to the S3 bucket. <ul> <li>Kinesis Data Streams buffer parameters</li> </ul> Parameter Default Description Shard number <code>&lt;requires input&gt;</code> The number of shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. Enable auto scaling? <code>No</code> This solution monitors the utilization of Kinesis Data Streams every 5 minutes, and scale in/out the number of shards automatically. The solution will scale in/out for a maximum of 8 times within 24 hours. Maximum Shard number <code>&lt;requires input&gt;</code> Required if auto scaling is eanbled. The maximum number of shards. <p>Important</p> <p>You may observe duplicated logs in OpenSearch if there is threshold error occurs in Kinesis Data Streams (KDS). This is because the Fluent Bit log agent uploads logs in chunk (contains multiple records), and will retry the chunk if upload failed. Each KDS shard can support up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. Please estimate your log volume and choose an appropriate shard number.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</p> </li> <li> <p>In the Log Lifecycle section, input the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>Add tags if needed.</p> </li> <li> <p>Choose Create.</p> </li> <li> <p>Wait for the application pipeline turning to \"Active\" state.</p> </li> </ol>"},{"location":"implementation-guide/applications/create-log-ingestion/","title":"Create log ingestion","text":""},{"location":"implementation-guide/applications/create-log-ingestion/#instance-group-as-log-source","title":"Instance Group as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Click on the application pipeline that has been created during the Prerequisites.</li> <li>Go to Permission tab and copy the provided JSON policy.</li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column, and </p> <ul> <li>Click Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your account id.</li> <li>Choose Next, Next, then enter the name for this policy. For example: <code>loghub-ec2-policy</code>.</li> <li>Attach the policy to your EC2 instance profile to allow the log agent have permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group (depends on your Auto Scaling Group's setup, please follow the AWS documentation to update Launch Template or Launch Configuration).</li> </ul> </li> <li> <p>Click the Create an Ingestion dropdown menu, and select From Instance Group.</p> </li> <li>Select Choose exists and choose Next.</li> <li>Select the instance group you have created during the Prerequisites and choose Next.</li> <li>(Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template.</li> <li>Select Choose exists and select the log config created in previous setup.</li> <li>Choose Next, then choose Create.</li> </ol>"},{"location":"implementation-guide/applications/create-log-ingestion/#eks-cluster-as-log-source","title":"EKS Cluster as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Click the EKS Cluster that has been imported as Log Source during the Prerequisites.</li> <li>Go to App Log Ingestion tab and click Create an Ingestion.<ul> <li>Select Choose exists and choose the application pipeline that has been created during the Prerequisites. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ul> </li> <li>Deploy Fluent-bit log agent following the guide generated by Log Hub. <ul> <li>Select the App Log Ingestion just created.</li> <li>Follow DaemonSet or Sidecar Guide to deploy the log agent.</li> </ul> </li> </ol>"},{"location":"implementation-guide/applications/create-log-ingestion/#syslog-as-log-source","title":"Syslog as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Click on the application pipeline that has been created during the Prerequisites.</li> <li>Click the Create an Ingestion dropdown menu, and select From Syslog.</li> <li>Fill in all the form fields to specify Syslog Source. You can use both UDP or TCP protocol with custom port number. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol>"},{"location":"implementation-guide/applications/create-log-source/","title":"Create a Log Source","text":"<p>You need to create a log source first before collecting application logs. Log Hub supports the following log sources:</p> <ul> <li>Amazon EC2 Instance Group</li> <li>Amazon EKS cluster</li> <li>Syslog</li> </ul> <p>For more information, see concepts.</p>"},{"location":"implementation-guide/applications/create-log-source/#amazon-ec2-instance-group","title":"Amazon EC2 Instance Group","text":"<p>An instance group means a group of EC2 Linux instances which host the same application. It is a way to associate a Log Config with a group of EC2 instances. Log Hub uses Systems Manager Agent(SSM Agent) to install/configure Fluent Bit agent, and sends log data to Kinesis Data Streams. </p>"},{"location":"implementation-guide/applications/create-log-source/#prerequisites","title":"Prerequisites","text":"<p>Make sure the instances meet the following requirements:</p> <ul> <li>SSM agent is installed on instances. Refer to install SSM agent on EC2 instances for Linux for more details.</li> <li>The <code>AmazonSSMManagedInstanceCore</code> policy is being associated with the instances.</li> <li>The OpenSSL 1.1 or later is installed. Refer to OpenSSL Installation for more details.</li> <li>The instances have network access to AWS Systems Manager.</li> <li>The instances have network access to Amazon Kinesis Data Streams, if you use it as the Log Buffer.</li> <li>The instances have network access to Amazon S3, if you use it as the Log Buffer.</li> <li>The operating system of the instances are supported by Fluent Bit. Refer to Supported Platform.</li> </ul>"},{"location":"implementation-guide/applications/create-log-source/#option-1-select-instances-to-create-an-instance-group","title":"(Option 1) Select instances to create an Instance Group","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Source, choose Instance Group.</li> <li>Click the Create an instance group button.</li> <li>In the Settings section, specify a group name.</li> <li>In the Configuration section, select Instances. You can use up to 5 tags to filter the instances.</li> <li>Verify that all the selected instances \"Pending Status\" is Online.</li> <li>(Optional) If the selected instances \"Pending Status\" are empty, click the Install log agent button and wait for \"Pending Status\" to become Online.</li> <li>(Optional) If you want to ingest logs from another account, select a linked account in the Account Settings section to create an instance group log source from another account.</li> <li>Choose Create.</li> </ol> <p>Known issue</p> <p>Use the Log Hub console to install Fluent Bit agent on Ubuntu instances in Beijing (cn-north-1) and Ningxia (cn-northwest-1) Region will cause installation error. The Fluent Bit assets cannot be downloaded successfully. You need to install the Fluent Bit agent by yourself.</p>"},{"location":"implementation-guide/applications/create-log-source/#option-2-select-an-auto-scaling-group-to-create-an-instance-group","title":"(Option 2) Select an Auto Scaling group to create an Instance Group","text":"<p>When creating an Instance Group with Amazon EC2 Auto Scaling group, the solution will generate a shell script which you should include in the EC2 User Data.  </p> <ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Source, choose Instance Group.</li> <li>Click the Create an instance group button.</li> <li>In the Settings section, specify a group name.</li> <li>In the Configuration section, select Auto Scaling Groups.</li> <li>In the Auto Scaling groups section, select the auto scaling group from which you want to collect logs.</li> <li>(Optional) If you want to ingest logs from another account, select a linked account in the Account Settings section to create an instance group log source from another account.</li> <li>Choose Create.</li> </ol> <p>After you created a Log Ingestion using the Instance Group, you can find the generated Shell Script in the details page.  Copy the shell script and update the User Data of the Auto Scaling Group's launch configurations or launch template. The shell script will automatically install Fluent Bit, SSM agent if needed, and download Fluent Bit configurations. Once you have updated the launch configurations or launch template, you need to start an instance refresh to update the instances within the Auto Scaling group.  The newly launched instances will ingest logs to the OpenSearch cluster or the Log Buffer layer.</p>"},{"location":"implementation-guide/applications/create-log-source/#amazon-eks-cluster","title":"Amazon EKS cluster","text":"<p>The EKS Cluster in Log Hub refers to the Amazon Elastic Kubernetes Service (Amazon EKS) from which you want to collect pod logs. Log Hub will guide you to deploy the log agent as a DaemonSet or Sidecar in the EKS Cluster.</p> <p>Important</p> <ul> <li>Log Hub does not support sending logs in one EKS cluster to more than one Amazon OpenSearch domain at same time.</li> <li>VPC peering connection between the log source EKS and the log destination OpenSearch is required if they are in different VPCs.</li> </ul> <p>Important</p> <p>Please make sure your EKS cluster's VPC is connected to AOS cluster' VPC so that log can be ingested. Refer to VPC Connectivity for more details regarding approaches to connect VPCs.</p> <ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Source, choose EKS Cluster.</li> <li>Click the Import a Cluster button.</li> <li>Choose the EKS Cluster where Log Hub collects logs from.  (Optional) If you want to ingest logs from another account, select a linked account from the Account dropdown to import an EKS log source from another account.</li> <li>Select DaemonSet or Sidecar as log agent's deployment pattern. </li> <li>Choose Next.</li> <li>Specify the Amazon OpenSearch where Log Hub sends the logs to.</li> <li>Follow the guidance to establish a VPC peering connection between EKS's VPC and OpenSearch's VPC.<ul> <li>Create and accept VPC peering connections</li> <li>Update your route tables for a VPC peering connection</li> <li>Update your security groups to reference peer VPC groups</li> </ul> </li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/applications/create-log-source/#syslog","title":"Syslog","text":"<p>Important</p> <p>To ingest logs, make sure your Syslog generator/sender\u2019s subnet is connected to Log Hub\u2019s two private subnets. Refer to VPC Connectivity for more details about how to connect VPCs.</p> <p>Syslog refers to logs generated by Linux instance, routers or network equipments, you can use both UDP or TCP protocol with custom port number to collect syslog in Log Hub.</p>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/","title":"Fluent-bit installation guide","text":"<p>Currently, Log hub uses fluent-bit as the log agent. The prerequisites for different operating system are different.</p>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/#amazon-linux-2","title":"Amazon Linux 2","text":"<pre><code>sudo yum install openssl11\n</code></pre>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/#centos-7","title":"CentOS 7","text":"<pre><code>cd /tmp\nsudo yum install -y unzip gcc perl\ncurl -SL https://github.com/openssl/openssl/archive/OpenSSL_1_1_1-stable.zip -o OpenSSL_1_1_1-stable.zip\nunzip OpenSSL_1_1_1-stable.zip\ncd openssl-OpenSSL_1_1_1-stable/\n./config\nsudo make install\nsudo echo \"/usr/local/lib64/\" &gt;&gt; /etc/ld.so.conf\nsudo ldconfig\n</code></pre>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/#ubuntu","title":"Ubuntu","text":""},{"location":"implementation-guide/applications/fluent-bit-install-guide/#ubuntu-2004","title":"Ubuntu 20.04","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/#ubuntu-1804","title":"Ubuntu 18.04","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/#red-hat-enterprise-linux-85","title":"Red Hat Enterprise Linux 8.5","text":"<p>None</p>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/#debian","title":"Debian","text":""},{"location":"implementation-guide/applications/fluent-bit-install-guide/#debian-gnu10","title":"Debian GNU/10","text":"<pre><code>ln -s  /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/#debian-gnu11","title":"Debian GNU/11","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/applications/fluent-bit-install-guide/#suse-linux-enterprise-server-15","title":"SUSE Linux Enterprise Server 15","text":"<p>None</p>"},{"location":"implementation-guide/applications/include-prerequisites/","title":"Include prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an AOS domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/include-supported-app-logs/","title":"Include supported app logs","text":"Log Format EC2 Instance Group EKS Cluster Syslog Nginx Yes Yes No Apache HTTP Server Yes Yes No JSON Yes Yes Yes Single-line Text Yes Yes Yes Multi-line Text Yes Yes No Multi-line Text (Spring Boot) Yes Yes No Syslog RFC5424/RFC3164 No No Yes Syslog Custom No No Yes"},{"location":"implementation-guide/applications/json/","title":"JSON format logs","text":"<p>You can configure Log Hub to ingest JSON logs.</p>"},{"location":"implementation-guide/applications/json/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an AOS domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/json/#step-1-create-a-json-config","title":"Step 1: Create a JSON config","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose JSON in the log type dropdown list.</li> <li> <p>In the Sample log parsing section, paste a sample JSON log and click Parse log to verify if the log parsing is successful.</p> <p>For example: <pre><code>{\"host\":\"81.95.250.9\", \"user-identifier\":\"-\", \"time\":\"08/Mar/2022:06:28:03 +0000\", \"method\": \"PATCH\", \"request\": \"/clicks-and-mortar/24%2f7\", \"protocol\":\"HTTP/2.0\", \"status\":502, \"bytes\":24337, \"referer\": \"http://www.investorturn-key.net/functionalities/innovative/integrated\"}\n</code></pre></p> </li> <li> <p>Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/json/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":"<p>The steps are similar to creating an application log ingestion for single-line text. Refer to Single-line Text for details.</p>"},{"location":"implementation-guide/applications/json/#step-3-view-your-logs-in-opensearch","title":"Step 3: View your logs in OpenSearch","text":"<ol> <li>Open OpenSearch dashboard in your browser.</li> <li>Create an Index Pattern     <ul> <li>Click the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and type in the index pattern name. Click Next step.</li> <li>Specify time field, and click Create index pattern. </li> </ul> </li> <li>Go to Discover section in the left sidebar.</li> <li>Change active index pattern to <code>&lt;Index name&gt;-*</code>.</li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/","title":"Multi-line Text","text":"<p>You can configure Log Hub to ingest multi-line text logs. Currently, Log Hub supports Spring Boot  style logs or customize the log format using Regular Expression.</p>"},{"location":"implementation-guide/applications/multi-line-text/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an AOS domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/#step-1-create-a-multi-line-text-config","title":"Step 1: Create a Multi-line text config","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose Multi-line Text in the log type dropdown menu.</li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/#java-spring-boot","title":"Java - Spring Boot","text":"<ol> <li> <p>For Java Spring Boot logs, you could provide a simple log format. For example:</p> <pre><code>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%thread] %logger : %msg%n\n</code></pre> </li> <li> <p>Paste a sample multi-line log. For example:</p> <pre><code>2022-02-18 10:32:26.400 ERROR [http-nio-8080-exec-1] org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.ArithmeticException: / by zero] with root cause\njava.lang.ArithmeticException: / by zero\n   at com.springexamples.demo.web.LoggerController.logs(LoggerController.java:22)\n   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke\n</code></pre> </li> <li> <p>Choose Parse Log.</p> </li> <li> <p>Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/#custom","title":"Custom","text":"<ol> <li> <p>For other kinds of logs, you could specify the first line regex pattern. For example:</p> <pre><code>(?&lt;time&gt;\\d{4}-\\d{2}-\\d{2}\\s*\\d{2}:\\d{2}:\\d{2}.\\d{3})\\s*(?&lt;level&gt;[\\S]+)\\s*\\[(?&lt;thread&gt;.+)\\]\\s*(?&lt;logger&gt;\\S+)\\s*:\\s*(?&lt;message&gt;[\\s\\S]+)\n</code></pre> </li> <li> <p>Paste a sample multi-line log. For example:</p> <pre><code>2022-02-18 10:32:26.400 ERROR [http-nio-8080-exec-1] org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.ArithmeticException: / by zero] with root cause\njava.lang.ArithmeticException: / by zero\n   at com.springexamples.demo.web.LoggerController.logs(LoggerController.java:22)\n   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke\n</code></pre> </li> <li> <p>Choose Parse Log.</p> </li> <li> <p>Check if each fields type mapping are correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/multi-line-text/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":"<p>The steps are similar to creating an application log ingestion for single-line text. Refer to Single-line Text for details.</p> <p>Note</p> <p>Currently, Log Hub does not support collect Multi-line text logs from S3 Bucket as Log Source.</p>"},{"location":"implementation-guide/applications/multi-line-text/#step-3-view-logs-in-opensearch","title":"Step 3: View logs in OpenSearch","text":"<ol> <li>Open OpenSearch console in your browser.</li> <li>Create an Index Pattern     <ul> <li>Click the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and type in the index pattern name. Click Next step.</li> <li>Specify time field, and click Create index pattern. </li> </ul> </li> <li>Go to Discover section in the left sidebar.</li> <li>Change active index pattern to <code>&lt;the application pipeline&gt;-*</code>.</li> </ol>"},{"location":"implementation-guide/applications/nginx/","title":"Nginx","text":"<p>Nginx is capable of writing error and access log files to a local directory. You can configure Log Hub to ingest Nginx logs.</p>"},{"location":"implementation-guide/applications/nginx/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an AOS domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/nginx/#step-1-create-a-nginx-log-config","title":"Step 1: Create a Nginx log config","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose Nginx in the log type dropdown menu.</li> <li> <p>In the Nginx Log Format section, paste your Nginx log format configuration It is in the format of <code>/etc/nginx/nginx.conf</code> and starts with <code>log_format</code>.</p> <p>For example:    <pre><code>log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n'$status $body_bytes_sent \"$http_referer\" '\n'\"$http_user_agent\" \"$http_x_forwarded_for\"';\n</code></pre></p> </li> <li> <p>(Optional) In the Sample log parsing section, paste a sample Nginx log to verify if the log parsing is successful.</p> <p>For example:    <pre><code>127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\"\n</code></pre></p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only. </p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/nginx/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":""},{"location":"implementation-guide/applications/nginx/#instance-group-as-log-source","title":"Instance Group as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Click on the application pipeline that has been created during the Prerequisites.</li> <li>Go to Permission tab and copy the provided JSON policy.</li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column, and </p> <ul> <li>Click Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your account id.</li> <li>Choose Next, Next, then enter the name for this policy. For example: <code>loghub-ec2-policy</code>.</li> <li>Attach the policy to your EC2 instance profile to allow the log agent have permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group (depends on your Auto Scaling Group's setup, please follow the AWS documentation to update [Launch Template][launch-template] or [Launch Configuration][launch-configuration]).</li> </ul> </li> <li> <p>Click the Create an Ingestion dropdown menu, and select From Instance Group.</p> </li> <li>Select Choose exists and choose Next.</li> <li>Select the instance group you have created during the Prerequisites and choose Next.</li> <li>(Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template.</li> <li>Select Choose exists and select the log config created in previous setup.</li> <li>Choose Next, then choose Create.</li> </ol>"},{"location":"implementation-guide/applications/nginx/#eks-cluster-as-log-source","title":"EKS Cluster as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Click the EKS Cluster that has been imported as Log Source during the Prerequisites.</li> <li>Go to App Log Ingestion tab and click Create an Ingestion.<ul> <li>Select Choose exists and choose the application pipeline that has been created during the Prerequisites. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ul> </li> <li>Deploy Fluent-bit log agent following the guide generated by Log Hub. <ul> <li>Select the App Log Ingestion just created.</li> <li>Follow DaemonSet or Sidecar Guide to deploy the log agent.</li> </ul> </li> </ol>"},{"location":"implementation-guide/applications/nginx/#step-3-check-built-in-nginx-dashboard-in-opensearch","title":"Step 3: Check built-in Nginx dashboard in OpenSearch","text":"<p>For Nginx logs, Log Hub creates a built-in sample dashboard.</p> <ol> <li>Open OpenSearch dashboard in your browser.</li> <li>Go to Dashboard section in the left sidebar.</li> <li>Find the dashboard which name starts with <code>&lt;the application pipeline&gt;</code>.</li> </ol>"},{"location":"implementation-guide/applications/nginx/#sample-dashboard","title":"Sample Dashboard","text":""},{"location":"implementation-guide/applications/single-line-text/","title":"Single-line Text","text":"<p>Log Hub uses custom Ruby Regular Expression to parse logs. It supports both single-line log format and multiple input format.</p> <p>You can configure Log Hub to ingest single-line text logs.</p>"},{"location":"implementation-guide/applications/single-line-text/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an AOS domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#step-1-create-a-single-line-text-config","title":"Step 1: Create a Single-line text config","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Specify Log Path. You can use <code>,</code> to separate multiple paths.</li> <li>Choose Single-line Text in the log type dropdown menu.</li> <li> <p>Write the regular expression in Rubular to validate first and enter the value. For example:</p> <pre><code>(?&lt;remote_addr&gt;\\S+)\\s*-\\s*(?&lt;remote_user&gt;\\S+)\\s*\\[(?&lt;time_local&gt;\\d+/\\S+/\\d+:\\d+:\\d+:\\d+)\\s+\\S+\\]\\s*\"(?&lt;request_method&gt;\\S+)\\s+(?&lt;request_uri&gt;\\S+)\\s+\\S+\"\\s*(?&lt;status&gt;\\S+)\\s*(?&lt;body_bytes_sent&gt;\\S+)\\s*\"(?&lt;http_referer&gt;[^\"]*)\"\\s*\"(?&lt;http_user_agent&gt;[^\"]*)\"\\s*\"(?&lt;http_x_forwarded_for&gt;[^\"]*)\".*\n</code></pre> </li> <li> <p>In the Sample log parsing section, paste a sample Single-line text log and click Parse log to verify if the log parsing is successful. For example:</p> <pre><code>127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\"\n</code></pre> </li> <li> <p>Check if each fields type mapping is correct. Change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types. </p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":""},{"location":"implementation-guide/applications/single-line-text/#instance-group-as-log-source","title":"Instance Group as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Click on the application pipeline that has been created during the Prerequisites.</li> <li>Go to Permission tab and copy the provided JSON policy.</li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column, and </p> <ul> <li>Click Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your account id.</li> <li>Choose Next, Next, then enter the name for this policy. For example: <code>loghub-ec2-policy</code>.</li> <li>Attach the policy to your EC2 instance profile to allow the log agent have permissions to send logs to the application log pipeline. If you are using Auto Scaling group, you need to update the IAM instance profile associated with the Auto Scaling Group (depends on your Auto Scaling Group's setup, please follow the AWS documentation to update Launch Template or Launch Configuration).</li> </ul> </li> <li> <p>Click the Create an Ingestion dropdown menu, and select From Instance Group.</p> </li> <li>Select Choose exists and choose Next.</li> <li>Select the instance group you have created during the Prerequisites and choose Next.</li> <li>(Auto Scaling Group only) If your instance group is created based on an Auto Scaling Group, after ingestion status become \"Created\", then you can find the generated Shell Script in the instance group's detail page. Copy the shell script and update the User Data of the Auto Scaling Launch configurations or Launch template.</li> <li>Select Choose exists and select the log config created in previous setup.</li> <li>Choose Next, then choose Create.</li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#eks-cluster-as-log-source","title":"EKS Cluster as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Source, choose EKS Clusters.</li> <li>Click the EKS Cluster that has been imported as Log Source during the Prerequisites.</li> <li>Go to App Log Ingestion tab and click Create an Ingestion.<ul> <li>Select Choose exists and choose the application pipeline that has been created during the Prerequisites. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ul> </li> <li>Deploy Fluent-bit log agent following the guide generated by Log Hub. <ul> <li>Select the App Log Ingestion just created.</li> <li>Follow DaemonSet or Sidecar Guide to deploy the log agent.</li> </ul> </li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#syslog-as-log-source","title":"Syslog as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Click on the application pipeline that has been created during the Prerequisites.</li> <li>Click the Create an Ingestion dropdown menu, and select From Syslog.</li> <li>Fill in all the form fields to specify Syslog Source. You can use both UDP or TCP protocol with custom port number. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol>"},{"location":"implementation-guide/applications/single-line-text/#step-3-view-logs-in-opensearch","title":"Step 3: View logs in OpenSearch","text":"<ol> <li>Open OpenSearch console in your browser.</li> <li>Create an Index Pattern     <ul> <li>Click the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and type in the index pattern name. Click Next step.</li> <li>Specify time field, and click Create index pattern. </li> </ul> </li> <li>Go to Discover section in the left sidebar.</li> <li>Change active index pattern to <code>&lt;the application pipeline&gt;-*</code>.</li> </ol>"},{"location":"implementation-guide/applications/syslog/","title":"Syslog","text":"<p>Syslog is used as a standard to produce, forward and collect logs produced on a Linux instance, routers or network equipments. You can configure Log Hub to ingest syslogs.</p> <p>Important</p> <p>Please make sure your Syslog generator/sender's subnet is connected to Log Hub' two private subnets so that log can be ingested, you need to use VPC Peering Connection or Transit Gateway to connect these VPCs.</p>"},{"location":"implementation-guide/applications/syslog/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have done the following:</p> <ol> <li>Import an AOS domain.</li> <li>Create a log source.</li> <li>Create an application log pipeline.</li> </ol>"},{"location":"implementation-guide/applications/syslog/#step-1-create-a-syslog-config","title":"Step 1: Create a Syslog config","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Resources, choose Log Config.</li> <li>Click the Create a log config button.</li> <li>Specify Config Name.</li> <li>Here we choose Syslog in the log type dropdown menu for example. But Log Hub also support Syslog with JSON format and Single-Line test format, you can refer to the corresponding tutorial to create a log config.</li> </ol>"},{"location":"implementation-guide/applications/syslog/#rfc5424","title":"RFC5424","text":"<ol> <li> <p>Paste a sample RFC5424 log. For example:</p> <pre><code>&lt;35&gt;1 2013-10-11T22:14:15Z client_machine su - - - 'su root' failed for joe on /dev/pts/2\n</code></pre> </li> <li> <p>Choose Parse Log.</p> </li> <li> <p>Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details. For example:</p> <pre><code>%Y-%m-%dT%H:%M:%SZ\n</code></pre> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/syslog/#rfc3164","title":"RFC3164","text":"<ol> <li> <p>Paste a sample RFC3164 log. For example:</p> <pre><code>&lt;35&gt;Oct 12 22:14:15 client_machine su: 'su root' failed for joe on /dev/pts/2\n</code></pre> </li> <li> <p>Choose Parse Log.</p> </li> <li> <p>Check if each fields type mapping is correct. You can change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types.</p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> <p>Since there is no year in the timestamp of RFC3164, it cannot be displayed as a time histogram in the discover interface of Amazon OpenSearch.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details. For example:</p> <pre><code>%b %m %H:%M:%S\n</code></pre> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/syslog/#custom","title":"Custom","text":"<ol> <li> <p>In the Syslog Format section, paste your Syslog log format configuration. It is in the format of <code>/etc/rsyslog.conf</code> and starts with <code>template</code> or <code>$template</code>. The format syntax follows Syslog Message Format.  For example:</p> <pre><code>&lt;%pri%&gt;1 %timestamp:::date-rfc3339% %HOSTNAME% %app-name% %procid% %msgid% %msg%\\n\n</code></pre> </li> <li> <p>In the Sample log parsing section, paste a sample Nginx log to verify if the log parsing is successful. For example:     <pre><code>&lt;35&gt;1 2013-10-11T22:14:15.003Z client_machine su - - 'su root' failed for joe on /dev/pts/2\n</code></pre></p> </li> <li> <p>Check if each fields type mapping is correct. Change the type by selecting the dropdown menu in the second column. For all supported types, see Data Types. </p> <p>Note</p> <p>You must specify the datetime of the log using key \u201ctime\u201d. If not specified, system time will be added.</p> </li> <li> <p>Specify the Time format. The format syntax follows strptime. Check this for details.</p> </li> <li> <p>(Optional) In the Filter section, you add some conditions to filter logs at the log agent side. The solution will ingest logs that match ALL the specified conditions only.</p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"implementation-guide/applications/syslog/#step-2-create-an-application-log-ingestion","title":"Step 2: Create an application log ingestion","text":""},{"location":"implementation-guide/applications/syslog/#syslog-as-log-source","title":"Syslog as Log Source","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the left sidebar, under Log Analytics Pipelines, choose Application Log.</li> <li>Click on the application pipeline that has been created during the Prerequisites.</li> <li>Click the Create an Ingestion dropdown menu, and select From Syslog.</li> <li>Fill in all the form fields to specify Syslog Source. You can use both UDP or TCP protocol with custom port number. Choose Next.</li> <li>Select the log config created in previous setup, and choose Next.</li> <li>Add tags as needed, then choose Create to finish creating an ingestion.</li> </ol>"},{"location":"implementation-guide/applications/syslog/#step-3-config-the-syslog-generator-to-send-the-log-to-log-hub","title":"Step 3: Config the Syslog generator to send the log to Log Hub","text":"<ol> <li>Click on the Ingestion ID that has been created during the Step 2.</li> <li>For Rsyslog user, follow the Syslog Configuration Guide to config the Rsyslog agent. You can also get the NLB DNS Name in this page.</li> </ol>"},{"location":"implementation-guide/applications/syslog/#step-4-view-logs-in-opensearch","title":"Step 4: View logs in OpenSearch","text":"<ol> <li>Open OpenSearch console in your browser.</li> <li>Create an Index Pattern     <ul> <li>Click the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and type in the index pattern name. Click Next step.</li> <li>Specify time field, and click Create index pattern. </li> </ul> </li> <li>Go to Discover section in the left sidebar.</li> <li>Change active index pattern to <code>&lt;the application pipeline&gt;-*</code>.</li> </ol>"},{"location":"implementation-guide/aws-services/","title":"Build AWS Service Log Analytics Pipelines","text":"<p>Log Hub supports ingesting AWS service logs into AOS through log analytics pipelines, which you can build using the Log Hub web console or via a standalone CloudFormation template. </p> <p>Log Hub reads the data source, parse, cleanup/enrich and ingest logs into AOS domains for analysis. Moreover, the solution provides templated dashboards to facilitate log visualization.</p> <p>Important</p> <ul> <li>AWS managed services must be in the same region as Log Hub. To ingest logs from different AWS regions, we recommend using S3 cross-region replication.</li> <li>The solution will rotate the index on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/#supported-aws-services","title":"Supported AWS Services","text":"<p>Most of AWS managed services output logs to Amazon CloudWatch Logs, Amazon S3, Amazon Kinesis Data Streams or Amazon Kinesis Firehose. </p> <p>The following table lists the supported AWS services and the corresponding features.</p> AWS Service Log Type Log Location Automatic Ingestion Built-in Dashboard Amazon CloudTrail N/A S3 Yes Yes Amazon S3 Access logs S3 Yes Yes Amazon RDS/Aurora MySQL Logs CloudWatch Logs Yes Yes Amazon CloudFront Standard access logs S3 Yes Yes Application Load Balancer Access logs S3 Yes Yes AWS WAF Web ACL logs S3 Yes Yes AWS Lambda N/A CloudWatch Logs Yes Yes Amazon VPC Flow logs S3 Yes Yes AWS Config N/A S3 Yes Yes <ul> <li>Automatic Ingestion: The solution detects the log location of the resource automatically and then reads the logs.</li> <li>Built-in Dashboard: An out-of-box dashboard for the specified AWS service. The solution will automatically ingest a dashboard into the AOS.</li> </ul> <p>Most of supported AWS services in Log Hub offers built-in dashboard when creating the log analytics pipelines. You go to the OpenSearch Dashboards to view the dashboards after the pipeline being provisioned.</p> <p>In this chapter, you will learn how to create log ingestion and dashboards for the following AWS services:</p> <ul> <li>Amazon CloudTrail</li> <li>Amazon S3</li> <li>Amazon RDS/Aurora</li> <li>Amazon CloudFront</li> <li>AWS Lambda</li> <li>Elastic Load Balancing</li> <li>AWS WAF</li> <li>Amazon VPC</li> <li>AWS Config</li> </ul>"},{"location":"implementation-guide/aws-services/cloudfront/","title":"Amazon CloudFront Logs","text":"<p>CloudFront standard logs provide detailed records about every request made to a distribution. </p>"},{"location":"implementation-guide/aws-services/cloudfront/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The CloudFront logging bucket must be the same region as the Log Hub solution.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/cloudfront/#using-the-log-hub-console","title":"Using the Log Hub Console","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose Amazon CloudFront.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for CloudFront logs enabling. The automatic mode will detect the CloudFront log location automatically.<ul> <li>For Automatic mode, choose the CloudFront distribution from the dropdown list.</li> <li>For Manual mode, enter the CloudFront Distribution ID and CloudFront Standard Log location.</li> <li>(Optional) If you are ingesting CloudFront logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix is the CloudFront distribution ID.</li> <li>In the Log Lifecycle section, input the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/cloudfront/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - CloudFront Standard Log Ingestion template in the AWS Cloud.</p> Launch in AWS Console Download Template AWS standard regions Template AWS China regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. Plugins <code>&lt;Optional input&gt;</code> List of plugins delimited by comma, leave blank if no available plugins to use. Validate inputs are <code>user_agent</code>, <code>geo_ip</code>. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/cloudfront/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/cloudtrail/","title":"CloudTrail Logs","text":"<p>Amazon CloudTrail monitors and records account activity across your AWS infrastructure. It outputs all the data to the specified S3 bucket. </p>"},{"location":"implementation-guide/aws-services/cloudtrail/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The CloudTrail region must be the same as the solution region.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/cloudtrail/#using-the-log-hub-console","title":"Using the Log Hub console","text":"<ol> <li>Sign in to the Log Hub console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log. </li> <li>Click the Create a log ingestion button.</li> <li>In the AWS Services section, choose Amazon CloudTrail.</li> <li>Choose Next.</li> <li>Under Specify settings, for Trail, select one from the dropdown list. (Optional) If you are ingesting CloudTrail logs from another account, select a linked account from the Account dropdown list first.</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain. </li> <li>Choose Yes for Sample dashboard if you want to ingest an associated built-in AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix is your trail name.</li> <li>In the Log Lifecycle section, enter the number of days to manage the AOS index lifecycle. Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/cloudtrail/#using-the-standalone-cloudformation-stack","title":"Using the standalone CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - CloudTrail Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/cloudtrail/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/config/","title":"AWS Config Logs","text":"<p>By default, AWS Config delivers configuration history and snapshot files to your Amazon S3 bucket.</p>"},{"location":"implementation-guide/aws-services/config/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>AWS Config must be enabled in the same region as the Log Hub solution.</li> <li>The Amazon S3 bucket region must be the same as the Log Hub solution.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/config/#using-the-log-hub-console","title":"Using the Log Hub Console","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose AWS Config Logs.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for Log creation.<ul> <li>For Automatic mode, make sure the S3 bucket location is correct, and enter the AWS Config Name. </li> <li>For Manual mode, enter the AWS Config Name and Log location.</li> <li>(Optional) If you are ingesting AWS Config logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated built-in AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix the AWS Config Name you entered in previous steps.</li> <li>In the Log Lifecycle section, enter the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/config/#using-the-standalone-cloudformation-stack","title":"Using the standalone CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - AWS Config Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/config/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/elb/","title":"Application Load Balancing (ALB) Logs","text":"<p>ALB Access logs provide access logs that capture detailed information about requests sent to your load balancer. ALB publishes a log file for each load  balancer node every 5 minutes. </p>"},{"location":"implementation-guide/aws-services/elb/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The ELB logging bucket must be the same as the Log Hub solution.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/elb/#using-the-log-hub-console","title":"Using the Log Hub Console","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose Elastic Load Balancer.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual.<ul> <li>For Automatic mode, choose an application load balancer in the dropdown list. (If the selected ALB access log is not enabled, click Enable to enable the ALB access log.)</li> <li>For Manual mode, enter the Application Load Balancer identifier and Log location.</li> <li>(Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix is the <code>Load Balancer Name</code>.</li> <li>In the Log Lifecycle section, input the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/elb/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - ELB Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS standard regions Template AWS China regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. Plugins <code>&lt;Optional input&gt;</code> List of plugins delimited by comma, leave blank if no available plugins to use. Validate inputs are <code>user_agent</code>, <code>geo_ip</code>. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/elb/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/include-cfn-common/","title":"Include cfn common","text":"<ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/include-cfn-plugins-common/","title":"Include cfn plugins common","text":"<ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. Plugins <code>&lt;Optional input&gt;</code> List of plugins delimited by comma, leave blank if no available plugins to use. Validate inputs are <code>user_agent</code>, <code>geo_ip</code>. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/include-cw-cfn-common/","title":"Include cw cfn common","text":"<ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the Log Hub in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> A S3 bucket name to export the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please link an account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please link an account first). Log Group Names <code>&lt;Requires input&gt;</code> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <code>&lt;requires input&gt;</code> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <code>&lt;requires input&gt;</code> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for SQS encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. 6. Choose Next. </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/aws-services/include-dashboard/","title":"Include dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p>"},{"location":"implementation-guide/aws-services/include-index-pattern/","title":"Include index pattern","text":"<ul> <li>Click the Stack Management in the left sidebar, and select Index Patterns.</li> <li>Choose Create index pattern, and type in the index pattern name. Click Next step.</li> <li>Specify time field, and click Create index pattern.</li> </ul>"},{"location":"implementation-guide/aws-services/include-supported-service-logs/","title":"Include supported service logs","text":"<p>The following table lists the supported AWS services and the corresponding features.</p> AWS Service Log Type Log Location Automatic Ingestion Built-in Dashboard Amazon CloudTrail N/A S3 Yes Yes Amazon S3 Access logs S3 Yes Yes Amazon RDS/Aurora MySQL Logs CloudWatch Logs Yes Yes Amazon CloudFront Standard access logs S3 Yes Yes Application Load Balancer Access logs S3 Yes Yes AWS WAF Web ACL logs S3 Yes Yes AWS Lambda N/A CloudWatch Logs Yes Yes Amazon VPC Flow logs S3 Yes Yes AWS Config N/A S3 Yes Yes <ul> <li>Automatic Ingestion: The solution detects the log location of the resource automatically and then reads the logs.</li> <li>Built-in Dashboard: An out-of-box dashboard for the specified AWS service. The solution will automatically ingest a dashboard into the AOS.</li> </ul>"},{"location":"implementation-guide/aws-services/lambda/","title":"AWS Lambda Logs","text":"<p>AWS Lambda automatically monitors Lambda functions on your behalf and sends function metrics to Amazon CloudWatch. </p>"},{"location":"implementation-guide/aws-services/lambda/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The Lambda region must be the same as the Log Hub solution.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/lambda/#using-the-log-hub-console","title":"Using the Log Hub Console","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose AWS Lambda.</li> <li>Choose Next.</li> <li>Under Specify settings, choose the Lambda function from the dropdown list. (Optional) If you are ingesting logs from another account, select a linked account from the Account dropdown first.</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix is the Lambda function name.</li> <li>In the Log Lifecycle section, input the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/lambda/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - Lambda Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS standard regions Template AWS China regions Template <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the Log Hub in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> A S3 bucket name to export the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please link an account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please link an account first). Log Group Names <code>&lt;Requires input&gt;</code> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <code>&lt;requires input&gt;</code> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <code>&lt;requires input&gt;</code> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for SQS encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. 6. Choose Next. </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/aws-services/lambda/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/rds/","title":"Amazon RDS/Aurora Logs","text":"<p>You can publish database instance logs to Amazon CloudWatch Logs. Then, you can perform real-time analysis of the log data, store the data in highly durable storage, and manage the data with the CloudWatch Logs Agent.</p>"},{"location":"implementation-guide/aws-services/rds/#prerequisites","title":"Prerequisites","text":"<p>Make sure your database logs are enabled. Some databases logs are not enabled by default, and you need to update your database parameters to enable the logs. </p> <p>Refer to How do I enable and monitor logs for an Amazon RDS MySQL DB instance? to learn how to output logs to CloudWatch Logs.</p> <p>The table below lists the requirements for RDS/Aurora MySQL parameters.</p> Parameter Requirement Audit Log The database instance must use a custom option group with the <code>MARIADB_AUDIT_PLUGIN</code> option. General log The database instance must use a custom parameter group with the parameter setting <code>general_log = 1</code> to enable the general log. Slow query log The database instance must use a custom parameter group with the parameter setting <code>slow_query_log = 1</code> to enable the slow query log. Log output The database instance must use a custom parameter group with the parameter setting <code>log_output = FILE</code> to write logs to the file system and publish them to CloudWatch Logs."},{"location":"implementation-guide/aws-services/rds/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The RDS and CloudWatch region must be the same as the Log Hub solution region.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/rds/#using-the-log-hub-console","title":"Using the Log Hub Console","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose Amazon RDS.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for RDS log enabling. The automatic mode will detect your RDS log configurations and ingest logs from CloudWatch.<ul> <li>For Automatic mode, choose the RDS cluster from the dropdown list.</li> <li>For Manual mode, enter the DB identifier, select the Database type and input the CloudWatch log location in Log type and location.</li> <li>(Optional) If you are ingesting RDS/Aurora logs from another account, select a linked account from the Account dropdown first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix is the <code>Database identifier</code>.</li> <li>In the Log Lifecycle section, input the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/rds/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - RDS Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS standard regions Template AWS China regions Template <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the Log Hub in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> A S3 bucket name to export the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the CloudWatch log group. Required for cross-account log ingestion (Please link an account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the CloudWatch log group. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please link an account first). Log Group Names <code>&lt;Requires input&gt;</code> The names of the CloudWatch log group for the logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <code>&lt;requires input&gt;</code> Select at least two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <code>&lt;requires input&gt;</code> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for SQS encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. 6. Choose Next. </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/aws-services/rds/#sample-dashboards","title":"Sample Dashboards","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p>"},{"location":"implementation-guide/aws-services/rds/#rdsaurora-mysql","title":"RDS/Aurora MySQL","text":""},{"location":"implementation-guide/aws-services/s3/","title":"Amazon S3 Logs","text":"<p>Amazon S3 server access logging provides detailed records for the requests made to the bucket. S3 access logs can be enabled and saved in another S3 bucket. </p>"},{"location":"implementation-guide/aws-services/s3/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The S3 Bucket region must be the same as the Log Hub solution region.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/s3/#using-the-log-hub-console","title":"Using the Log Hub Console","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose Amazon S3.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for S3 Access Log enabling. The automatic mode will enable the S3 Access Log and save the logs to a centralized S3 bucket if logging is not enabled yet.<ul> <li>For Automatic mode, choose the S3 bucket from the dropdown list. </li> <li>For Manual mode, enter the Bucket Name and S3 Access Log location.</li> <li>(Optional) If you are ingesting Amazon S3 logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated built-in AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix is your bucket name.</li> <li>In the Log Lifecycle section, enter the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/s3/#using-the-standalone-cloudformation-stack","title":"Using the standalone CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - S3 Access Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/s3/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/vpc/","title":"VPC Flow Logs","text":"<p>VPC Flow Logs enable you to capture information about the IP traffic going to and from network interfaces in your VPC. </p>"},{"location":"implementation-guide/aws-services/vpc/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>Log Hub only supports VPCs who publish the flow log data to an Amazon S3 bucket. The S3 Bucket region must be the same as the Log Hub solution region.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/vpc/#using-the-log-hub-console","title":"Using the Log Hub Console","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose VPC Flow Logs.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual for VPC Flow Log enabling. The automatic mode will enable the VPC Flow Log and save the logs to a centralized S3 bucket if logging is not enabled yet.<ul> <li>For Automatic mode, choose the VPC from the dropdown list. </li> <li>For Manual mode, enter the VPC Name and VPC Flow Logs location.</li> <li>(Optional) If you are ingesting VPC Flow logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated built-in AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix is your VPC name.</li> <li>In the Log Lifecycle section, enter the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/vpc/#using-the-standalone-cloudformation-stack","title":"Using the standalone CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - VPC Flow Logs Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS Standard Regions Template AWS China Regions Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/vpc/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/aws-services/waf/","title":"AWS WAF Logs","text":"<p>WAF Access logs provide detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched.</p>"},{"location":"implementation-guide/aws-services/waf/#create-log-ingestion","title":"Create log ingestion","text":"<p>You can create a log ingestion into AOS either by using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Important</p> <ul> <li>The WAF logging bucket must be the same as the Log Hub solution.</li> <li>WAF Classic logs are not supported in Log Hub. Learn more about migrating rules from WAF Classic to the new AWS WAF.</li> <li>The AOS index is rotated on a daily basis, and cannot be adjusted.</li> </ul>"},{"location":"implementation-guide/aws-services/waf/#using-the-log-hub-console","title":"Using the Log Hub Console","text":"<ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.</li> <li>In the AWS Services section, choose AWS WAF.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic or Manual.<ul> <li>For Automatic mode, choose a Web ACL in the dropdown list. </li> <li>For Manual mode, enter the Web ACL name.</li> <li>(Optional) If you are ingesting WAF logs from another account, select a linked account from the Account dropdown list first.</li> </ul> </li> <li>Specify an Ingest Options. Choose between Sampled Request or Full Request.<ul> <li>For Sampled Request, enter how often you want to ingest sampled requests in minutes.</li> <li>For Full Request. If the Web ACL log is not enabled, click Enable to enable the access log, or enter Log location in Manual mode. Note that, Log Hub will automatically enable logging with a Kinesis Data Firehose stream as destination for your WAF.</li> </ul> </li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard if you want to ingest an associated templated AOS dashboard.</li> <li>You can change the Index Prefix of the target AOS index if needed. The default prefix is the <code>Web ACL Name</code>.</li> <li>In the Log Lifecycle section, input the number of days to manage the AOS index lifecycle. The Log Hub will create the associated Index State Management (ISM) policy automatically for this pipeline.</li> <li>Choose Next.</li> <li>Add tags if needed.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/aws-services/waf/#using-the-cloudformation-stack","title":"Using the CloudFormation Stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - WAF Log Ingestion solution in the AWS Cloud.</p> Launch in AWS Console Download Template AWS standard regions (Full requests) Template AWS China regions (Full requests) Template AWS standard regions (Sampled requests) Template AWS China regions (Sampled requests) Template <ol> <li> <p>Log in to the AWS Management Console and select above button to launch the AWS CloudFormation template. You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> <ul> <li>Full Request only parameters</li> </ul> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the logs. <ul> <li>Sampled Request only parameters</li> </ul> Parameter Default Description WebACL Names <code>&lt;Requires input&gt;</code> The list of WebACL names, delimited by comma. Interval <code>1</code> The default internval (in minutes) to get sampled logs. <ul> <li>Common parameters</li> </ul> Parameter Default Description Log Source Account ID <code>&lt;Optional input&gt;</code> The AWS Account ID of the S3 bucket. Required for cross-account log ingestion (Please add a member account first). By default, the Account ID you logged in at Step 1 will be used. Log Source Region <code>&lt;Optional input&gt;</code> The AWS Region of the S3 bucket. By default, the Region you selected at Step 2 will be used. Log Source Account Assume Role <code>&lt;Optional input&gt;</code> The IAM Role ARN used for cross-account log ingestion. Required for cross-account log ingestion (Please add a member account first). Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. For example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;Requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be <code>&lt;Index Prefix&gt;-&lt;log-type&gt;-&lt;YYYY-MM-DD&gt;</code>. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;Requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will reside in the selected VPC. Subnet IDs <code>&lt;Requires input&gt;</code> Select at least two subnets which have access to the OpenSearch domain. The log processing Lambda will reside in the subnets. Make sure the subnets have access to the Amazon S3 service. Security Group ID <code>&lt;Requires input&gt;</code> Select a Security Group which will be associated with the log processing Lambda. Make sure the Security Group has access to the OpenSearch domain. S3 Backup Bucket <code>&lt;Requires input&gt;</code> The S3 backup bucket name to store the failed ingestion logs. KMS-CMK ARN <code>&lt;Optional input&gt;</code> The KMS-CMK ARN for encryption. Leave empty to create a new KMS CMK. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes. Keep the size of each shard between 10-50 GiB. Number of Replicas 1 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Warm Storage 0 The number of days required to move the index into warm storage. This takes effect only when the value is larger than 0 and warm storage is enabled in OpenSearch. Days to Cold Storage 0 The number of days required to move the index into cold storage. This takes effect only when the value is larger than 0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index. If value is 0, the index will not be deleted. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"implementation-guide/aws-services/waf/#sample-dashboard","title":"Sample Dashboard","text":"<p>You can access the built-in dashboard in Amazon OpenSearch to view log data. For more information, see View Dashboard.</p> <p>You can click the below image to view the high-resolution sample dashboard.</p> <p></p>"},{"location":"implementation-guide/deployment/","title":"Overview","text":"<p>Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this  guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.</p>"},{"location":"implementation-guide/deployment/#prerequisites","title":"Prerequisites","text":"<p>Review all the considerations and make sure you have the following in the target region you want to deploy the solution:</p> <ul> <li>At least one vacancy to create new VPCs, if you choose to launch with new VPC.</li> <li>At least two Elastic IP (EIP) addresses, if you choose to launch with new VPC.</li> <li>At least eight S3 buckets.</li> </ul>"},{"location":"implementation-guide/deployment/#deployment-in-aws-standard-regions","title":"Deployment in AWS Standard Regions","text":"<p>Log Hub provides two ways to authenticate and log into the Log Hub console. For some AWS regions where Cognito User Pool is not available (for example, Hong Kong), you need to launch the solution with OpenID Connect provider. </p> <ul> <li>Launch with Cognito User Pool</li> <li>Launch with OpenID Connect</li> </ul> <p>For more information about supported regions, see Regional deployments.</p>"},{"location":"implementation-guide/deployment/#deployment-in-aws-china-regions","title":"Deployment in AWS China Regions","text":"<p>AWS China Regions do not have Cognito User Pool. You need to launch the solution with OpenID Connect.</p> <ul> <li>Launch with OpenID Connect</li> </ul>"},{"location":"implementation-guide/deployment/with-cognito/","title":"Launch with Cognito User Pool","text":"<p>Time to deploy: Approximately 15 minutes</p>"},{"location":"implementation-guide/deployment/with-cognito/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Launch the stack</p> <p>Step 2. Launch the web console</p>"},{"location":"implementation-guide/deployment/with-cognito/#step-1-launch-the-stack","title":"Step 1. Launch the stack","text":"<p>This AWS CloudFormation template automatically deploys the Log Hub solution on AWS.</p> <ol> <li> <p>Sign in to the AWS Management Console and select the button to launch the <code>log-hub</code> AWS CloudFormation template.</p> Launch in AWS Console Launch with a new VPC Launch with an existing VPC </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Log Hub solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li>If you are launching the solution in a new VPC, this solution uses the following parameters:</li> </ul> Parameter Default Description Admin User Email <code>&lt;Requires input&gt;</code> Specify the email of the Administrator. This email address will receive a temporary password to access the Log Hub web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. <ul> <li>If you are launching the solution in an existing VPC, this solution uses the following parameters:</li> </ul> Parameter Default Description Admin User Email <code>&lt;Requires input&gt;</code> Specify the email of the Administrator. This email address will receive a temporary password to access the Log Hub web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. VPC ID <code>&lt;Requires input&gt;</code> Specify the existing VPC ID in which you are launching the Log Hub solution. Public Subnet IDs <code>&lt;Requires input&gt;</code> Specify the two public subnets in the selected VPC. The subnets must have routes point to an Internet Gateway. Private Subnet IDs <code>&lt;Requires input&gt;</code> Specify the two private subnets in the selected VPC. The subnets must have routes point to an NAT Gateway. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/deployment/with-cognito/#step-2-launch-the-web-console","title":"Step 2. Launch the web Console","text":"<p>After the stack is successfully created, this solution generates a CloudFront domain name that gives you access to the Log Hub web console.  Meanwhile, an auto-generated temporary password (excluding the last digit <code>.</code>) will be sent to your email address.</p> <ol> <li> <p>Sign in to the AWS CloudFormation console.</p> </li> <li> <p>On the Stacks page, select the solution\u2019s stack.</p> </li> <li> <p>Choose the Outputs tab and record the domain name.</p> </li> <li> <p>Open the WebConsoleUrl using a web browser, and navigate to a sign-in page.</p> </li> <li> <p>Enter the Email and the temporary password.</p> <p>a. Set a new account password.</p> <p>b. (Optional) Verify your email address for account recovery.</p> </li> <li> <p>After the verification is complete, the system opens the Log Hub web console.</p> </li> </ol> <p>Once you have logged into the Log Hub console, you can import an AOS domain and build log analytics pipelines.</p>"},{"location":"implementation-guide/deployment/with-oidc/","title":"Launch with OpenID Connect (OIDC)","text":"<p>Time to deploy: Approximately 30 minutes</p>"},{"location":"implementation-guide/deployment/with-oidc/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>The Log Hub console is served via CloudFront distribution which is considered as an Internet information service. If you are deploying the solution in AWS China Regions, the domain must have a valid ICP Recordal. </p> <ul> <li>A domain. You will use this domain to access the Log Hub console. </li> <li>An SSL certificate in AWS IAM. The SSL must be associated with the given domain. Follow this guide to upload SSL certificate to IAM.</li> </ul>"},{"location":"implementation-guide/deployment/with-oidc/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Create OIDC client</p> <p>Step 2. Launch the stack</p> <p>Step 3. Setup DNS Resolver</p> <p>Step 4. Launch the web console</p>"},{"location":"implementation-guide/deployment/with-oidc/#step-1-create-oidc-client","title":"Step 1. Create OIDC client","text":"<p>You can use different kinds of OpenID Connector (OIDC) providers. This section introduces Option 1 to Option 4.</p> <ul> <li>(Option 1) Using Amazon Cognito from another region as OIDC provider.</li> <li>(Option 2) Authing, which is an example of a third-party authentication provider. </li> <li>(Option 3) Keycloak, which is a solution maintained by AWS and can serve as an authentication identity provider. </li> <li>(Option 4) ADFS, which is a service offered by Microsoft.</li> <li>(Option 5) Other third-party authentication platforms such as Auth0.</li> </ul> <p>Follow the steps below to create an OIDC client, and obtain the <code>client_id</code> and <code>issuer</code>. </p>"},{"location":"implementation-guide/deployment/with-oidc/#option-1-using-cognito-user-pool-from-another-region","title":"(Option 1) Using Cognito User Pool from another region","text":"<p>You can leverage the Cognito User Pool in a supported AWS Standard Region as the OIDC provider.</p> <ol> <li>Go to the Amazon Cognito console in an AWS Standard Region.</li> <li>Set up the hosted UI with the Amazon Cognito console based on this guide.</li> <li>Note that there is a new UX design for the Amazon Cognito console.  If you are using the new UX of Amazon Cognito console, make sure you are following the instruction of New console. Moreover,<ul> <li>For New Amazon Cognito Console: make sure that Public client is chosen when selecting the App type.</li> <li>For Original Amazon Cognito Console: make sure that the option Generate client secret is unchecked when adding an app client. It is checked by default.</li> </ul> </li> <li>Enter the Callback URL and Sign out URL using your domain name for Log Hub console. If your hosted UI is set up, you should be able to see something like below:<ul> <li>For New Amazon Cognito Console, the hosted UI is Available:    </li> <li>For Original Amazon Cognito Console, the Launch Hosted UI link is available:    </li> </ul> </li> <li>Save the App client ID, User pool ID and the AWS Region to a file, which will be used later.<ul> <li>For New Amazon Cognito Console:     </li> <li>For Original Amazon Cognito Console:     </li> </ul> </li> </ol> <p>In Step 2. Launch the stack, the OidcClientID is the <code>App client ID</code>, and OidcProvider is <code>https://cognito-idp.${REGION}.amazonaws.com/${USER_POOL_ID}</code>.</p>"},{"location":"implementation-guide/deployment/with-oidc/#option-2-authingcn-oidc-client","title":"(Option 2) Authing.cn OIDC client","text":"<ol> <li>Go to the Authing console.</li> <li>Create a user pool if you don't have one.</li> <li>Select the user pool.</li> <li>On the left navigation bar, select Self-built App under Applications. </li> <li>Click the Create button.</li> <li>Enter the Application Name, and Subdomain.</li> <li> <p>Save the <code>App ID</code> (that is, <code>client_id</code>) and <code>Issuer</code> to a text file from Endpoint Information, which will be used later.     </p> </li> <li> <p>Update the <code>Login Callback URL</code> and <code>Logout Callback URL</code> to your IPC recorded domain name.     </p> </li> <li> <p>Set the Authorization Configuration.     </p> </li> </ol> <p>You have successfully created an authing self-built application. </p>"},{"location":"implementation-guide/deployment/with-oidc/#option-3-keycloak-oidc-client","title":"(Option 3) Keycloak OIDC client","text":"<ol> <li> <p>Deploy the Keycloak solution in AWS China Regions following this guide.</p> </li> <li> <p>Make sure you can log in to the Keycloak console.</p> </li> <li> <p>On the left navigation bar, select Add realm. Skip this step if you already have a realm. </p> </li> <li> <p>Go to the realm setting page. Choose Endpoints, and then OpenID Endpoint Configuration from the list.</p> <p></p> </li> <li> <p>In the JSON file that opens up in your browser, record the issuer value which will be used later.</p> <p></p> </li> <li> <p>Go back to Keycloak console and select Clients on the left navigation bar, and choose Create.</p> </li> <li>Enter a Client ID, which must contain 24 letters (case-insensitive) or numbers. Record the Client ID which will be used later.</li> <li> <p>Change client settings. Enter <code>https://&lt;Log Hub Console domain&gt;</code> in Valid Redirect URIs\uff0cand enter <code>*</code> and <code>+</code> in Web Origins, as shown below.</p> <p></p> </li> <li> <p>In the Advanced Settings, set the Access Token Lifespan to at least 5 minutes.</p> <p></p> </li> <li> <p>Select Users on the left navigation bar.</p> </li> <li>Click Add user and enter Username.</li> <li>After the user is created, select Credentials, and enter Password.</li> </ol> <p>The issuer value is <code>https://&lt;KEYCLOAK_DOMAIN_NAME&gt;/auth/realms/&lt;REALM_NAME&gt;</code>. </p>"},{"location":"implementation-guide/deployment/with-oidc/#option-4-adfs-openid-connect-client","title":"(Option 4) ADFS OpenID Connect Client","text":"<ol> <li>Make sure your ADFS is installed. For information about how to install ADFS, refer to this guide.</li> <li> <p>Make sure you can log in to the ADFS Sign On page. The URL should be <code>https://adfs.domain.com/adfs/ls/idpinitiatedSignOn.aspx</code>, and you need to replace adfs.domain.com with your real ADFS domain.</p> <p></p> </li> <li> <p>Log on your Domain Controller, and open Active Directory Users and Computers.</p> </li> <li> <p>Create a Security Group for Log Hub Users, and add your planned Log Hub users to this Security Group.</p> <p></p> </li> <li> <p>Log on to ADFS server, and open ADFS Management.</p> <p></p> </li> <li> <p>Right click Application Groups, click Application Group, and enter the name for the Application Group, such as LogHub. Select Web browser accessing a web application option under Client-Server Applications, and click Next.</p> <p></p> </li> <li> <p>Record the Client Identifier (<code>client_id</code>) under Redirect URI, enter your Log Hub domain (for example, <code>loghub.domain.com</code>), and click Add, and then click Next.</p> <p></p> </li> <li> <p>In the Choose Access Control Policy window, select Permit specific group, click parameters under Policy part, add the created Security Group in Step 4, then click Next. You can configure other access control policy based on your requirements.</p> <p></p> </li> <li> <p>Under Summary window, click Next, and click Close.</p> </li> <li> <p>Open the Windows PowerShell on ADFS Server, and run the following commands to configure ADFS to allow CORS for your planned LogHub URL.</p> <pre><code>Set-AdfsResponseHeaders -EnableCORS $true\nSet-AdfsResponseHeaders -CORSTrustedOrigins https://&lt;your-loghub-domain&gt;\n</code></pre> </li> <li> <p>Under Windows PowerShell on ADFS server, run the following command to get the Issuer (<code>issuer</code>) of ADFS, which is similar to <code>https://adfs.domain.com/adfs</code>.</p> <pre><code>Get-ADFSProperties | Select IdTokenIssuer\n</code></pre> <p></p> </li> </ol>"},{"location":"implementation-guide/deployment/with-oidc/#step-2-launch-the-stack","title":"Step 2. Launch the stack","text":"<p>Important</p> <p>You can only have one active Log Hub solution stack in one region of an AWS account.  If your deployment failed (for example, not meeting the requirements in prerequisites), make sure you have deleted the failed stack before retrying the deployment.</p> <ol> <li> <p>Sign in to the AWS Management Console and use the button below to launch the <code>log-hub</code> AWS CloudFormation template.</p> Launch in AWS Console Launch with a new VPC in standard regions Launch with an existing VPC in standard regions Launch with a new VPC in China Regions Launch with an existing VPC in China Regions </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Log Hub solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide.</li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li>If you are launching the solution in a new VPC, this solution uses the following parameters:</li> </ul> Parameter Default Description OidcClientId <code>&lt;Requires input&gt;</code> OpenID Connector client Id. OidcProvider <code>&lt;Requires input&gt;</code> OpenID Connector provider issuer. The issuer must begin with <code>https://</code> Domain <code>&lt;Requires input&gt;</code> Custom domain for Log Hub console. Do NOT add <code>http(s)</code> prefix. IamCertificateID <code>&lt;Requires input&gt;</code> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the <code>list-server-certificates</code> command to retrieve the ID. <ul> <li>If you are launching the solution in an existing VPC, this solution uses the following parameters:</li> </ul> Parameter Default Description OidcClientId <code>&lt;Requires input&gt;</code> OpenID Connector client Id. OidcProvider <code>&lt;Requires input&gt;</code> OpenID Connector provider issuer. The issuer must begin with <code>https://</code> Domain <code>&lt;Requires input&gt;</code> Custom domain for Log Hub console. Do NOT add <code>http(s)</code> prefix. IamCertificateID <code>&lt;Requires input&gt;</code> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the <code>list-server-certificates</code> command to retrieve the ID. VPC ID <code>&lt;Requires input&gt;</code> Specify the existing VPC ID which you are launching the Log Hub solution in. Public Subnet IDs <code>&lt;Requires input&gt;</code> Specify the two public subnets in the selected VPC. The subnets must have routes point to an Internet Gateway. Private Subnet IDs <code>&lt;Requires input&gt;</code> Specify the two private subnets in the selected VPC. The subnets must have routes point to an NAT Gateway. </li> <li> <p>Choose Next.</p> </li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</li> <li>Choose Create stack  to deploy the stack.</li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/deployment/with-oidc/#step-3-setup-dns-resolver","title":"Step 3. Setup DNS Resolver","text":"<p>This solution provisions a CloudFront distribution that gives you access to the Log Hub console.  </p> <ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select the solution's stack.</li> <li>Choose the Outputs tab.</li> <li>Obtain the WebConsoleUrl as the endpoint.</li> <li>Create a CNAME record in DNS resolver, which points to the endpoint address. </li> </ol>"},{"location":"implementation-guide/deployment/with-oidc/#step-4-launch-the-web-console","title":"Step 4. Launch the web console","text":"<p>Important</p> <p>You login credentials (username &amp; password) is managed by the OIDC provider. Before signing in to the Log Hub console, make sure you have created at least one user in the OIDC provider's user pool.</p> <ol> <li>Use the previous assigned CNAME to open the OIDC Customer Domain URL using a web browser.</li> <li>Choose Sign in to Log Hub, and navigate to OIDC provider.</li> <li>Enter username and password. You may be asked to change your default password for first-time login, which depends on your OIDC provider's policy.</li> <li>After the verification is complete, the system opens the Log Hub web console.</li> </ol> <p>Once you have logged into the Log Hub console, you can import an AOS domain and build log analytics pipelines.</p>"},{"location":"implementation-guide/domains/","title":"Overview","text":"<p>This chapter describes how to manage Amazon OpenSearch Service (AOS) domains on the Log Hub console. An AOS domain is synonymous with an AOS cluster.</p> <p>In this chapter, you will learn: </p> <ul> <li>Import &amp; remove an AOS Domain</li> <li>Create an access proxy</li> <li>Create recommended alarms</li> </ul> <p>You can read the Getting Started chapter first and walk through the basic steps for using the Log Hub solution.</p>"},{"location":"implementation-guide/domains/alarms/","title":"Recommended Alarms","text":"<p>Amazon OpenSearch provides a set of recommended CloudWatch alarms to monitor the health of AOS domains. Log Hub helps you to create the alarms automatically, and send notification to your email (or SMS) via SNS.</p>"},{"location":"implementation-guide/domains/alarms/#create-alarms","title":"Create alarms","text":""},{"location":"implementation-guide/domains/alarms/#using-the-log-hub-console","title":"Using the Log Hub console","text":"<ol> <li>Log in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Under General configuration, choose Enable at the Alarms label.</li> <li>Enter the Email.</li> <li>Choose the alarms you want to create and adjust the settings if necessary.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/domains/alarms/#using-the-cloudformation-stack","title":"Using the CloudFormation stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - Alarms solution in the AWS Cloud.</p> <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the <code>log-hub-alarms</code> AWS CloudFormation template.</p> <p></p> <p>You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description Endpoint <code>&lt;Requires input&gt;</code> The endpoint of the OpenSearch domain, for example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code>. DomainName <code>&lt;Requires input&gt;</code> The name of the OpenSearch domain. Email <code>&lt;Requires input&gt;</code> The notification email address. Alarms will be sent to this email address via SNS. ClusterStatusRed <code>Yes</code> Whether to enable alarm when at least one primary shard and its replicas are not allocated to a node. ClusterStatusYellow <code>Yes</code> Whether to enable alarm when at least one replica shard is not allocated to a node. FreeStorageSpace <code>10</code> Whether to enable alarm when a node in your cluster is down to the free storage space you entered in GiB. We recommend setting it to 25% of the storage space for each node. <code>0</code> means the alarm is disabled. ClusterIndexWritesBlocked <code>1</code> Index writes blocked error occurs for &gt;= x times in 5 minutes, 1 consecutive time. Input <code>0</code> to disable this alarm. UnreachableNodeNumber <code>3</code> Nodes minimum is &lt; x for 1 day, 1 consecutive time. <code>0</code> means the alarm is disabled. AutomatedSnapshotFailure <code>Yes</code> Whether to enable alarm when automated snapshot failed. AutomatedSnapshotFailure maximum is &gt;= 1 for 1 minute, 1 consecutive time. CPUUtilization <code>Yes</code> Whether to enable alarm when sustained high usage of CPU occurred. CPUUtilization or WarmCPUUtilization maximum is &gt;= 80% for 15 minutes, 3 consecutive times. JVMMemoryPressure <code>Yes</code> Whether to enable alarm when JVM RAM usage peak occurred. JVMMemoryPressure or WarmJVMMemoryPressure maximum is &gt;= 80% for 5 minutes, 3 consecutive times. MasterCPUUtilization <code>Yes</code> Whether to enable alarm when sustained high usage of CPU occurred in master nodes. MasterCPUUtilization maximum is &gt;= 50% for 15 minutes, 3 consecutive times. MasterJVMMemoryPressure <code>Yes</code> Whether to enable alarm when JVM RAM usage peak occurred in master nodes. MasterJVMMemoryPressure maximum is &gt;= 80% for 15 minutes, 1 consecutive time. KMSKeyError <code>Yes</code> Whether to enable alarm when KMS encryption key is disabled. KMSKeyError is &gt;= 1 for 1 minute, 1 consecutive time. KMSKeyInaccessible <code>Yes</code> Whether to enable alarm when KMS encryption key has been deleted or has revoked its grants to OpenSearch Service. KMSKeyInaccessible is &gt;= 1 for 1 minute, 1 consecutive time. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 5 minutes.</p> <p>Once you have created the alarms, a confirmation email will be sent to your email address. You need to click the Confirm link in the email.</p> <p>Go to the CloudWatch Alarms page by clicking the General configuration &gt; Alarms &gt; CloudWatch Alarms link on the Log Hub console , link location shown as follows:</p> <p></p> <p>Make sure that all the alarms are in OK status. Because you might have missed the notification if alarms have changed it's status before subscription. </p> <p>Note</p> <p>Note that alarm will not send SNS notification to your email address if triggered before subscription! Which means that if your newly created alarm is triggered right after it's creation, you will not be able to get notifications. We recommend you check the alarms status after enabling the OpenSearch alarms, if you see any alarm which is in In Alarm status, please fix that issue first. </p>"},{"location":"implementation-guide/domains/alarms/#delete-alarms","title":"Delete alarms","text":"<ol> <li>Log in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Choose the Alarms tab.</li> <li>Choose the Delete.</li> <li>On the confirmation prompt, choose Delete.</li> </ol>"},{"location":"implementation-guide/domains/import/","title":"Domain Operations","text":"<p>Once logged into the Log Hub console, you can import an AOS domain. Log Hub supports OpenSearch domain with fine-grained access control enabled within a VPC only. </p>"},{"location":"implementation-guide/domains/import/#prerequisite","title":"Prerequisite","text":"<ol> <li>Log Hub supports Amazon OpenSearch Service, engine version Elasticsearch 7.10 and above, and engine version OpenSearch 1.0 and above.</li> <li>Log Hub supports OpenSearch clusters within VPC. If you don't have an AOS domain yet, you can create an AOS domain within VPC. See Launching your Amazon OpenSearch Service domains within a VPC.</li> <li>Log Hub supports OpenSearch clusters with fine-grained access control only. In the security configuration, the Access policy should look like the image below:    </li> </ol>"},{"location":"implementation-guide/domains/import/#import-an-aos-domain","title":"Import an AOS Domain","text":"<ol> <li>Sign in to the Log Hub console.</li> <li>In the left navigation panel, under Domains, choose Import OpenSearch Domain.</li> <li>On the Select domain page, choose a domain from the dropdown list. The dropdown list will display only domains in the same region as the solution.</li> <li>Choose Next.</li> <li>On the Configure network page, under Network creation,<ul> <li>choose Manual and click Next;</li> <li>or choose Automatic, and go to step 9.</li> </ul> </li> <li>Under VPC, choose a VPC from the list. By default, the solution creates a standalone VPC, and you can choose the one named <code>LogHubVpc/DefaultVPC</code>. You can also choose the same VPC as your AOS domains.</li> <li>Under Log Processing Subnet Group, select at least 2 subnets from the dropdown list. By default, the solution creates two private subnets. You can choose subnets named <code>LogHubVpc/DefaultVPC/privateSubnet1</code> and <code>LogHubVpc/DefaultVPC/privateSubnet2</code>.</li> <li>Under Log Processing Security Group, select one from the dropdown list. By default, the solution creates one Security Group named <code>ProcessSecurityGroup</code>.</li> <li>On the Create tags page, add tags if needed.</li> <li>Choose Import.</li> </ol>"},{"location":"implementation-guide/domains/import/#set-up-vpc-peering","title":"Set up VPC Peering","text":"<p>By default, the solution creates a standalone VPC. You need to create VPC Peering to allow the log processing layer to have access to your AOS domains.</p> <p>Note</p> <p>Automatic mode will create VPC peering and configure route table automatically. You do not need to set up VPC peering again.</p> <p></p> <p>Follow this section to create VPC peering, update security group and update route tables.</p>"},{"location":"implementation-guide/domains/import/#create-vpc-peering-connection","title":"Create VPC Peering Connection","text":"<ol> <li>Sign in to the Log Hub console.</li> <li>In the left navigation panel, under Domains, select OpenSearch Domains.</li> <li>Find the domain you imported and select the domain name.</li> <li>Choose the Network tab.</li> <li>Copy the VPC ID in both sections OpenSearch domain network and Log processing network. You will create Peering Connection between these two VPCs.</li> <li>Navigate to VPC Console Peering Connections.</li> <li>Select the Create peering connection button.</li> <li>On the Create peering connection page, enter a name, for example, <code>log-hub</code>.</li> <li>For the Select a local VPC to peer with, VPC ID (Requester), select the VPC ID of the Log processing network.</li> <li>For the Select another VPC to peer with, VPC ID (Accepter), select the VPC ID of the OpenSearch domain network.</li> <li>Choose Create peering connection, and navigate to the peering connection detail page.</li> <li>Click the Actions button and choose Accept request.</li> </ol>"},{"location":"implementation-guide/domains/import/#update-route-tables","title":"Update Route Tables","text":"<ol> <li>Go to the Log Hub console.</li> <li>In the OpenSearch domain network section, click the subnet under AZs and Subnets to open the subnet console in a new tab.</li> <li>Select the subnet, and choose the Route table tab.</li> <li>Select the associated route table of the subnet to open the route table configuration page.</li> <li>Select the Routes tab, and choose Edit routes.</li> <li>Add a route <code>10.255.0.0/16</code> (the CIDR of Log Hub\uff0c if you created Log Hub with existing VPC, please change this value) pointing to the Peering Connection you just created.</li> <li>Go back to the Log Hub console.</li> <li>Click the VPC ID under the OpenSearch domain network section.</li> <li>Select the VPC ID on the VPC Console and find its IPv4 CIDR.</li> <li>On the Log Hub console, in the Log processing network section, click the subnets under AZs and Subnets to open the subnets in new tabs.</li> <li>Repeat step 3, 4, 5, 6 to add an opposite route. Namely, configure the IPv4 CIDR of the OpenSearch VPC to point to the Peering Connection. You need to repeat the steps for each subnet of Log processing network.</li> </ol>"},{"location":"implementation-guide/domains/import/#update-security-group-of-opensearch-domain","title":"Update Security Group of OpenSearch Domain","text":"<ol> <li>On the Log Hub console, under the OpenSearch domain network section, select the Security Group ID in Security Groups to open the Security Group in a new tab.</li> <li>On the console, select Edit inbound rules.</li> <li>Add the rule <code>ALLOW TCP/443 from 10.255.0.0/16</code> (the CIDR of Log Hub\uff0c if you created Log Hub with existing VPC, please change this value).    </li> <li>Choose Save rules.</li> </ol>"},{"location":"implementation-guide/domains/import/#remove-an-aos-domain","title":"Remove an AOS domain","text":"<p>If needed, you can remove the AOS domains. </p> <p>Important</p> <p>Removing the domain from Log Hub will NOT delete the AOS domain in your AWS account. It will NOT impact any existing log analytics pipelines.</p> <ol> <li>Sign in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose OpenSearch Domains.</li> <li>Select the domain from the table.</li> <li>Choose Remove.</li> <li>In the confirmation dialog box, choose Remove.</li> </ol>"},{"location":"implementation-guide/domains/proxy/","title":"Access Proxy","text":"<p>By default, an AOS domain within VPC cannot be accessed from the Internet. Log Hub creates a highly available Nginx cluster which allows you to access the OpenSearch Dashboards from the Internet. Alternatively, you can choose to access the AOS domains using SSH Tunnel.</p> <p>This section introduces the proxy stack architecture and how to complete the following:</p> <ol> <li>Create a proxy</li> <li>Create an associated DNS record</li> <li>Access AOS via proxy</li> <li>Delete a proxy</li> </ol>"},{"location":"implementation-guide/domains/proxy/#architecture","title":"Architecture","text":"<p>Log Hub creates an Auto Scaling Group (ASG) together with an Application Load Balancer (ALB).</p> <p></p> <p>The workflow is as follows:</p> <ol> <li> <p>Users access the custom domain for the proxy, and the domain needs to be resolved via DNS service (for example, using Route 53 on AWS).</p> </li> <li> <p>The DNS service routes the traffic to internet-facing ALB.</p> </li> <li> <p>The ALB distributes traffic to backend Nginx server running on Amazon EC2 within ASG. </p> </li> <li> <p>The Nginx server redirects the requests to OpenSearch Dashboards.</p> </li> <li> <p>(optional) VPC peering is required if the VPC for the proxy is not the same as the OpenSearch service.</p> </li> </ol>"},{"location":"implementation-guide/domains/proxy/#create-a-proxy","title":"Create a proxy","text":"<p>You can create the Nginx-based proxy using the Log Hub console or by deploying a standalone CloudFormation stack.</p> <p>Prerequisites</p> <ul> <li>Make sure an AOS domain within VPC is available.</li> <li>The domain associated SSL certificate is created or uploaded in Amazon Certificate Manager (ACM).</li> <li>Make sure you have the EC2 private key (.pem) file.</li> </ul>"},{"location":"implementation-guide/domains/proxy/#using-the-log-hub-console","title":"Using the Log Hub console","text":"<ol> <li>Log in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li> <p>Under General configuration, choose Enable at the Access Proxy label.</p> <p>Note</p> <p>Once the access proxy is enabled, a link to the access proxy will be available. </p> </li> <li> <p>On the Create access proxy page, under Public access proxy, select at least 2 subnets for Public Subnets. You can choose 2 public subnets named <code>LogHubVPC/DefaultVPC/publicSubnet</code>, which are created by Log Hub by default.</p> </li> <li>Choose a Security Group of the ALB in Public Security Group. You can choose a security group named <code>ProxySecurityGroup</code>, which is created by Log Hub default.</li> <li>Enter the Domain Name.</li> <li>Choose Load Balancer SSL Certificate associated with the domain name.</li> <li>Choose the Nginx Instance Key Name.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/domains/proxy/#using-the-cloudformation-stack","title":"Using the CloudFormation stack","text":"<p>This automated AWS CloudFormation template deploys the Log Hub - Nginx access proxy solution in the AWS Cloud.</p> <ol> <li> <p>Log in to the AWS Management Console and select the button to launch the <code>nginx-for-opensearch</code> AWS CloudFormation template.</p> <p></p> <p>You can also download the template as a starting point for your own implementation.</p> </li> <li> <p>To launch the stack in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your stack.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following parameters.</p> Parameter Default Description VPCId <code>&lt;Requires input&gt;</code> The VPC to deploy the Nginx proxy resources, for example, <code>vpc-bef13dc7</code>. PublicSubnetIds <code>&lt;Requires input&gt;</code> The public subnets where ELB are deployed. You need to select at least two public subnets, for example, <code>subnet-12345abc, subnet-54321cba</code>. ELBSecurityGroupId <code>&lt;Requires input&gt;</code> The Security group being associated with the ELB, for example, <code>sg-123456</code>. ELBDomain <code>&lt;Requires input&gt;</code> The custom domain name of the ELB, for example, <code>dashboard.example.com</code>. ELBDomainCertificateArn <code>&lt;Requires input&gt;</code> The SSL certificate ARN associated with the ELBDomain. The certificate must be created from Amazon Certificate Manager (ACM). PrivateSubnetIds <code>&lt;Requires input&gt;</code> The private subnets where Nginx instances are deployed. You need to select at least two private subnets, for example, <code>subnet-12345abc, subnet-54321cba</code>. NginxSecurityGroupId <code>&lt;Requires input&gt;</code> The Security group associated with the Nginx instances. The security group must allow access from ELB security group. KeyName <code>&lt;Requires input&gt;</code> The PEM key name of the Nginx instances. EngineType OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint, for example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code>. CognitoEndpoint <code>&lt;Optional&gt;</code> The Cognito User Pool endpoint URL of the OpenSearch domain, for example, <code>mydomain.auth.us-east-1.amazoncognito.com</code>. Leave empty if your OpenSearch domain is not authenticated through Cognito User Pool. </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"implementation-guide/domains/proxy/#create-an-associated-dns-record","title":"Create an associated DNS record","text":"<p>After provisioning the proxy infrastructure, you need to create an associated DNS record in your DNS resolver. The following introduces how to find the ALB domain, and then create a CNAME record pointing to this domain.</p> <ol> <li>Log in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Choose the Access Proxy tab.    You can see Load Balancer Domain which is the ALB domain.</li> <li>Go to the DNS resolver, create a CNAME record pointing to this domain.      If your domain is managed by Amazon Route 53, refer to Creating records by using the Amazon Route 53 console.</li> </ol>"},{"location":"implementation-guide/domains/proxy/#access-aos-via-proxy","title":"Access AOS via proxy","text":"<p>After the DNS record takes effect, you can access the AOS built-in dashboard from anywhere via proxy. You can enter the domain of the proxy in your browser, or click the Link button under Access Proxy in the General Configuration section.</p> <p></p>"},{"location":"implementation-guide/domains/proxy/#delete-a-proxy","title":"Delete a Proxy","text":"<ol> <li>Log in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Choose the Access Proxy tab.</li> <li>Choose the Delete.</li> <li>On the confirmation prompt, choose Delete.</li> </ol>"},{"location":"implementation-guide/getting-started/","title":"Getting Started","text":"<p>After deploying the solution, you can read this chapter first to learn quickly how to leverage Log Hub for log ingestion (Amazon CloudTrail logs as an example), and log visualization. </p> <p>You can also choose to start with Domain management , then build AWS Service Log Analytics Pipelines and Application Log Analytics Pipelines.</p>"},{"location":"implementation-guide/getting-started/#steps","title":"Steps","text":"<ul> <li>Step 1: Import AOS domain. Import an existing AOS domain into the solution.</li> <li>Step 2: Create Access Proxy. Create a public access proxy which allows you to access AOS templated dashboard from anywhere.</li> <li>Step 3: Ingest CloudTrail Logs. Ingest CloudTrail logs into the specified AOS domain.</li> <li>Step 4: Access AOS built-in dashboard. View the dashboard of CloudTrail logs.  </li> </ul>"},{"location":"implementation-guide/getting-started/1.import-domain/","title":"Step 1: Import an Amazon OpenSearch domain","text":"<p>To use the Log Hub solution for the first time, you must import AOS domains first.</p> <p>Log Hub supports Amazon OpenSearch domain with fine-grained access control enabled within a VPC only. </p> <p>Important</p> <p>Currently, Log Hub supports Amazon Elasticsearch 7.10 and later, or Amazon OpenSearch 1.0 and later.</p>"},{"location":"implementation-guide/getting-started/1.import-domain/#prerequisite","title":"Prerequisite","text":"<p>At least one AOS domain within VPC. If you don't have an AOS domain yet, you can create an AOS domain within VPC. See Launching your Amazon OpenSearch Service domains within a VPC. </p>"},{"location":"implementation-guide/getting-started/1.import-domain/#steps","title":"Steps","text":"<p>Use the following procedure to import an AOS domain on the Log Hub console.</p> <ol> <li>Sign in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose Import OpenSearch Domain. </li> <li>On the Step 1. Select domain page, choose a domain from the dropdown list. </li> <li>Choose Next.</li> <li>On the Step 2. Configure network page, under Network creation, choose Automatic. If your Log Hub and OpenSearch domains resides in two different VPCs, the Automatic mode will create a VPC Peering Connection between them, and update route tables. See details in Set up VPC Peering. </li> <li>On the Step 3. Create tags page, choose Import.</li> </ol>"},{"location":"implementation-guide/getting-started/2.create-proxy/","title":"Step 2: Create Access Proxy","text":"<p>You can create a Nginx proxy and create an DNS record pointing to the proxy, so that you can access the AOS dashboard securely from public network. For more information, refer to Access Proxy in the Domain Management chapter.</p>"},{"location":"implementation-guide/getting-started/2.create-proxy/#create-a-nginx-proxy","title":"Create a Nginx proxy","text":"<ol> <li>Sign in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Under General configuration, choose Enable at the Access Proxy label.</li> <li>On the Create access proxy page, under Public access proxy, select at least 2 subnets which contain <code>LogHubVpc/DefaultVPC/publicSubnetX</code> for the Public Subnets.</li> <li>For Public Security Group, choose the Security Group which contains <code>ProxySecurityGroup</code>.</li> <li>Enter the Domain Name.</li> <li>Choose the associated Load Balancer SSL Certificate which applies to the domain name.</li> <li>Choose the Nginx Instance Key Name.     </li> <li>Choose Create.</li> </ol> <p>After provisioning the proxy infrastructure, you need to create an associated DNS record in your DNS resolver. The following introduces how to find the Application Load Balancing (ALB) domain, and then create a CNAME record pointing to this domain.</p>"},{"location":"implementation-guide/getting-started/2.create-proxy/#create-an-dns-record","title":"Create an DNS record","text":"<ol> <li>Sign in to the Log Hub console.</li> <li>In the navigation pane, under Domains, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Choose the Access Proxy tab. Find Load Balancer Domain, which is the ALB domain.</li> <li>Go to the DNS resolver, and create a CNAME record pointing to this domain. If your domain is managed by Amazon Route 53, refer to Creating records by using the Amazon Route 53 console.</li> </ol>"},{"location":"implementation-guide/getting-started/3.build-cloudtrail-pipeline/","title":"Step 3: Ingest Amazon CloudTrail Logs","text":"<p>You can build a log analytics pipeline to ingest Amazon CloudTrail logs.</p> <p>Important</p> <p>Make sure your CloudTrail and Log Hub are in the same AWS region.</p> <ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, select AWS Service Log Analytics Pipelines.</li> <li>Choose Create a log ingestion.</li> <li>In the AWS Services section, choose Amazon CloudTrail.</li> <li>Choose Next.</li> <li>Under Specify settings, for Trail, select one from the dropdown list.</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select the imported domain for Amazon OpenSearch domain.</li> <li>Choose Yes for Sample dashboard. </li> <li>Keep default values and choose Next.</li> <li>Choose Create.</li> </ol>"},{"location":"implementation-guide/getting-started/4.view-dashboard/","title":"Step 4: Access AOS Build-in Dashboard","text":"<p>After the DNS record takes effect, you can access the AOS built-in dashboard from anywhere via proxy. </p> <ol> <li>Enter the domain of the proxy in your browser. Alternatively, click the Link button under Access Proxy in the General Configuration section of the domain.</li> <li>Enter your credentials to log in to Amazon OpenSearch Dashboard.</li> <li>Click the username icon of AOS dashboard from the top right corner. </li> <li>Choose Switch Tenants.</li> <li>On the Select your tenant page, choose Global, and click Confirm.</li> <li>On the left navigation panel, choose Dashboards. </li> <li>Choose the AOS dashboard created automatically and start to explore your data.</li> </ol>"},{"location":"implementation-guide/link-account/","title":"Cross-Account Ingestion","text":"<p>Log Hub supports ingesting AWS Service logs and Application logs in different AWS accounts within the same region. After deploying Log Hub in one account (main account), you can launch the CloudFormation stack in a different account (member account), and associate the two accounts (main account and member account) to implement cross-account ingestion.</p>"},{"location":"implementation-guide/link-account/#concepts","title":"Concepts","text":"<ul> <li>Main account: One account in which you deployed the Log Hub console. The OpenSearch cluster(s) must also be in the same account.</li> <li>Member account: Another account from which you want to ingest AWS Service logs or application logs. </li> </ul> <p>The CloudFormation stack in the member account has the least privileges. Log Hub need to provision some AWS resources in the member account to collect logs, and will assume an IAM role provisioned in the member account to list or create resources. </p> <p>For more information, refer to the Architecture section.</p>"},{"location":"implementation-guide/link-account/#add-a-member-account","title":"Add a member account","text":""},{"location":"implementation-guide/link-account/#step-1-launch-a-cloudformation-stack-in-the-member-account","title":"Step 1. Launch a CloudFormation stack in the member account","text":"<ol> <li> <p>Sign in to the Log Hub console.</p> </li> <li> <p>In the navigation pane, under Resources, choose Cross-Account Ingestion. </p> </li> <li> <p>Click the Link an Account button. It displays the steps to deploy the CloudFormation stack in the member account. </p> <p>Important</p> <p>You need to copy the template URL, which will be used later.</p> </li> <li> <p>Go to the CloudFormation console of the member account.</p> </li> <li> <p>Click the Create stack button and choose With new resources (standard).</p> </li> <li> <p>In the Create stack page, enter the template URL you have copied in Amazon S3 URL.  </p> </li> <li> <p>Follow the steps to create the CloudFormation stack and wait until the CloudFormation stack is provisioned. </p> </li> <li> <p>Go to the Outputs tab to check the parameters which will be used in Step 2.</p> </li> </ol>"},{"location":"implementation-guide/link-account/#step-2-link-a-member-account","title":"Step 2. Link a member account","text":"<ol> <li>Go back to the Log Hub console.</li> <li>(Optional) In the navigation panel, under Resources, choose Cross-Account Ingestion.</li> <li> <p>In Step 2. Link an account, enter the parameters using the Outputs parameters from Step 1. </p> Parameter CloudFormation Outputs Description Account Name N/A Name of the member account. Account ID N/A 12-digit AWS account ID. Cross Account Role ARN CrossAccountRoleARN Log Hub will assume this role to operate resources in the member account. FluentBit Agent Installation Document AgentInstallDocument Log Hub will use this SSM Document to install Fluent Bit agent on EC2 instances in the member account. FluentBit Agent Configuration Document AgentConfigDocument Log Hub will use this SSM Document to deliver Fluent Bit configuration to EC2 instances. Cross Account S3 Bucket CrossAccountS3Bucket You can use the Log Hub console to enable some AWS Service logs and output them to Amazon S3. The logs will be stored in this account. Cross Account Stack ID CrossAccountStackId CloudFormation stack ID in the member account. Cross Account KMS Key CrossAccountKMSKeyARN Log Hub will use the Key Management Services (KMS) key to encrypt Simple Queue Service (SQS). </li> <li> <p>Click the Link button.</p> </li> </ol>"},{"location":"implementation-guide/resources/aws-services/","title":"AWS Services","text":"<ul> <li>AWS CloudFormation</li> <li>Amazon OpenSearch Service</li> <li>Amazon S3</li> <li>AWS Lambda</li> <li>Amazon CloudFront</li> <li>AWS AppSync</li> <li>Amazon Cognito User Pool</li> <li>AWS Step Functions</li> <li>Amazon DynamoDB</li> <li>AWS Systems Manager</li> <li>Amazon EventBridge</li> <li>Amazon Kinesis Data Streams</li> <li>Amazon Kinesis Data Firehose</li> </ul>"},{"location":"implementation-guide/resources/open-ssl/","title":"OpenSSL 1.1 Installation","text":"<p>Log Hub uses Fluent Bit as the log agent, which requires OpenSSL 1.1 or later. You can install the dependency according to your operating system (OS). It is recommended to make your own AMI with OpenSSL 1.1 installed.</p> <p>Important</p> <p>If your OS is not listed below, it does not mean you cannot use Log Hub. You need to find a way to install OpenSSL 1.1.</p>"},{"location":"implementation-guide/resources/open-ssl/#amazon-linux-2","title":"Amazon Linux 2","text":"<pre><code>sudo yum install openssl11\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#ubuntu","title":"Ubuntu","text":""},{"location":"implementation-guide/resources/open-ssl/#2004","title":"20.04","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#1804","title":"18.04","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#debian","title":"Debian","text":""},{"location":"implementation-guide/resources/open-ssl/#gnu10","title":"GNU/10","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#gnu11","title":"GNU/11","text":"<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libsasl2.so.2 /usr/lib/libsasl2.so.3\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#red-hat-enterprise-linux","title":"Red Hat Enterprise Linux","text":""},{"location":"implementation-guide/resources/open-ssl/#8x","title":"8.X","text":"<p>OpenSSL 1.1 is installed by default.</p>"},{"location":"implementation-guide/resources/open-ssl/#7x","title":"7.X","text":"<pre><code>sudo su -\n\nyum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\n\nsystemctl enable amazon-ssm-agent\nsystemctl start amazon-ssm-agent\n\nyum install -y wget perl unzip gcc zlib-devel\nmkdir /tmp/openssl\ncd /tmp/openssl\nwget https://www.openssl.org/source/openssl-1.1.1s.tar.gz\ntar xzvf openssl-1.1.1s.tar.gz\ncd openssl-1.1.1s\n./config --prefix=/usr/local/openssl11 --openssldir=/usr/local/openssl11 shared zlib\nmake\nmake install\n\necho /usr/local/openssl11/lib/ &gt;&gt; /etc/ld.so.conf\nldconfig\n</code></pre>"},{"location":"implementation-guide/resources/open-ssl/#suse-linux-enterprise-server","title":"SUSE Linux Enterprise Server","text":""},{"location":"implementation-guide/resources/open-ssl/#15","title":"15","text":"<p>OpenSSL 1.1 is installed by default.</p>"},{"location":"implementation-guide/resources/upload-ssl-certificate/","title":"Upload SSL Certificate to IAM","text":"<p>Upload the SSL certificate by running the AWS CLI command <code>upload-server-certificate</code> similar to the following:</p> <pre><code>aws iam upload-server-certificate --path /cloudfront/ \\\n--server-certificate-name YourCertificate \\\n--certificate-body file://Certificate.pem \\\n--certificate-chain file://CertificateChain.pem \\\n--private-key file://PrivateKey.pem\n</code></pre> <p>Replace the file names and Your Certificate with the names for your uploaded files and certificate. You must specify the <code>file://</code> prefix in the certificate-body, certificate-chain and private-key parameters in the API request.  Otherwise, the request fails with a <code>MalformedCertificate: Unknown</code> error message.</p> <p>Note</p> <p>You must specify a path using the --path option. The path must begin with /cloudfront and must include a   trailing slash (for example, /cloudfront/test/).</p> <p>After the certificate is uploaded, the AWS command <code>upload-server-certificate</code> returns metadata for the uploaded certificate, including the certificate's Amazon Resource Name (ARN), friendly name, identifier (ID), and expiration date.</p> <p>To view the uploaded certificate, run the AWS CLI command <code>list-server-certificates</code>:</p> <pre><code>aws iam list-server-certificates\n</code></pre> <p>For more information, see uploading a server certificate to IAM.</p>"},{"location":"updates/roadmap/","title":"Roadmap","text":"<p>Note</p> <p>Feature request and bug report is welcome, please submit your requset via GitHub Issues.</p>"},{"location":"updates/roadmap/#v010-12312021","title":"v0.1.0, 12/31/2021","text":"Category Feature Description Domain Management Import/Remove AOS domain Import or remove an AOS domain within VPC into the Log Hub solution through the web console. Domain Management Public access proxy Automatically create an Nginx-based proxy that allows the customers to access the AOS dashboards through the Internet. Domain Management Recommended alarms Create the recommended AOS alarms and send notifications to customers through SNS. AWS Service Log CloudTrail log template (1) Support automatic CloudTrail log ingestion (2) One-click to create a dashboard for CloudTrail using templates. AWS Service Log Amazon S3 access log template (1) Support automatic S3 log ingestion from the a selected S3 location. (2) One-click to create a dashboard for S3 access log. AWS Service Log Lifecycle Management Automate log lifecycle using Index State Management (ISM) Support UltraWarm and Cold Storage. AWS Service Log Amazon RDS MySQL/Aurora Slow query log (1) Support automatic RDS/Aurora MySQL log ingestion (2) One-click to create the dashboard using template. AWS Service Log Amazon RDS MySQL/Aurora Error log (1) Support automatic RDS/Aurora MySQL log ingestion(2) One-click to create the dashboard using template. AWS Service Log Amazon CloudFront standard access log template (1) Support automatic log ingestion.(2) One-click to create the dashboard using template. Application Log Instance Group Create a group of instance to apply the same log configuration. Application Log Log Config Create a log agent configuration (e.g. log type, log file path) that applies to a certain version of log agent. Application Log Log pipeline Create a log pipeline (on top of Amazon Kinesis Data Streams) which allows the log agent to collect and send logs. Application Log JSON format log ingestion Build an end-to-end pipeline to ingest JSON format log data. Deployment Web console CDK/CloudFormation deployment Deploy the solution via AWS CDK or Amazon CloudFormation. It will provision a stack with a built-in Log Hub web console. Deployment Standalone log pipeline CDK/CloudFormation deployment Deploy a single log pipeline via AWS CDK or Amazon CloudFormation. All supported log type can be deployed as a standalone stack. Workshop Data insights workshop Demonstrate how to use Log Hub to build a centralized logging platform to explore the data insights."},{"location":"updates/roadmap/#v020-3152022","title":"v0.2.0, 3/15/2022","text":"Category Feature Description AWS Service Log ELB log template Application Load Balancer access log ingestion and visualization. AWS Service Log WAF Log template WAF ingestion and visualization. Application Log RegEx single-line text log template Support Parse and ingest logs using FluentBit Regular Expression in single-line text format . Application Log RegEx multi-line text custom log template Support Parse and ingest logs using FluentBit Regular Expression in multi-line text format. Application Log RegEx Java - Spring Boot log template Support Parse and ingest logs using FluentBit Regular Expression in Spring Boot log format. Application Log Nginx Log template Support ingest Nginx format log and create visualization dashboard. Application Log Apache HTTP Server template Support ingest Apache HTTP Server format log and create dashboard. Codeless Log Processors IP2Location Converter A plugin to convert the IP address to location information. Web Console Zh-CN Add Simplified Chinese user interface. Web Console OpenID Connect (OIDC) authentication Expand the authentication for OIDC to support China regions deployment."},{"location":"updates/roadmap/#v100-mvp-5312022","title":"v1.0.0 (MVP), 5/31/2022","text":"Category Feature Description Application Log Ingest logs from S3 bucket Allows ingest JSON/Single-line Text format logs from a S3 bucket. Codeless Log Processors User Agent to Device Info A plugin to convert User-Agent filed to device information. Application Log EKS Pod log ingestion Wizard to ingest logs from EKS clusters."},{"location":"updates/roadmap/#v110-q32022","title":"v1.1.0, Q3/2022","text":"Category Feature Description AWS Service Log AWS Config log template AWS Config log ingestion and visualization. AWS Service Log VPC Flow Logs template VPC Flows Logs log ingestion and visualization. AWS Service Log Cross-region/account region support Add support for cross-region/account service log ingestion. Application Log Cross-region/account support Add support for cross-region/account application log ingestion. Codeless Log Processors Data Prepper integration Allows customers to import existing Data Prepper domain and use the built-in processors to process logs. This depends on whether Data Prepper supports log processing."},{"location":"updates/roadmap/#v120-q42022","title":"v1.2.0, Q4/2022","text":"Category Feature Description Codeless Log Processors Log preview Allows customers to preview log on the Log Hub console. Codeless Log Processors Embedded plugin code editor Allows customers to write plugin code in Python directly in Log Hub console. Console Resource viewer Add resource list in the Log Pipeline detail."}]}