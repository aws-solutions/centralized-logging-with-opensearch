[SERVICE]
    # Flush
    # =====
    # set an interval of seconds before to flush records to a destination
    flush        5
    # Daemon
    # ======
    # instruct Fluent Bit to run in foreground or background mode.
    daemon       Off
    # Log_Level
    # =========
    # Set the verbosity level of the service, values can be:
    #
    # - error
    # - warning
    # - info
    # - debug
    # - trace
    #
    # by default 'info' is set, that means it includes 'error' and 'warning'.
    log_level    info
    log_File /tmp/log-agent.log
    # HTTP Server
    # ===========
    # Enable/Disable the built-in HTTP Server for metrics, you use prometheus to monitor it.
    http_server  On
    http_listen  0.0.0.0
    http_port    2022
    storage.path /opt/fluent-bit/flb-storage/
    # ------------
    # absolute file system path to store filesystem data buffers (chunks).
    #
    # storage.sync
    # ------------
    # configure the synchronization mode used to store the data into the
    # filesystem. It can take the values normal or full.
    #
    # storage.checksum
    # ----------------
    # enable the data integrity check when writing and reading data from the
    # filesystem. The storage layer uses the CRC32 algorithm.
    #
    # storage.checksum off
    # storage.backlog.mem_limit
    # -------------------------
    # if storage.path is set, Fluent Bit will look for data chunks that were
    # not delivered and are still in the storage layer, these are called
    # backlog data. This option configure a hint of maximum value of memory
    # to use when processing these records.
    #
    Parsers_File /opt/fluent-bit/etc/applog_parsers.conf

[FILTER]
    Name aws
    Match *
    az true
    ec2_instance_id true
    ec2_instance_type false
    private_ip true
    ami_id false
    account_id false
    hostname true
    vpc_id false

[INPUT]
    Name tail
    # log file location
    Path /var/log/nginx/access.log
    #Lambda will use Path_key to get file name
    Path_Key file_name
    Tag  339039e1-9812-43f8-9962-165e3adbc805_d27b96a9-7b78-4fe1-94e6-3e42f57f4339
    Read_from_head false
    #use this as checkpoint
    DB   /tmp/checkpoint-339039e1-9812-43f8-9962-165e3adbc805_d27b96a9-7b78-4fe1-94e6-3e42f57f4339.db
    DB.locking    True
    DB.Sync Normal
    # https://docs.fluentbit.io/manual/administration/backpressure#mem_buf_limit
    Mem_Buf_Limit 30M
    storage.type      filesystem
    # Since "Skip_Long_Lines" is set to "On", be sure to adjust the "Buffer_Chunk_Size","Buffer_Max_Size" according to the actual log. If the parameters are adjusted too much, the number of duplicate records will increase. If the value is too small, data will be lost.
    # https://docs.fluentbit.io/manual/pipeline/inputs/tail
    Buffer_Chunk_Size 512k
    Buffer_Max_Size   5M
    Skip_Long_Lines   On
    Skip_Empty_Lines  On
    Refresh_Interval  10
    Rotate_Wait       30
    #parser
    Parser nginx_d27b96a9-7b78-4fe1-94e6-3e42f57f4339_8a76e4b1-5164-491d-9991-05a579b42299_339039e1-9812-43f8-9962-165e3adbc805
[OUTPUT]
        Name                kinesis_streams
        Match               339039e1-9812-43f8-9962-165e3adbc805_d27b96a9-7b78-4fe1-94e6-3e42f57f4339
        AWS_Region          us-east-1
        Stream              LogHub-AppPipe-d27b9-Stream790BDEE4-oXbw9MVMUZho
        Retry_Limit         False

[FILTER]
    Name    lua
    Match   *
    time_as_table   on
    script  uniform-time-format.lua
    call    cb_print