{
  "name": "Application log",
  "title": "Application log pipelines",
  "list": {
    "os": "OpenSearch",
    "lightEngine": "Light Engine",
    "engineType": "Engine type",
    "indexTable": "Index / Table name",
    "indexName": "Index name",
    "streamName": "Stream name",
    "bufferLayer": "Buffer",
    "created": "Created",
    "logPath": "Log path",
    "status": "Status",
    "logConfig": "Log Config",
    "export": "Export configurations",
    "exporting": "Exporting Application log pipelines",
    "exported": "Application log pipelines exported"
  },
  "edit": {
    "name": "Edit",
    "logConfig": "Log Config",
    "logConfigDesc": "Choose the Log Config version from the below list.",
    "notSupported": "Not supported",
    "editNotSupported": "This pipeline does not support editing"
  },
  "import": {
    "import": "Import",
    "specifyTemplate": "Specify template",
    "importTemplate": "Import template",
    "templateFile": "Template file",
    "templateFileDesc": "Selecting a YAML template file to import application log pipelines.",
    "errors": "Errors",
    "warnings": "Warnings",
    "Suggestions": "Suggestions",
    "templateErrorMessage": "The provided template have errors. Please address the identified errors and resubmit the template."
  },
  "delete": "Delete Application Log",
  "deletePipeline": {
    "alarm": "Please open the pipeline and delete all log sources first."
  },
  "deleteTips": "Are you sure you want to delete the application log ",
  "detail": {
    "syslogSettings": "Syslog settings",
    "indexSuffix": "Index suffix",
    "rolloverSize": "Rollover size",
    "compressionType": "Compression type",
    "openShards": "Open shards",
    "shardNumber": "Shard number",
    "domain": "Domain",
    "sampleDashboard": "Sample dashboard",
    "fanout": "Consumers with enhanced fan-out",
    "shards": "Shards (Kinesis Data Streams)",
    "autoScaling": "(Auto Scaling)",
    "enabledAutoScaling": "Enable Auto Scaling",
    "maxShardNum": "Maximum shard number",
    "bucket": "Bucket",
    "kds": "Kinesis Data Streams",
    "noneBuffer": "None buffer",
    "analyticsEngine": "Analytics Engine",
    "grafanaDashboard": "Grafana dashboard",
    "grafanaDashboardDetail": "Grafana dashboard - Detail",
    "indexSettings": "Index settings",
    "tableSettings": "Table settings",
    "metricTableSettings": "Table settings - Metric",
    "cfnStack": "CloudFormation Stack",
    "tip1": "If you create an log source from an Instance Group, make sure you have attached these ",
    "tip2": "permissions",
    "tip3": " to the instances. This is NOT required if you create log source from Syslog or EKS cluster.",
    "noLogging": "This log pipeline has no buffer, logs are sent directly to OpenSearch domain from source, no log processor lambda was created, hence no logs to display",
    "noBuffer": "No buffer",
    "noBufferDesc": "There is no buffer layer configured for this log analytics pipeline.",
    "tab": {
      "logSources": "Log sources",
      "permission": "Permission",
      "logConfig": "Log Config",
      "bufferLayer": "Buffer",
      "lifecycle": "Lifecycle settings",
      "analyticsEngine": "Analytics Engine",
      "sourceDetail": "Source detail",
      "tags": "Tags",
      "monitoring": "Metrics",
      "alarm": "Alarms",
      "logging": "Logs"
    },
    "ingestion": {
      "generalConfig": "General configuration",
      "prefixFilter": "Prefix filter",
      "compression": "Compression",
      "source": "Resource",
      "sourceType": "Type",
      "status": "Status",
      "ingestionMode": "Log source mode",
      "logConfig": "Log Config",
      "instanceGroup": "Instance Group",
      "created": "Created",
      "delete": "Delete log source",
      "deleteTips": "Are you sure you want to delete the log source(s)? ",
      "upgradeGuide": "Upgrade Guide",
      "eksTips1": "If you need to create additional EKS log source(s) in this pipeline, go to ",
      "eksTips2": "ESK Cluster",
      "eksTips3": " create log source and select current pipeline.",
      "oneMoreStep": "One more step to complete the log source setup...",
      "groupTips": " We detect you had created an log source with an Instance Group based on EC2 Auto Scaling group, please wait for the log source's status become \"Created\", then go to the detail page of Instance Group(s), and follow the Auto Scaling group guide to update the User Data in the associated Launch Configuration.",
      "oneMoreStepEKS": "Deploy Fluent Bit in your EKS cluster",
      "eksDeamonSetTips_0": "One more step…",
      "eksDeamonSetTips_1": "In order to activate the log pipeline, please follow the procedures in ",
      "eksDeamonSetTips_2": " to deploy Fluent Bit in your EKS cluster.",
      "eksDeamonSetLink": "DeamonSet guide",
      "permissionInfo": "For source of Instance Group, please make sure the instances in the Instance Group has correct permission attached. You can view the the required permission in the the source detail page by clicking the source ID."
    },
    "logConfig": {
      "name": "Log Config"
    },
    "permission": {
      "name": "Instance permission",
      "notYet": "Please waiting for pipeline creation.",
      "alert": "Add permission to the instances",
      "alertDesc": "Solution use Systems Manager to configure and update Log Agent, S3 to store configurations and Kinesis to collect data. Please grant the following permissions to your EC2 Instance Profile or Auto Scaling group. Learn more about ",
      "grant": "Grant permissions to EC2 instances"
    },
    "lifecycle": {
      "name": "Lifecycle",
      "warmLog": "Warm log transition (days)",
      "coldLog": "Cold log transition (days)",
      "logRetention": "Log retention (days)"
    },
    "logProcessor": {
      "logProcessorTitle": "Log processor",
      "mergerTitle": "Log merger",
      "archiverTitle": "Log archiver",
      "metricMergerTitle": "Merger - Metric",
      "metricArchiverTitle": "Archiver - Metric",
      "stepFunction": "Step Function",
      "scheduler": "Scheduler",
      "scheduleExpression": "Schedule - expression",
      "lifecycle": "Lifecycle",
      "days": "days"
    }
  },
  "create": {
    "name": "Create",
    "ingestSetting": {
      "index": "Index",
      "indexName": "Index name",
      "indexNameDesc": "Enter the index name of the log saved in OpenSearch.",
      "indexNameError": "Please input an index name",
      "indexNameFormatError": "Index name must start with a lowercase letter and can only support lowercase letter, number, - and _",
      "indexNameDuplicated": "Please change index name",
      "create": "Create",
      "buffer": "Buffer",
      "bufferNone": "None",
      "bufferNoneDesc": "Log will ingested to OpenSearch directly. Use this option if your OpenSearch domain can support your log peak throughput.",
      "bufferKDS": "Kinesis Data Streams",
      "bufferKDSDesc": "Second-level ingestion with additional cost. Use this option if you need real-time ingestion and your log throughput is higher than your OpenSearch domain ingestion capacity.",
      "bufferS3": "Amazon S3",
      "bufferS3Desc": "Minute-level ingestion with additional cost. Use this option if you can accept minute-level latency for your log ingestion and your log throughput is higher than your OpenSearch domain ingestion capacity.",
      "bufferS3LightEngineDesc": "The bucket stores log data in its original format to ensure data integrity and traceability.",
      "bufferNoneNetworkTitle": "Network connectivity",
      "bufferNoneNetworkDesc": "When no buffer is selected, the log source will send logs directly to OpenSearch. Ensure that the log sources (e.g., instance group, EKS cluster, syslog agents) can connect to the selected OpenSearch domain.",
      "bufferNoneConfirm": " I acknowledge the requirement of network connectivity.",
      "bufferNoneNotCheckError": "Please confirm and check acknowledge the requirement of network connectivity.",
      "s3Bucket": "Amazon S3 bucket",
      "s3BucketDesc": "Select a bucket to store the log data.",
      "s3BucketPrefix": "Amazon S3 bucket prefix",
      "selectS3Bucket": "Please select Amazon S3 bucket",
      "s3PrefixInvalid": "Amazon S3 bucket prefix should not be '/' and must be end with '/'.",
      "s3BucketPrefixLightEngineDesc": "Select a bucket to store the log data. By default. The solution will add prefix of 'LightEngine/AppLogs/<table-name>' to log data.",
      "s3BucketPrefixDesc": "Override the default S3 prefix of the log location. By default, the solution will add prefix of 'AppLogs/<index-prefix>/year=%Y/month=%m/day=%d/' (in UTC) to log data.",
      "bufferSize": "Buffer size (MiB)",
      "bufferSizeDesc": "Specify the buffer size in your log source before sending to Amazon S3. Minimum: 1 MiB, maximum: 50 MiB.",
      "bufferSizeError": "The size of the buffer must be between 1MiB and 50MiB",
      "bufferInt": "Buffer interval (Seconds)",
      "bufferIntDesc": "Specify the interval to deliver data to S3 bucket. Minimum: 5, maximum: 300.",
      "bufferIntError": "The Buffer interval must be between 1 second and 86400 seconds",
      "compressType": "Compression for data records",
      "compressTypeDesc": "Solution can compress records before delivering them to your S3 bucket",
      "compressChoose": "Choose a compress type",
      "compressionMethod": "Compression",
      "shardNum": "Shard number",
      "shardNumDesc": "Specify the number of Shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second.",
      "shardNumError": "Shard Number must be greater than or equal to 1",
      "enableAutoS": "Auto Scaling",
      "enableAutoSDesc": "Choose Yes if you want the system automatically scale the shard numbers. The maximum number of scaling within 24 hours is 8.",
      "maxShardNum": "Maximum shard number",
      "maxShardNumDesc": "Specify the maximum number of Shards of the Kinesis Data Streams. ",
      "maxShardNumError": "Max shard number must be greater than Shard number",
      "overlapIndexError": "It looks like there is an active or deleted pipeline configured was overlap with the index you entered.  Please choose 'Edit' to update. ({{message}})",
      "overlapWithPrefix": "The index name overlaps with the index name you created earlier.",
      "duplicatedIndexError": "It looks like there is an active or deleted pipeline configured with the index you entered. To proceed with creating this new pipeline and sending logs to the same index, please select 'Continue Create'. If you would like to use a different index name instead, please choose 'Edit' to update.",
      "s3StorageClass": "S3 storage class",
      "s3StorageClassDesc": "Choose a S3 storage class to buffer your log data. "
    },
    "specifyOS": {
      "aosDomain": "OpenSearch domain",
      "aosDomainDesc1": "Select an imported cluster. You must ",
      "aosDomainDesc2": "import cluster",
      "aosDomainDesc3": " before you select from the list.",
      "aosDomainError": "Please select an OpenSearch domain",
      "selectDomain": "Select an OpenSearch domain",
      "logLifecycle": "Log Lifecycle",
      "warmLog": "Warm log transition (days)",
      "warmLogDesc1": "Move aged logs from hot storage to warm storage to save cost. You must enable ",
      "warmLogDesc2": "UltraWarm",
      "warmLogDesc3": " before using this.",
      "warmLogInvalid": "Warm log transition invalid.",
      "coldLog": "Cold log transition (days)",
      "coldLogDesc1": "Move aged logs from warm storage to cold storage to save cost. You must enable ",
      "coldLogDesc2": "Cold Storage",
      "coldLogDesc3": " before using this.",
      "coldLogInvalid": "Cold log transition invalid.",
      "coldLogMustThanWarm": "Cold log must larger than warm log transition.",
      "logRetention": "Log retention (days)",
      "logRetentionDesc": "Delete aged logs from OpenSearch domain.",
      "logRetentionError": "Log retention invalid.",
      "logRetentionMustLargeThanCodeAndWarm": "Log retention must larger than cold log transition and warm log transition.",
      "indexSuffix": "Index name",
      "indexSuffixDesc": "The default index is <index name>-YYYY-MM-DD. You can choose a different suffix to adjust the index rollover time window. For example, if you select “YYYY-MM-DD-HH” suffix, OpenSearch will rollover the index by hour."
    },
    "domainStatusCheckFailedError": "The connectivity check for the selected domain has failed. Please check its network or choose another domain."
  },
  "ingestion": {
    "name": "Ingest",
    "selectInstance": "Please select instances",
    "selectOnlineInstance": "Please select log agent online instance or waiting all log agent installed successfully.",
    "type": "Type",
    "source": "Source",
    "syslogConfig": "Syslog configuration guide",
    "syslogConfig5424": "RFC5424",
    "syslogConfig3164": "RFC3164",
    "step": {
      "createInstanceGroup": "Create Instance Group",
      "chooseInstanceGroup": "Choose Instance Groups",
      "applyConfig": "Apply Log Config",
      "createTags": "Create tags"
    },
    "createInstanceGroup": {
      "index": "Index",
      "method": "Creation Method",
      "new": "Create new",
      "newDesc": "Create a new log group and select instances.",
      "exists": "Choose exists",
      "existsDesc": "Choose existing log groups on the next page."
    },
    "chooseInstanceGroup": {
      "choose": "Choose Instance Groups",
      "instanceGroupRequiredError": "Please select Instance Group.",
      "chooseDesc": "Select multiple Instance Groups and apply the same Log Config.",
      "list": {
        "groups": "Groups",
        "name": "Name",
        "platform": "Platform",
        "created": "Created"
      }
    },
    "applyConfig": {
      "nameDesc": "Create a new Log Config or select exists.",
      "logConfig": "Log Config",
      "method": "Creation Method",
      "new": "Create new",
      "newDesc": "Create a new log config.",
      "exists": "Choose exists",
      "existsDesc": "Choose existing log configs.",
      "chooseExists": "Choose an existing log config.",
      "chooseExistsSyslog": "Choose an existing log config.  Only can choose syslog-supported log config (i.e., Single-line Text, Syslog, JSON).",
      "chooseConfig": "Choose a Log Config",
      "config": "Configuration",
      "configRequiredError": "Please select a Log Config",
      "inputLogPath": "Please input the log path",
      "logPathMustBeginWithSlash": "Log path must begin with / (slash)",
      "logPathMustBeginWithDriver": "Log path must begin with Driver (e.g. C:\\)"
    },
    "s3": {
      "ingestFromS3": "Ingest from Amazon Amazon S3",
      "step": {
        "specifySource": "Specify source",
        "specifyConfig": "Specify log config"
      },
      "specifySource": {
        "fromS3": "Ingest from Amazon S3 bucket",
        "s3": "Amazon S3 bucket",
        "s3Desc": "Select an Amazon S3 bucket where your application log lives",
        "selectS3": "Please select a Amazon S3 bucket",
        "chooseS3": "Select a Amazon S3 bucket",
        "logPrefix": "Log Prefix",
        "logPrefixDesc": "Specify the prefix of the logs",
        "fileType": "File Type",
        "fileTypeDesc": "Specify the way you compress your logs",
        "selectFileType": "Please select a log file type",
        "chooseFileType": "Choose a File Type",
        "fileZipped": "The file is zipped",
        "vpcId": "VPC",
        "vpcIdDesc": "Select the VPC will start EC2 instances for log ingestion.",
        "chooseVpc": "Please choose a VPC",
        "subnetIds": "Public subnets",
        "subnetIdsDesc": "Select the public subnets will start EC2 instances for log ingestion.",
        "chooseSubnet": "Please choose public subnets"
      },
      "specifyConfig": {
        "alert": "The 'Log Path' in the Log Config will be ignored as the Amazon S3 file path will be used when parsing your logs.",
        "logConfig": "Log Config",
        "selectConfig": "Please select a Log Config",
        "configDesc": "Choose an existing Log Config for ",
        "configDesc2": " file type specified as source. To create a new config, go to ",
        "config": "Configuration"
      }
    },
    "eks": {
      "ingestFromEKS": "Ingest from EKS",
      "specifySource": {
        "eksTitle": "EKS cluster",
        "eksDesc1": "Select an imported EKS cluster. You must ",
        "eksDesc2": "Import an EKS cluster",
        "eksDesc3": " before you select from the list.",
        "chooseEKS": "Select an EKS cluster",
        "selectEKS": "Please select an EKS cluster"
      }
    },
    "syslog": {
      "ingestFromSysLog": "Ingest from syslog",
      "settings": "Syslog server settings",
      "protocol": "Protocol",
      "protocolDesc": "Select the protocol of your syslog source",
      "protocolRequire": "Please select protocol",
      "chooseProtocol": "Choose a protocol type",
      "port": "Port number",
      "portDesc": "Specify the port to receive the syslog.",
      "portConflict": "Port conflicts with existing log source(s)",
      "portOutofRange": "The port number must be between 500 and 20000",
      "changePort": "Change port number",
      "guide": {
        "title": "Please refer to the following steps to ingest system logs",
        "alert": "Make sure that the Syslog sender is connected to the NLB network. The following is an example configuration of Rsyslog for your reference.",
        "step1Title": "1. Open the Rsyslog configuration file.",
        "step2Desc1": "Usually the Rsyslog configuration file path is",
        "step2Title": "2. Add the following information to the end of the Rsyslog configuration file",
        "step3Title": "3. Restart Rsyslog Service",
        "step3Desc1": "Execute",
        "step3Desc2": " or ",
        "step3Desc3": " command to restart Rsyslog.",
        "step4Title": "4. Use the logger command to generate a test log.",
        "step4Desc1": "For example, execute ",
        "step4Desc2": " command generates a log.",
        "step5Title": "5. View Rsyslog logs",
        "step5Desc1": "In Amazon Linux 2, Rsyslog logs are saved in ",
        "step5Desc2": " by default (note that other OS might use a different path), you can view the logs using ",
        "step5Desc3": " or ",
        "step5Desc4": " commands."
      }
    }
  },
  "logSourceDesc": {
    "ec2": {
      "create": "Create a pipeline",
      "title": "Instance Group",
      "instanceGroup": "Instance Group",
      "instanceGroups": "Instance Groups",
      "desc": " is a concept that represents a collection of one or multiple Amazon EC2 instances. The solution supports automatically collecting logs from the EC2 instances within an Instance Group. With this feature, you can:",
      "li1": "Select Amazon EC2 instance(s) or an Auto Scaling group to create Instance Group.",
      "li2": "One-click to install log agent (Fluent Bit) on all instances in an Instance Group.",
      "li3": "Use built-in or custom Log Config to parse logs before ingesting into OpenSearch.",
      "step1": {
        "permissions": "Permissions",
        "permissionsTitleDesc": "Grant permissions to Instance Group to ingest logs.",
        "permissionMethod": "Grant method",
        "permissionsDesc": "Choose how to grant permission to the Instance Group. The Instance Groups needs permissions to access System Manager, S3, and Kinesis Data Stream services for log collection agent configuration and log ingestion. Learn more about ",
        "permissionsDesc2": "Grant permission to EC2 instances",
        "auto": "Automatic",
        "permissionAuto": "Automatically add the permissions to Instance Profile associated with the instances.",
        "manual": "Manual",
        "permissionManual": "I will manually add the below required permissions after pipeline creation.",
        "permissionExpand": "Required permissions",
        "userDataExpand": "Expand to view the user data scripts",
        "title": "Choose an Instance Group",
        "titleDesc": "Instance Group is a collection of multiple instances that will be applied with same Log Config."
      },
      "step2": {
        "logPathDesc": "Enter the location of the log files. All files under the specified folder will be included."
      },
      "step3": {
        "panelTitle": "Buffer",
        "title": "Buffer settings",
        "desc": "Select a layer to buffer your logs before ingestion"
      }
    },
    "eks": {
      "title": "Amazon EKS",
      "desc": "Amazon EKS is a fully-managed and certified Kubernetes service. The solution supports automatically collecting logs from application runs on Amazon EKS. With this feature, you can:",
      "li1": "Automatically generate ready-to-use YAML file to easily install of log agent (Fluent Bit) on EKS.",
      "li2": "Use built-in or custom Log Config to parse logs before ingesting into OpenSearch.",
      "li3": "Ingest logs from EKS in different AWS accounts.",
      "step1": {
        "title": "Choose an Amazon EKS cluster",
        "settings": "Account Settings",
        "account": "Account",
        "accountDesc": "Select the AWS account at which the log is stored. To import a member account, go to ",
        "cluster": "Amazon EKS cluster"
      },
      "step2": {
        "titleDesc": "Create a new Log Config or select exists.",
        "panelName": "Log Config",
        "logConfig": "Log Config",
        "logConfigDesc": "Choose a Log Config to parse your logs.",
        "chooseALogConfig": "Choose a Log Config",
        "createNew": "Create new",
        "selectConfig": "Please select a Log Config",
        "logConfigName": "Log Config name"
      },
      "step3_1": {
        "title": "Select log processor",
        "desc": "Processor type",
        "lambda": "Lambda",
        "lambdaDesc": "Serverless processor that automatically scale out or in, cost efficient processor when daily log volume is less then 1 TB.",
        "osis": "OpenSearch Ingestion Service",
        "osisDesc": "Managed ingestion service that is cost efficient when log volume > 1TB/day, also provides rich log processing capabilities."
      },
      "step3": {
        "title": "Buffer",
        "desc": "Select a layer to buffer your logs before ingestion"
      },
      "step4": {
        "title": "Specify OpenSearch domain settings",
        "lightEngineTitle": "Specify Light Engine configuration"
      },
      "step5": {
        "title": "Pipeline alarms and tags"
      },
      "deleteAlarm1": "Please open the pipeline and delete all log sources first.",
      "roleCheckFailed": "The role used to send logs to Buffering layer takes too long to be active. Please try again."
    },
    "syslog": {
      "title": "Syslog endpoint",
      "desc": "Syslog is a standard network-based logging protocol that works on a wide variety of different types of devices and applications, such as Linux servers, firewalls, and network devices. This solution supports collecting logs from system agents (e.g., rsyslog, syslog-ng) directly. With this feature, you can:",
      "li1": "Automatically deploy a highly-available network endpoint to collect syslog sent over network.",
      "li2": "Use built-in Log Configs to parse standard syslog formats. (e.g., RFC3164, RFC5424)",
      "li3": "Ingest syslog via UDP and TCP protocols.",
      "step1": {
        "title": "Specify Syslog settings",
        "titleDesc": "Specify the settings for the endpoint to receive syslog.",
        "panelName": "Endpoint Settings",
        "desc": "Select the S3 bucket where your logs are stored.",
        "alert": "Currently only support ingesting logs from solution-deployed region.",
        "title2": "File path prefix filter",
        "desc2": "Filter files by file path prefix to accurately locate the files to be ingested. We support both full file path matching and Wildcard matching. For example, if the files to be ingested are all in the log/ folder, you can specify the prefix as log/. If you want to only ingest file with log as file name, you can specify the prefix as log/*.log. If this parameter is not set, the entire S3 bucket will be traversed.",
        "title3": "Ingestion mode",
        "desc3": "Specify the interval for the ingestion job to execute.",
        "onGoing": "On-going",
        "onGoingDesc": "The ingestion job will run when a new file is delivered to the specified S3 location",
        "oneTimeLoad": "One-time load",
        "oneTimeLoadDesc": "The ingestion job will run at creation and only will run once to load all files in the specified location."
      },
      "step2": {
        "titleDesc": "Create a new Log Config or select exists.",
        "panelName": "Log Config",
        "logConfig": "Log Config",
        "logConfigDesc": "Choose a log config to parse your logs. Only can choose syslog-supported log config (i.e., Single-line Text, Syslog, JSON).",
        "chooseALogConfig": "Choose a log config",
        "createNew": "Create new"
      },
      "step3_1": {
        "title": "Select log processor",
        "desc": "Processor type",
        "lambda": "Lambda",
        "lambdaDesc": "Serverless processor that automatically scale out or in, cost efficient processor when daily log volume is less then 1 TB.",
        "osis": "OpenSearch Ingestion Service",
        "osisDesc": "Managed ingestion service that is cost efficient when log volume > 1TB/day, also provides rich log processing capabilities."
      },
      "step3": {
        "title": "Buffer",
        "desc": "Select a layer to buffer your logs before ingestion"
      },
      "step4": {
        "title": "Specify OpenSearch domain settings"
      },
      "step5": {
        "title": "Pipeline alarms and tags"
      }
    },
    "s3": {
      "config": "S3 configuration",
      "title": "Amazon S3",
      "desc": "This solution supports ingesting logs that stored in a Amazon S3 bucket. You can use this feature to ingest logs of AWS services that support delivering logs to S3, ingest logs from on-premises, or ingest logs from third-party cloud providers that could be transferred to a S3 bucket. With this features, you can:",
      "li1": "Ingest logs in a specified S3 location continuously or perform one-time ingestion.",
      "li2": "Filter logs based on S3 prefix.",
      "li3": "Parse logs with custom Log Config.",
      "step1": {
        "title": "Choose a S3 Bucket",
        "alertCompressionType": "Note you can not change compression type when adding a new source into an existing pipeline, please make sure the new log source is in the same compression format.",
        "titleDesc": "Specify the settings for the log sources.",
        "panelName": "Amazon S3 Bucket settings",
        "desc": "Select the S3 bucket where your logs are stored. Note that only the S3 buckets in the same region can be selected.",
        "alert": "Currently only support ingesting logs from solution-deployed region.",
        "title2": "Prefix filter - optional",
        "desc2": "Filter log files by prefix to accurately locate the files to be ingested. The solution supports both full file path matching and Wildcard matching. If this parameter is not set, the entire S3 bucket will be traversed. Note that S3 prefix does NOT start with “/“.",
        "title3": "Ingestion mode",
        "desc3": "Specify the interval for the ingestion job to execute.",
        "title4": "Compression",
        "desc4": "Specify how the log files are compressed in S3",
        "athenaFormat": "Refer to the Athena supported compression format, click <0>This guide</0> to learn more.",
        "onGoing": "On-going",
        "onGoingDesc": "The ingestion job will run when a new file is delivered to the specified S3 location",
        "oneTimeLoad": "One-time",
        "oneTimeLoadDesc": "The ingestion job will run at creation and only will run once to load all files in the specified location."
      },
      "step2": {
        "titleDesc": "Create a new Log Config or select exists.",
        "panelName": "Log Config",
        "logConfig": "Log Config",
        "logConfigDesc": "Choose a log config to parse your logs.",
        "chooseALogConfig": "Choose a log config",
        "createNew": "Create new",
        "alert": "If you select a log config with filter enabled, filter in the log config will not take effect.",
        "alertCanNotChangeLogConfig": "Note you can not change log config when add a new source into an existing pipeline, please review and ensure the existing log config can parse the new log source.",
        "alertLightEngine": "Light Engine - S3 as source not support multi-line log format"
      },
      "step3": {
        "title": "Select log processor",
        "desc": "Processor type",
        "lambda": "Lambda",
        "lambdaDesc": "Serverless processor that automatically scale out or in, cost efficient processor when daily log volume is less then 1 TB.",
        "osis": "OpenSearch Ingestion Service",
        "osisDesc": "Managed ingestion service that is cost efficient when log volume > 1TB/day, also provides rich log processing capabilities."
      },
      "step4": {
        "title": "Specify OpenSearch domain settings"
      },
      "step5": {
        "title": "Pipeline alarms and tags"
      }
    }
  },
  "selectLogSource": {
    "create": "Create a pipeline",
    "name": "Select a log source",
    "logSources": "Log sources"
  },
  "installAgent": {
    "refreshSwitch": "Automatically refresh",
    "lastUpdateTime": "Last update: ",
    "invocationOutputFirst": "Instance is Offline, due to \"",
    "invocationOutputMid": "\". Refer to  ",
    "invocationOutputLast": " to troubleshoot and fix."
  },
  "instancePermission": {
    "title": "Instance permission",
    "alertContent": "Please make sure your instance have the following permissions."
  }
}
