{
  "name": "Application Log",
  "title": "Application Log Pipelines",
  "list": {
    "os": "OpenSearch",
    "lightEngine": "Light Engine",
    "engineType": "Engine Type",
    "indexTable": "Index / Table Name",
    "indexName": "Index name",
    "streamName": "Stream Name",
    "bufferLayer": "Buffer Layer",
    "created": "Created time",
    "logPath": "Log Path",
    "status": "Status",
    "logConfig": "Log Config"
  },
  "delete": "Delete Application Log",
  "deletePipeline": {
    "alarm": "Please open the pipeline and delete all log sources first."
  },
  "deleteTips": "Are you sure you want to delete the application log ",
  "detail": {
    "config": "General configuration",
    "osIndex": "OpenSearch Index",
    "logTable": "Log Table",
    "indexSuffix": "Index Suffix",
    "rolloverSize": "Rollover Size",
    "compressionType": "Compression Type",
    "openShards": "Open shards",
    "shardNumber": "Shard number",
    "aos": "OpenSearch",
    "domain": "Domain",
    "sampleDashboard": "Sample Dashboard",
    "fanout": "Consumers with enhanced fan-out",
    "shards": "Shards (Kinesis Data Streams)",
    "autoScaling": "(Auto Scaling)",
    "enabledAutoScaling": "Enable auto scaling",
    "maxShardNum": "Maximum Shard number",
    "created": "Created",
    "bucket": "Bucket",
    "kds": "Kinesis Data Streams",
    "noneBuffer": "None buffer layer",
    "analyticsEngine": "Analytics Engine",
    "grafanaDashboard": "Grafana Dashboard",
    "grafanaDashboardDetail": "Grafana Dashboard - Detail",
    "indexSettings": "Index Settings",
    "tableSettings": "Table Settings",
    "metricTableSettings": "Table Settings - Metric",
    "cfnStack": "CloudFormation Stack",
    "tip1": "If you create an log source from an instance group, make sure you have attached these ",
    "tip2": "permissions",
    "tip3": " to the instances. This is NOT required if you create log source from Syslog or EKS cluster.",
    "noLogging": "This log pipeline has no buffer layer, logs are sent drectly to OpenSearch domain from source, no log processor lambda was created, hence no logs to dispiay",
    "tab": {
      "ingestion": "Log Source",
      "sources": "Log Source",
      "logSources": "Log sources",
      "permission": "Permission",
      "logConfig": "Log Config",
      "bufferLayer": "Buffer Layer",
      "lifecycle": "Lifecycle settings",
      "analyticsEngine": "Analytics Engine",
      "sourceDetail": "Source detail",
      "tags": "Tags",
      "monitoring": "Monitoring",
      "alarm": "Pipeline Alarm",
      "logging": "Logs"
    },
    "ingestion": {
      "ingestionOverview": "Log Source Overview",
      "prefixFilter": "Prefix filter",
      "compression": "Compression",
      "source": "Source",
      "sourceType": "Source type",
      "status": "Status",
      "ingestionMode": "Log Source Mode",
      "logConfig": "Log Config",
      "instanceGroup": "Instance Group",
      "created": "Created",
      "delete": "Delete Log Source",
      "deleteTips": "Are you sure you want to delete the log source(s)? ",
      "upgradeGuide": "Upgrade Guide",
      "eksTips1": "If you need to create additional EKS log source(s) in this pipeline, go to ",
      "eksTips2": "ESK Cluster",
      "eksTips3": " create log source and select current pipeline.",
      "oneMoreStep": "One more step to complete the log source setup...",
      "groupTips": " We detect you had created an log source with an instance group based on EC2 auto scaling group, please wait for the log source's status become \"Created\", then go to the detail page of instance group(s), and follow the Auto Scaling Group Guide to update the User Data in the associated Launch Configuration.",
      "oneMoreStepEKS": "Deploy Fluent Bit in your EKS cluster",
      "eksDeamonSetTips_0": "One more step…",
      "eksDeamonSetTips_1": "In order to activate the log pipeline, please follow the procedures in ",
      "eksDeamonSetTips_2": " to deploy Fluent Bit in your EKS cluster.",
      "eksDeamonSetLink": "DeamonSet Guide",
      "permissionInfo": "For source of Instance Group, please make sure the instances in the Instance Group has correct permission attached. You can view the the required permission in the the source detail page by clicking the source ID."
    },
    "logConfig": {
      "name": "Log Config"
    },
    "permission": {
      "name": "Instance Permission",
      "notYet": "Please waiting for pipeline creation.",
      "alert": "Add permission to the instances",
      "alertDesc": "Solution use Systems Manager to configure and update Log Agent, S3 to store configurations and Kinesis to collect data. Please grant the following permissions to your EC2 Instance Profile or Autoscaling group. Learn more about ",
      "grant": "Grant permissions to EC2 instances"
    },
    "lifecycle": {
      "name": "Lifecycle",
      "warmLog": "Warm log transition(days)",
      "coldLog": "Cold log transition(days)",
      "logRetention": "Log retention(days)"
    },
    "logProcessor": {
      "logProcessorTitle": "Processor",
      "mergerTitle": "Merger",
      "archiverTitle": "Archiver",
      "metricMergerTitle": "Merger - Metric",
      "metricArchiverTitle": "Archiver - Metric",
      "stepFunction": "Step Function",
      "scheduler": "Scheduler",
      "scheduleExpression": "Schedule - expression",
      "lifecycle": "Lifecycle",
      "days": "days"
    }
  },
  "create": {
    "name": "Create",
    "step": {
      "ingestSetting": "Specify ingest setting",
      "specifyOS": "Specify OpenSearch domain settings",
      "createTags": "Alarms and Tags"
    },
    "ingestSetting": {
      "index": "Index",
      "indexName": "Index name",
      "indexNameDesc": "Enter the index name of the log saved in OpenSearch.",
      "indexNameError": "Please input an index name",
      "indexNameFormatError": "Index name must start with a lowercase letter and can only support lowercase letter, number, - and _",
      "indexNameDuplicated": "Please change index name",
      "create": "Create",
      "buffer": "Buffer",
      "bufferNone": "None",
      "bufferNoneDesc": "Log will ingested to OpenSearch directly. Use this option if your OpenSearch domain can support your log peak throughput.",
      "bufferKDS": "Kinesis Data Streams",
      "bufferKDSDesc": "Second-level ingestion with additional cost. Use this option if you need real-time ingestion and your log throughput is higher than your OpenSearch domain ingestion capacity.",
      "bufferS3": "Amazon S3",
      "bufferS3Desc": "Minute-level ingestion with additional cost. Use this option if you can accept minute-level latency for your log ingestion and your log throughput is higher than your OpenSearch domain ingestion capacity.",
      "bufferS3LightEngineDesc": "The bucket stores log data in its original format to ensure data integrity and traceability.",
      "bufferNoneNetworkTitle": "Network connectivity",
      "bufferNoneNetworkDesc": "When no buffer is selected, the log source will send logs to OpenSearch directly, you need to ensure the log sources (i.e., Instance group, EKS cluster, syslog agents) can connect to selected OpenSearch domain.",
      "bufferNoneConfirm": "I acknowledge the requirement of network connectivity.",
      "bufferNoneNotCheckError": "Please confirm and check acknowledge the requirement of network connectivity.",
      "s3Bucket": "Amazon S3 Bucket",
      "s3BucketDesc": "Select a bucket to store the log data. By default. The solution will add prefix of 'AppLogs/<index-prefix>/year=%Y/month=%m/day=%d/' (in UTC) to log data. You can customize the prefix in additional settings.",
      "s3BucketPrefix": "Amazon S3 Bucket Prefix",
      "selectS3Bucket": "Please select Amazon S3 Bucket",
      "s3PrefixInvalid": "Amazon S3 Bucket Prefix should not be '/' and must be end with '/'.",
      "s3BucketPrefixLightEngineDesc": "Select a bucket to store the log data. By default. The solution will add prefix of 'LightEngine/AppLogs/<table-name>' to log data.",
      "s3BucketPrefixDesc1": "By default, Solution appends the prefix '",
      "s3BucketPrefixDesc2": "' (in UTC) to the data it delivers to Amazon S3. You can override this default by specifying a custom prefix that includes expressions that are evaluated at runtime. If the files to be ingested are all in the log/ folder, you can specify the prefix as log/",
      "bufferSize": "Buffer size",
      "bufferSizeDesc": "Specify the buffer size in your log source before sending to Amazon S3. The higher buffer size may be lower in cost with higher latency. The lower buffer size will be faster in delivery with higher cost. Minimum: 1 MiB, maximum: 50 MiB.",
      "bufferSizeError": "The size of the buffer must be between 1MiB and 50MiB",
      "bufferInt": "Buffer interval",
      "bufferIntDesc": "The higher interval allows more time to collect data and the size of data may be bigger. The lower interval sends the data more frequently and may be more advantageous when looking at shorter cycles of data activity.",
      "bufferIntError": "The Buffer interval must be between 1 second and 86400 seconds",
      "compressType": "Compression for data records",
      "compressTypeDesc": "Solution can compress records before delivering them to your S3 bucket",
      "compressChoose": "Choose a compress type",
      "compressionMethod": "Compression method",
      "shardNum": "Shard number",
      "shardNumDesc": "Specify the number of Shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second. (* Kinesis shard adjustment limit per 24 hours)",
      "shardNumError": "Shard Number must be greater than or equal to 1",
      "enableAutoS": "Enable auto scaling?",
      "enableAutoSDesc": "Enable auto scaling of the Kinesis Data Streams shards?",
      "maxShardNum": "Maximum Shard number",
      "maxShardNumDesc": "Specify maximum number of shards.",
      "maxShardNumError": "Max Shard Number must be greater than Shard number",
      "overlapIndexError": "It looks like there is an active or deleted pipeline configured was overlap with the index you entered.  Please choose 'Edit' to update. ({{message}})",
      "duplicatedIndexError": "It looks like there is an active or deleted pipeline configured with the index you entered. To proceed with creating this new pipeline and sending logs to the same index, please select 'Continue Create'. If you would like to use a different index prefix instead, please choose 'Edit' to update.",
      "s3StorageClass": "S3 storage class for prefix",
      "s3StorageClassDesc": "Select a storage class for the prefix that buffer your log data. "
    },
    "specifyOS": {
      "aosDomain": "OpenSearch domain",
      "aosDomainDesc1": "Select an imported cluster. You must ",
      "aosDomainDesc2": "import cluster",
      "aosDomainDesc3": " before you select from the list.",
      "aosDomainError": "Please select an OpenSearch domain",
      "selectDomain": "Select an OpenSearch domain",
      "logLifecycle": "Log Lifecycle",
      "warmLog": "Warm log transition (days)",
      "warmLogDesc1": "Move aged logs from hot storage to warm storage to save cost. You must enable ",
      "warmLogDesc2": "UltraWarm",
      "warmLogDesc3": " before using this.",
      "warmLogInvalid": "Warm log transition invalid.",
      "coldLog": "Cold log transition (days)",
      "coldLogDesc1": "Move aged logs from warm storage to cold storage to save cost. You must enable ",
      "coldLogDesc2": "Cold Storage",
      "coldLogDesc3": " before using this.",
      "coldLogInvalid": "Cold log transition invalid.",
      "coldLogMustThanWarm": "Cold log must larger than warm log transition.",
      "logRetention": "Log retention (days)",
      "logRetentionDesc": "Delete aged logs from OpenSearch domain.",
      "logRetentionError": "Log retention invalid.",
      "logRetentionMustLargeThanCodeAndWarm": "Log retention must larger than cold log transition and warm log transition.",
      "indexSuffix": "Index Prefix",
      "indexSuffixDesc": "The default index is <index name>-YYYY-MM-DD. You can choose a different suffix to adjust the index rollover time window. For example, if you select “YYYY-MM-DD-HH” suffix, OpenSearch will rollover the index by hour."
    },
    "domainStatusCheckFailedError": "The connectivity check for the selected domain has failed. Please check its network or choose another domain."
  },
  "ingestion": {
    "name": "Ingest",
    "selectInstance": "Please select instances",
    "selectOnlineInstance": "Please select log agent online instance or waiting all log agent installed successfully.",
    "type": "Type",
    "source": "Source",
    "syslogConfig": "Syslog Configuration Guide",
    "syslogConfig5424": "RFC5424",
    "syslogConfig3164": "RFC3164",
    "step": {
      "createInstanceGroup": "Create instance group",
      "chooseInstanceGroup": "Choose instance groups",
      "applyConfig": "Apply log config",
      "createTags": "Create tags"
    },
    "createInstanceGroup": {
      "index": "Index",
      "method": "Creation Method",
      "new": "Create new",
      "newDesc": "Create a new log group and select instances.",
      "exists": "Choose exists",
      "existsDesc": "Choose existing log groups on the next page."
    },
    "chooseInstanceGroup": {
      "choose": "Choose instance groups",
      "instanceGroupRequiredError": "Please select Instance Group.",
      "chooseDesc": "Select multiple instance groups and apply the same Log Config.",
      "list": {
        "groups": "Groups",
        "name": "Name",
        "platform": "Platform",
        "created": "Created time"
      }
    },
    "applyConfig": {
      "name": "Apply log config",
      "nameDesc": "Create a new Log Config or select exists.",
      "logConfig": "Log Config",
      "method": "Creation Method",
      "new": "Create new",
      "newDesc": "Create a new log config.",
      "exists": "Choose exists",
      "existsDesc": "Choose existing log configs.",
      "chooseExists": "Choose an existing log config.",
      "chooseExistsSyslog": "Choose an existing log config.  Only can choose syslog-supported log config (i.e., Single-line Text, Syslog, JSON).",
      "chooseConfig": "Choose a Log Config",
      "config": "Configuration",
      "configRequiredError": "Please select a Log Config",
      "inputLogPath": "Please input the log path",
      "logPathMustBeginWithSlash": "Log path must begin with / slash"
    },
    "s3": {
      "ingestFromS3": "Ingest from Amazon Amazon S3",
      "step": {
        "specifySource": "Specify source",
        "specifyConfig": "Specify log config"
      },
      "specifySource": {
        "fromS3": "Ingest from Amazon S3 Bucket",
        "s3": "Amazon S3 Bucket",
        "s3Desc": "Select an Amazon S3 bucket where your application log lives",
        "selectS3": "Please select a Amazon S3 Bucket",
        "chooseS3": "Select a Amazon S3 Bucket",
        "logPrefix": "Log Prefix",
        "logPrefixDesc": "Specify the prefix of the logs",
        "fileType": "File Type",
        "fileTypeDesc": "Specify the way you compress your logs",
        "selectFileType": "Please select a log file type",
        "chooseFileType": "Choose a File Type",
        "fileZipped": "The file is zipped",
        "vpcId": "VPC",
        "vpcIdDesc": "Select the VPC will start EC2 instances for log ingestion.",
        "chooseVpc": "Please choose a VPC",
        "subnetIds": "Public Subnets",
        "subnetIdsDesc": "Select the public subnets will start EC2 instances for log ingestion.",
        "chooseSubnet": "Please choose public subnets"
      },
      "specifyConfig": {
        "alert": "The 'Log Path' in the Log Config will be ignored as the Amazon S3 file path will be used when parsing your logs.",
        "logConfig": "Log Config",
        "selectConfig": "Please select a Log Config",
        "configDesc": "Choose an existing log config for ",
        "configDesc2": " file type specified as source. To create a new config, go to ",
        "config": "Configuration"
      }
    },
    "eks": {
      "ingestFromEKS": "Ingest from EKS",
      "specifySource": {
        "eksTitle": "EKS Cluster",
        "eksDesc1": "Select an imported EKS cluster. You must ",
        "eksDesc2": "Import an EKS Cluster",
        "eksDesc3": " before you select from the list.",
        "chooseEKS": "Select an EKS Cluster",
        "selectEKS": "Please select an EKS Cluster"
      }
    },
    "syslog": {
      "ingestFromSysLog": "Ingest from syslog",
      "settings": "Syslog Source Setting",
      "protocol": "Protocol",
      "protocolDesc": "Select the protocol of your syslog source",
      "protocolRequire": "Please select protocol",
      "chooseProtocol": "Choose a protocol type",
      "port": "Port",
      "portDesc": "Specifies the port to receive the log server",
      "portConflict": "Port conflicts with existing log source(s)",
      "portOutofRange": "The port number must be between 500 and 20000",
      "changePort": "Change port number",
      "guide": {
        "title": "Please refer to the following steps to ingest system logs",
        "alert": "Make sure that the Syslog sender is connected to the NLB network. The following is an example configuration of rsyslog for your reference.",
        "step1Title": "1. Open the Rsyslog configuration file.",
        "step2Desc1": "Usually the Rsyslog configuration file path is",
        "step2Title": "2. Add the following information to the end of the Rsyslog configuration file",
        "step3Title": "3. Restart Rsyslog Service",
        "step3Desc1": "Execute",
        "step3Desc2": " or ",
        "step3Desc3": " command to restart Rsyslog.",
        "step4Title": "4. Use the logger command to generate a test log.",
        "step4Desc1": "For example, execute ",
        "step4Desc2": " command generates a log.",
        "step5Title": "5. View Rsyslog logs",
        "step5Desc1": "In Amazon Linux 2, Rsyslog logs are saved in ",
        "step5Desc2": " by default (note that other OS might use a different path), you can view the logs using ",
        "step5Desc3": " or ",
        "step5Desc4": " commands."
      }
    }
  },
  "logSourceDesc": {
    "ec2": {
      "title": "Instance Group",
      "instanceGroup": "Instance group",
      "desc": " is a concept that represents a collection of one or multiple Amazon EC2 instances. The solution supports automatically collecting logs from the EC2 instances within an Instance Group. With this feature, you can:",
      "li1": "Select Amazon EC2 instance(s) or an Auto Scaling Group to create Instance Group.",
      "li2": "One-click to install log agent (Fluent Bit) on all instances in an Instance Group.",
      "li3": "Use built-in or custom Log Config to parse logs before ingesting into OpenSearch.",
      "arch": {
        "title": "Architecture Diagram",
        "desc": "The architecture diagram to ingest log data into Amazon OpenSearch domain.",
        "descLightEngine": "The architecture diagram to ingest log data into Light Engine."
      },
      "step1": {
        "naviTitle": "Instance group",
        "permissions": "Permissions",
        "permissionMethod": "Permission grant method",
        "permissionsDesc": "The instance groups needs permissions to access System Manager, S3, and Kinesis Data Stream services for log collection agent configuration and log transmission. Please select a method to add permissions to the instances. Learn more about ",
        "permissionsDesc2": "Grant permission to EC2 instances",
        "permissionAuto": "Solution automatically adds the required permission to the selected instances’ Instance Profile",
        "permissionManual": "I will manually add the below required permissions after pipeline creation",
        "permissionExpand": "Expand to view required permissions",
        "userDataExpand": "Expand to view the user data scripts",
        "title": "Choose instance groups",
        "titleDesc": "Instance group is a collection of multiple instances that will be applied with same Log Config."
      },
      "step2": {
        "logPathDesc": "Enter the location of the log files. All files under the specified folder will be included."
      },
      "step3": {
        "naviTitle": "Buffer layer settings",
        "panelTitle": "Specify buffer layer settings",
        "title": "Buffer layer",
        "desc": "Select a layer to buffer your logs before ingestion"
      }
    },
    "eks": {
      "title": "Amazon EKS",
      "desc": "Amazon EKS is a fully-managed and certified Kubernetes service. The solution supports automatically collecting logs from application runs on Amazon EKS. With this feature, you can:",
      "li1": "Automatically generate ready-to-use YAML file to easily install of log agent (Fluent Bit) on EKS.",
      "li2": "Use built-in or custom Log Config to parse logs before ingesting into OpenSearch.",
      "li3": "Ingest logs from EKS in different AWS accounts.",
      "arch": {
        "title": "Architecture Diagram",
        "desc": "The architecture diagram to ingest log data into Amazon OpenSearch domain.",
        "descLightEngine": "The architecture diagram to ingest log data into Light Engine."
      },
      "step1": {
        "naviTitle": "Choose EKS cluster",
        "title": "Choose EKS cluster",
        "titleDesc": "Select an EKS cluster to collect logs from.",
        "settings": "Account Settings",
        "account": "Account",
        "accountDesc": "Select the AWS account at which the log is stored. To create a member account, go to ",
        "cluster": "EKS cluster"
      },
      "step2": {
        "naviTitle": "Log config",
        "title": "Apply log config",
        "titleDesc": "Create a new Log Config or select exists.",
        "panelName": "Log Config",
        "logConfig": "Log Config",
        "logConfigDesc": "Choose a log config to parse your logs.",
        "chooseALogConfig": "Choose a log config",
        "createNew": "Create new",
        "selectConfig": "Please select a Log Config",
        "logConfigName": "Log Config Name"
      },
      "step3_1": {
        "naviTitle": "Log processor",
        "title": "Select log processor",
        "desc": "Processor type",
        "lambda": "Lambda",
        "lambdaDesc": "Serverless processor that automatically scale out or in, cost efficient processor when daily log volume is less then 1 TB.",
        "osis": "OpenSearch Ingestion Service",
        "osisDesc": "Managed ingestion service that is cost efficient when log volume > 1TB/day, also provides rich log processing capabilities."
      },
      "step3": {
        "naviTitle": "Buffer layer settings",
        "title": "Buffer layer",
        "desc": "Select a layer to buffer your logs before ingestion"
      },
      "step4": {
        "naviTitle": "OpenSearch domain settings",
        "title": "Specify OpenSearch domain settings",
        "lightEngineTitle": "Specify Light Engine configuration"
      },
      "step5": {
        "naviTitle": "Pipeline alarms and tags",
        "title": "Pipeline alarms and tags"
      },
      "deleteAlarm1": "Please open the pipeline and delete all log sources first.",
      "roleCheckFailed": "The role used to send logs to Buffering layer takes too long to be active. Please try again."
    },
    "syslog": {
      "title": "Syslog Endpoint",
      "desc": "Syslog is a standard network-based logging protocol that works on a wide variety of different types of devices and applications, such as Linux servers, firewalls, and network devices. This solution supports collecting logs from system agents (e.g., rsyslog, syslog-ng) directly. With this feature, you can:",
      "li1": "Automatically deploy a highly-available network endpoint to collect syslog sent over network.",
      "li2": "Use built-in Log Configs to parse standard syslog formats. (e.g., RFC3164, RFC5424)",
      "li3": "Ingest syslog via UDP and TCP protocols.",
      "arch": {
        "title": "Architecture Diagram",
        "desc": "The architecture diagram to ingest log data into Amazon OpenSearch domain."
      },
      "step1": {
        "naviTitle": "Ingestion endpoint",
        "title": "Ingestion endpoint",
        "titleDesc": "Specify the settings for the endpoint to receive syslog.",
        "panelName": "Endpoint Settings",
        "desc": "Select the S3 bucket where your logs are stored.",
        "alert": "Currently only support ingesting logs from solution-deployed region.",
        "title2": "File path prefix filter",
        "desc2": "Filter files by file path prefix to accurately locate the files to be ingested. We support both full file path matching and Wildcard matching. For example, if the files to be ingested are all in the log/ folder, you can specify the prefix as log/. If you want to only ingest file with log as file name, you can specify the prefix as log/*.log. If this parameter is not set, the entire S3 bucket will be traversed.",
        "title3": "Ingestion mode",
        "desc3": "Specify the interval for the ingestion job to execute.",
        "onGoing": "On-going",
        "onGoingDesc": "The ingestion job will run when a new file is delivered to the specified S3 location",
        "oneTimeLoad": "One-time load",
        "oneTimeLoadDesc": "The ingestion job will run at creation and only will run once to load all files in the specified location."
      },
      "step2": {
        "naviTitle": "Log config",
        "title": "Apply log config",
        "titleDesc": "Create a new Log Config or select exists.",
        "panelName": "Log Config",
        "logConfig": "Log Config",
        "logConfigDesc": "Choose a log config to parse your logs. Only can choose syslog-supported log config (i.e., Single-line Text, Syslog, JSON).",
        "chooseALogConfig": "Choose a log config",
        "createNew": "Create new"
      },
      "step3_1": {
        "naviTitle": "Log processor",
        "title": "Select log processor",
        "desc": "Processor type",
        "lambda": "Lambda",
        "lambdaDesc": "Serverless processor that automatically scale out or in, cost efficient processor when daily log volume is less then 1 TB.",
        "osis": "OpenSearch Ingestion Service",
        "osisDesc": "Managed ingestion service that is cost efficient when log volume > 1TB/day, also provides rich log processing capabilities."
      },
      "step3": {
        "naviTitle": "Buffer layer settings",
        "title": "Buffer layer",
        "desc": "Select a layer to buffer your logs before ingestion"
      },
      "step4": {
        "naviTitle": "OpenSearch domain settings",
        "title": "Specify OpenSearch domain settings"
      },
      "step5": {
        "naviTitle": "Pipeline alarms and tags",
        "title": "Pipeline alarms and tags"
      }
    },
    "s3": {
      "title": "Amazon S3",
      "desc": "This solution supports ingesting logs that stored in a Amazon S3 bucket. You can use this feature to ingest logs of AWS services that support delivering logs to S3, ingest logs from on-premises, or ingest logs from third-party cloud providers that could be transferred to a S3 bucket. With this features, you can:",
      "li1": "Ingest logs in a specified S3 location continuously or perform one-time ingestion.",
      "li2": "Filter logs based on S3 prefix.",
      "li3": "Parse logs with custom Log Config.",
      "arch": {
        "title": "Architecture Diagram",
        "desc": "The architecture diagram to ingest log data into Amazon OpenSearch domain."
      },
      "step1": {
        "naviTitle": "Source settings",
        "title": "Specify source settings",
        "alertCompressionType": "Note you can not change compression type when adding a new source into an existing pipeline, please make sure the new log source is in the same compression format.",
        "titleDesc": "Specify the settings for the log sources.",
        "panelName": "Ingest from S3 bucket",
        "desc": "Select the S3 bucket where your logs are stored. Note that only the S3 buckets in the same region can be selected.",
        "alert": "Currently only support ingesting logs from solution-deployed region.",
        "title2": "Prefix filter - optional",
        "desc2": "Filter log files by prefix to accurately locate the files to be ingested. The solution supports both full file path matching and Wildcard matching. If this parameter is not set, the entire S3 bucket will be traversed.",
        "title3": "Ingestion mode",
        "desc3": "Specify the interval for the ingestion job to execute.",
        "title4": "Compression format",
        "desc4": "Specify how the log files are compressed in S3",
        "onGoing": "On-going",
        "onGoingDesc": "The ingestion job will run when a new file is delivered to the specified S3 location",
        "oneTimeLoad": "One-time",
        "oneTimeLoadDesc": "The ingestion job will run at creation and only will run once to load all files in the specified location."
      },
      "step2": {
        "naviTitle": "Log config",
        "title": "Apply log config",
        "titleDesc": "Create a new Log Config or select exists.",
        "panelName": "Log Config",
        "logConfig": "Log Config",
        "logConfigDesc": "Choose a log config to parse your logs.",
        "chooseALogConfig": "Choose a log config",
        "createNew": "Create new",
        "alert": "If you select a log config with filter enabled, filter in the log config will not take effect.",
        "alertCanNotChangeLogConfig": "Note you can not change log config when add a new source into an existing pipeline, please review and ensure the existing log config can parse the new log source."
      },
      "step3": {
        "naviTitle": "Log processor",
        "title": "Select log processor",
        "desc": "Processor type",
        "lambda": "Lambda",
        "lambdaDesc": "Serverless processor that automatically scale out or in, cost efficient processor when daily log volume is less then 1 TB.",
        "osis": "OpenSearch Ingestion Service",
        "osisDesc": "Managed ingestion service that is cost efficient when log volume > 1TB/day, also provides rich log processing capabilities."
      },
      "step4": {
        "naviTitle": "OpenSearch domain settings",
        "title": "Specify OpenSearch domain settings"
      },
      "step5": {
        "naviTitle": "Pipeline alarms and tags",
        "title": "Pipeline alarms and tags"
      }
    }
  },
  "selectLogSource": {
    "name": "Select log source",
    "logSources": "Log sources"
  },
  "installAgent": {
    "refreshSwitch": "Automatically refresh",
    "lastUpdateTime": "Last update: ",
    "invocationOutputFirst": "Instance is Offline, due to \"",
    "invocationOutputMid": "\". Refer to  ",
    "invocationOutputLast": " to troubleshoot and fix."
  },
  "instancePermission": {
    "title": "Instance Permission",
    "alertContent": "Please make sure your instance have the following permissions."
  }
}
