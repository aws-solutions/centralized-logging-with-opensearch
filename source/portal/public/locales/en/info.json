{
  "learnMore": "Learn more",
  "accessProxy": {
    "name": "Access Proxy",
    "tip1": "Access Proxy creates a Nginx based proxy (behind ",
    "alb": "Application Load Balancer",
    "tip2": ") which allows you to access the OpenSearch Dashboards through Internet.",
    "prerequisites": "Prerequisites",
    "pre1": "1. Domain name",
    "pre2": "2. The domain associated SSL certificate in ",
    "acm": "Amazon Certificate Manager (ACM)",
    "pre3": "3. An EC2 public key",
    "createProxy": "Create an Access Proxy"
  },
  "alarm": {
    "name": "Alarms",
    "tip1": "Amazon OpenSearch Service provides a set of ",
    "tip2": "recommended CloudWatch alarms",
    "tip3": ", Centralized Logging with OpenSearch can help customers to create the alarms automatically, and sent notification to your email (or SMS) via SNS.",
    "createAlarm": "Create OpenSearch Alarms"
  },
  "monitoring": {
    "intro": "This tab provides key metrics of each components within the log ingestion pipeline, making it easy for you to monitor the end-to-end log ingestion status(Linux instance supported). Please note, enabling monitoring will incur additional cost, please refer to the following link for cost estimation.",
    "monitoringCost": "Monitoring Cost",
    "flbAgents": "Fluent Bit agents",
    "syslog": "Syslog metrics"
  },
  "apacheLogFormat": {
    "name": "Apache Log Format",
    "tip1": "Apache HTTP Server capture detailed information about errors and request in log files. You can find the log format configuration in Apache HTTP Server configuration file, such as the ",
    "tip2": " file. The log format directive starts with ",
    "sample": "Sample Configuration",
    "apacheLog": "Apache HTTP Server Log Files"
  },
  "apacheLogParsing": {
    "name": "Sample log parsing",
    "tip1": "Please provide your apache log in Apache log file such as ",
    "sampleLog": "Sample Log",
    "configLogApache": "Configuring Logging in Apache"
  },
  "creationMethod": {
    "name": "Creation method",
    "tip1": "When import OpenSearch domains, you need to specify the networking configuration associated with the Log Processing Layer. The solution will automatically place Lambda (or other compute resource) in this layer. The Log Processing Layer must have access to the OpenSearch domain.",
    "auto": "Automatic",
    "tip21": "The solution will detect if there is a need to create a ",
    "tip22": "VPC Peering Connection",
    "tip23": ". If needed, the solution will automatically create a VPC Peering Connection, update route table, and update the security group of OpenSearch domain.",
    "manual": "Manual",
    "tip3": " Manually specify the Log Processing Layer networking information. You may need to create VPC Peering Connection, update route table and security group of OpenSearch domain.",
    "importDomain": "Import OpenSearch domain"
  },
  "ingestionCreationMethod": {
    "name": "Log enabling",
    "tip1": "Solution can automatically detect the log location, or you can specify the log location manually.",
    "auto": "Automatic",
    "tip2": "The solution will automatically detect the log location of the selected AWS service. If needed, it will enable the service log and save to a centralized log bucket.",
    "manual": "Manual",
    "tip3": "Manually input the AWS service source and its log location. The solution will read logs from the location you specified."
  },
  "instanceGroupCreationMethod": {
    "name": "Instance Group Creation",
    "tip1": "Create a new instance group, or choose an existing Instance Group created before.",
    "instanceGroup": "Instance Group"
  },
  "logConfigPath": {
    "name": "Log Path",
    "tip1": "Specify the log file locations. If you have multiple locations, please write all the locations and split using ' , '. e.g.",
    "eks": {
      "title": "For EKS Log, for Sidecar and DamonSet, refer to the following instructions for configuration:",
      "dtip1": "For nginx as an example, when Amazon Linux2 is selected as the image of the node, if the user deploys the same application under the same EKS Cluster and uses the Namespace to distinguish different environments, it is recommended to use the following log path",
      "dtip2Title": "The format of log path is:",
      "dtip2": "The <namespace> is the corresponding Namespace that distinguishes different environments, <application_name> is the name of the deployed application, <container_name> is the name of deployed container. The names of application and container are defined in the Yaml file. This is different from deploying nginx on EC2, where log names are often used under EC2. For access.log, the log name is defined in nginx.conf. When creating a configuration file in Solution, pay attention to the path location.",
      "stip1": "That is, attach a dedicated log collection container to the Pod, and use emptyDir to share the log directory to allow the Fluent Bit container to read the data. Fluent Bit containers share storage, network and other resources with application containers. The volume defined in Solution is named app-log. For example: log path",
      "stip2": ",The corresponding deployed emptyDir shared volume is named app-log, refer to the following figure:"
    }
  },
  "logLifecycle": {
    "name": "Log lifecycle",
    "tip1": "The solution will insert an ",
    "ism": "Index State Management (ISM)",
    "tip2": " into the OpenSearch domain. The life cycle will periodically move your indices in OpenSearch to save cost.",
    "ismLink": "Index State Management"
  },
  "logProcessing": {
    "name": "Log Processing",
    "tip1": "The solution will provision Lambda (or other compute resource) to process logs using these networking configurations. You can specify the log processing networking layer when import OpenSearch domains.",
    "note": "Note",
    "tip2": "The log processing layer has access to the OpenSearch domain.",
    "importDomain": "Import OpenSearch domain"
  },
  "logProcessingNetwork": {
    "name": "Log processing network",
    "tip1": "When import OpenSearch domains, you need to specify the networking configuration associated with the Log Processing Layer. The solution will automatically place Lambda (or other compute resource) in this layer. The Log Processing Layer must have access to the OpenSearch domain.",
    "s3Access": "Amazon S3 Service access",
    "tip21": "By default, The solution will output error logs to Amazon S3. Please guarantee the log processing layer has network access to Amazon S3. You can do it by place the log processing layer in public subnets, use ",
    "tip22": "AWS PrivateLink for Amazon S3",
    "tip23": " or via ",
    "tip24": " NAT Gateways",
    "cwLogs": "CloudWatch Logs access",
    "tip31": " Many AWS services output service logs to ",
    "tip32": "CloudWatch Logs",
    "tip33": ". If you use Solution to ingest service logs. Please guarantee the log processing layer has network access to CloudWatch Logs.",
    "kdsAccess": "Kinesis Data Streams access",
    "tip4": "Application logs are sent to Kinesis Data Streams in Solution. Please guarantee the log processing layer has networking access to Kinesis Data Streams."
  },
  "nginxLogFormat": {
    "name": "Nginx Log Format",
    "tip1": "Nginx capture detailed information about errors and request in log files. You can find the log format configuration in Nginx configuration file, such as the ",
    "tip2": "format directive starts with ",
    "sample": "Sample Configuration",
    "configNginx": "Configuring Logging in Nginx",
    "alert1": "Note: Nginx type log configuration does not support Nginx configuration in JSON format. If your Nginx configuration is in JSON format, please select the log type as JSON."
  },
  "nginxLogParsing": {
    "name": "Sample log parsing",
    "tip1": "Please provide your nginx log in Nginx log file such as ",
    "sample": "Sample Log",
    "configNginx": "Configuring Logging in Nginx"
  },
  "regExLogFormat": {
    "name": "RegEx Log Format",
    "tip1": "Solution uses custom Ruby Regular Expression to parse logs. It supports both single-line log format and multiple input format. Write the regular expression in ",
    "rubular": "Rubular",
    "tip2": " to validate first and input the value here.",
    "link1": "Regular Expression",
    "link2": "Rubular: A Rudy-based regular expression editor",
    "link3": "Regular Expression in Fluent Bit"
  },
  "sampleDashboard": {
    "name": "Sample dashboard",
    "tip1": "The solution will insert a pre-configured dashboard into the OpenSearch domain if ",
    "tip2": " being selected. The dashboard name will be consistent with your index name."
  },
  "lightEngineSampleDashboard": {
    "name": "Sample dashboard",
    "tip1": "The solution will insert a pre-configured dashboard into the Grafana server if ",
    "tip2": " being selected. The dashboard name will be consistent with your table name."
  },
  "lightEngineTableName": {
    "name": "Log table name",
    "tip": "Table names are recommended to use lowercase letters, numbers, and other special characters except underscore (_). The length must be less than or equal to 255 characters. Exceeding this limit generates an error. For detailed specifications, please refer to the <0>Document</0>"
  },
  "lightEngineLogProcess": {
    "name": "Log Processor",
    "tip1": "Set the frequency for triggering the Log Processor task execution, for example, rate(5 minutes) indicates executing it every 5 minutes.",
    "tip2": "The primary purpose of the Log Processor (implemented as an Amazon Step Functions) is to efficiently handle raw log files stored on Amazon S3, processing them in batches, converting them into Apache Parquet format, and autonomously partitioning the data according to factors like time and region."
  },
  "lightEngineLogMerge": {
    "name": "Log Merger",
    "tip1": "Set the execution schedule for triggering the Log Merger task, and it can be executed once a day, for example, cron(0 1 * * ? *) indicates execution at 1 AM (UTC )daily.",
    "tip2": "The primary purpose of the Log Merger (implemented as an Amazon Step Functions) is to consolidate Parquet small files and data partitions, thereby reducing the number of files, minimizing S3 API operation costs, lowering S3 storage expenses, and enhancing performance as the volume of query data grows."
  },
  "lightEngineLogArchive": {
    "name": "Log Archiver",
    "tip1": "Set the execution schedule for triggering the Log Archive task, and it can be executed once a day, for example, cron(0 2 * * ? *) indicates execution at 2 AM (UTC) daily.",
    "tip2": "The primary purpose of the Log Archiver (implemented as an Amazon Step Functions) is to transfer expired data from Centralized storage to an archive until the lifecycle rule deletes the files, as well as to update the Glue data catalog and remove expired table partitions."
  },
  "s3FileType": {
    "name": "File Type",
    "tips": "You can choose a specific file type for logs stored in Amazon S3. Gzip only supports the compression of a single file."
  },
  "eksPattern": {
    "name": "Collection Mode",
    "tip1": " ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them.",
    "tip2": " is a separate container that runs alongside an application container in a Kubernetes pod."
  },
  "eksIamRole": {
    "name": "IAM Role ARN",
    "tip1": "When importing an EKS cluster, we automatically create an EKS IAM role for use in EKS delivery streams."
  },
  "configTimeFormat": {
    "name": "Time format",
    "strftime": "strftime function",
    "generateFormat": "Generate time format",
    "tip1": "Solution supports all time formats provided by the ",
    "tip2": ". That is, log time strings that can be formatted by the strftime function can be parsed and used by Solution."
  },
  "configFilter": {
    "name": "Filter",
    "sample": "Filter example: ",
    "tips1": "Take the Apache Json format log as an example, the log content is as follows:",
    "tips2": "Log filter conditions:",
    "tips2_1": "Only retain requests whose method is POST, GET, POST, DELETE",
    "tips2_2": "Filter out requests starting with /user/ and requests with paths /login and /logout",
    "tips2_3": "Only retain logs whose level is error and warn",
    "tips3": "The filters used are as follows:",
    "tips4": "The filtered logs are as follows:",
    "filterLink": "Select or exclude records per patterns"
  },
  "proxyInstance": {
    "name": "Proxy Instance Type/Number",
    "tips": "This recommendation table is base on page refresh time and average query delay, please feel free to create proxy and test base on your own use cases.",
    "conUser": "Concurrent User Number",
    "instanceType": "Proxy Instance Type",
    "proxyNumber": "Proxy Instance Number"
  },
  "s3PrefixFilter": {
    "name": "Prefix filter",
    "desc": "Here are some sample for you to set prefix filter:",
    "li1": "If the files to be ingested are all in the log/ folder, you can specify the prefix as log/",
    "li2": "If you want to only ingest file with log as file extension, you can specify the prefix as log/*.log"
  },
  "pipelineAlarm": {
    "name": "Pipeline Alarms",
    "desc": "Pipeline alarms will be triggered if any critical metric cross the threshold, which will send notifications to a SNS topic. Those alarms are created based on the best practices for log pipeline monitoring and troubleshooting, you can custom those alarms or create new alarms in CloudWatch",
    "link1": "Pipeline alarms",
    "link2": "Amazon CloudWatch Alarms"
  },
  "osi": {
    "name": "Amazon OpenSearch Ingestion",
    "desc": "Amazon OpenSearch Ingestion is a fully managed, serverless data collector that delivers real-time log, metric, and trace data to Amazon OpenSearch Service domains and OpenSearch Serverless collections.",
    "link1": "Learn more",
    "link2": "Cost details"
  },
  "bufferLayer": {
    "name": "Buffer layer",
    "desc": "Buffer layer is designed for a robust system between the log source and log destination. This layer can decouple the source and destination and accept more log ingestion requests, and also can buffer the logs for retry when log analytics engine has server issue or performance issue."
  }
}
