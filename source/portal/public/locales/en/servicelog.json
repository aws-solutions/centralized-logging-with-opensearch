{
  "name": "AWS service log",
  "title": "AWS service log pipelines",
  "list": {
    "type": "AWS Service type",
    "account": "Account",
    "source": "Source",
    "domain": "OpenSearch domain",
    "engineType": "Engine type",
    "target": "Index / Table name",
    "created": "Created",
    "status": "Status"
  },
  "delete": "Delete Service Log",
  "deleteTips": "Are you sure you want to delete the service log ",
  "detail": {
    "type": "AWS service type",
    "resources": "AWS service resource",
    "functionName": "Function name",
    "bucketName": "Bucket name",
    "distributionId": "Distribution ID",
    "trailName": "CloudTrail name",
    "dbID": "Database identifier",
    "albName": "ALB name",
    "wafName": "Web ACL name",
    "vpcId": "VPC ID",
    "config": "Config name",
    "indexSuffix": "Index suffix",
    "rolloverSize": "Rollover size",
    "compressionType": "Compression type",
    "samplingRate": "Sampling rate",
    "logType": "Log type",
    "kdsShardNum": "Kinesis Data Streams shard number",
    "enableAS": "Auto Scaling",
    "kdsMaxShard": "Maximum shard number",
    "fields": "Fields",
    "lifecycleSettings": "Lifecycle settings",
    "processor": {
      "type": "Processor type",
      "resource": "Resource",
      "lambda": "AWS Lambda",
      "osi": "OpenSearch Ingestion Service",
      "pipelineCapacity": "OpenSearch Ingestion capacity",
      "ingestionOCU": "OCU",
      "concurrency": "Reserved concurrency"
    },
    "lightEngine": {
      "table": "Glue table",
      "tableName": "Table name",
      "database": "Database",
      "classification": "Classification"
    }
  },
  "tab": {
    "overview": "Overview",
    "lifecycle": "Lifecycle",
    "logSource": "Log source",
    "analyticsEngine": "Analytics Engine",
    "logProcessor": "Log processor",
    "monitoring": "Metrics",
    "logging": "Logs",
    "alarm": "Alarms",
    "tags": "Tags"
  },
  "overview": {
    "name": "Overview",
    "logLocation": "Log location",
    "logGroup": "Log group",
    "createSample": "Create sample dashboard",
    "created": "Created"
  },
  "lifecycle": {
    "name": "Lifecycle",
    "warmLog": "Warm log transition (days)",
    "coldLog": "Cold log transition (days)",
    "retention": "Log retention (days)"
  },
  "create": {
    "name": "Create a pipeline",
    "desc": "Select an AWS service from which to ingest logs into the log analytics engine.",
    "select": "Select an AWS service",
    "awsServices": "AWS services",
    "logSourceEnable": "Log source enabling",
    "logSourceEnableDesc": "This solution can automatically detect the log location, or you can specify the log location manually.",
    "auto": "Automatic",
    "autoDesc": "The solution will automatically detect the log location of the selected AWS service. If needed, it will enable the service log and save to a centralized log bucket.",
    "manual": "Manual",
    "manualDesc": "Manually input the AWS service source and its log location. The solution will read logs from the location you specified.",
    "awsServiceLogSettings": "AWS service log settings",
    "awsServiceLogSettingsDesc": "Specify the log source setting of the AWS service.",
    "source": "Source",
    "sourceDesc": "Select the source type where the AWS service delivers to. Review your AWS service settings to determine where the logs are being delivered.",
    "ingestLogType": "Log type",
    "ingestLogTypeDesc": "Choose the log type you would like to ingest.",
    "ingestTypeS3": "Standard access logs",
    "ingestTypeKDS": "Real-time logs",
    "ingestTypeFullRequest": "Full Request",
    "ingestTypeSampledRequest": "Sampled Request",
    "ingestTypeALB": "Application Load Balancer access logs",
    "ingestTypeAmazonS3": "Amazon S3",
    "ingestTypeCloudWatch": "Amazon CloudWatch Logs",
    "ingestTypeMySQL": "RDS/Aurora MySQL",
    "ingestTypePostgreSQL": "RDS/Aurora PostgreSQL",
    "logLocation": "Log location",
    "logLocationDesc": "Specify the location of the logs in Amazon S3. For instance, s3://bucket-name/prefix.",
    "logLocationError": "Please input the log location",
    "logLocationInvalidError": "Amazon S3 log location invalid.",
    "logLocationRDSDesc": "Select the log type and specify the CloudWatch Log Group ARN of the RDS ${conjection}.",
    "flowLogLocation": "VPC Flow Logs location",
    "flowLogLocationDesc": "Specify the ARN of the CloudWatch Logs log group where the flow log stores.",
    "savedTips": "The AWS service has logs saved at ",
    "service": {
      "s3": "Amazon S3",
      "trail": "AWS CloudTrail",
      "rds": "Amazon RDS",
      "cloudfront": "Amazon CloudFront",
      "lambda": "AWS Lambda",
      "elb": "Elastic Load Balancing",
      "waf": "AWS WAF",
      "vpc": "VPC Flow Logs",
      "config": "AWS Config"
    }
  },
  "cluster": {
    "aosDomain": "OpenSearch domain",
    "aosDomainDesc1": "Select an imported cluster. You must ",
    "aosDomainDesc2": "import cluster",
    "aosDomainDesc3": " before you select from the list.",
    "aosDomainError": "Please select an OpenSearch domain",
    "selectOS": "Select an OpenSearch domain",
    "sampleDashboard": "Sample dashboard",
    "sampleDashboardDesc": "Create a predefined sample dashboard in the log analytics engine.",
    "additionalSetting": "Additional settings",
    "indexPrefix": "Index name",
    "indexPrefixDesc": "Enter the index name to be stored in OpenSearch. By default, the index rolls over daily. You can choose a different suffix to adjust the index rollover time window.",
    "indexPrefixDesc1": "The default index is <index name>",
    "indexPrefixDesc2": ". You can choose a different suffix to adjust the index rollover time window. For example, if you select “YYYY-MM-DD-HH” suffix, OpenSearch will rollover the index by hour.",
    "inputIndex": "Input the index name",
    "logLifecycle": "Log lifecycle",
    "warmLog": "Warm log transition settings",
    "warmLogDesc1": "Move aged logs from hot storage to warm storage to save cost. You must enable ",
    "warmLogDesc2": "UltraWarm",
    "warmLogDesc3": " before using this.",
    "coldLog": "Cold log transition (days)",
    "coldLogDesc1": "Move aged logs from warm storage to cold storage to save cost. You must enable ",
    "coldLogDesc2": "Cold storage",
    "coldLogDesc3": " before using this.",
    "logRetention": "Log retention (days)",
    "logRetentionDesc": "Delete aged logs from OpenSearch domain.",
    "shardNum": "Number of shards",
    "shardNumDesc": "Specify the number of shards. Click the info button to learn how to choose the number of shards.",
    "inputShardNum": "Please input number of Shards",
    "replicaNum": "Number of replicas",
    "replicaNumDesc": "Specify the number of replicas. Replicas improve search performance, you might want more if you have a read-heavy workload. ",
    "inputReplica": "Please input number of Replicas",
    "shardEmptyError": "Please input number of Shards",
    "shardNumError": "Number of Shards must be greater than or equal to 1.",
    "rolloverError": "Index rollover by capacity must be greater than 0.",
    "rolloverTitle": "Rollover by capacity",
    "rolloverDesc": "OpenSearch will rollover your index when index size reaches below specified size, regardless of the rollover time window. Note OpenSearch will append a 6 digit suffix to the index name automatically.",
    "compressType": "Compression type",
    "compressTypeDesc": "Specify which compression type to use for this index. By default, the solution uses best_compression which uses DEFLATE for a higher compression ratio, at the expense of slightly slower stored fields performance. ",
    "warmImmediately": "Move rollover index to warm storage immediately",
    "warmByDays": "Move log by age (days)",
    "domainStatusCheckFailedError": "The connectivity check for the selected domain has failed. Please check its network or choose another domain."
  },
  "s3": {
    "s3logEnable": "Amazon S3 Access log enabling",
    "title": "Amazon S3",
    "alert": "The Log processing Layer must have access to Amazon S3.",
    "bucket": "Amazon S3 bucket",
    "bucketDesc": "Select an existing Amazon S3 bucket from the selected AWS account.",
    "bucketEmptyError": "Please select a Amazon S3 bucket",
    "notEnableTips": "The bucket has not enabled Amazon S3 Access Log. The system will enable the logs.",
    "selectBucket": "Select a Amazon S3 bucket",
    "bucketName": "Bucket name",
    "bucketNameDesc": "Input the bucket name which the Access Logs belongs to.",
    "inputBucket": "Please input Amazon S3 bucket",
    "needEnableLogging": "Please enable the Amazon S3 Access logging",
    "noLogOutput": "No log output",
    "noLogOutputDesc": "The log configuration of the selected bucket has not been enabled. Would you like to enable logging and output to the ",
    "desc": {
      "ingest": "Amazon S3 server access logging provides detailed records for the requests made to the bucket. S3 access log can be enabled and saved in another S3 bucket."
    }
  },
  "trail": {
    "logEnable": "AWS CloudTrail log enabling",
    "alert": "The Log Processing Layer must have access to the Amazon S3.",
    "trail": "Trail",
    "select": "Select the Trail from the selected AWS account.",
    "manual": "Enter an existing trail from the selected AWS account. ",
    "trailError": "Please select a CloudTrail",
    "trailManualError": "Please enter a CloudTrail",
    "selectTrail": "Select a CloudTrail",
    "logSource": "Log source",
    "logSourceDesc": "Specify the CloudTrail log source location. By default, CloudTrail outputs the logs into a S3 bucket, if you had configured CloudTrail outputs logs to CloudWatch Logs, the solution also supports pulling logs from CloudWatch Logs.",
    "logSourceCWLDest": "The trail is saved at the log group of ",
    "logSourceEmptyError": "Please select log source",
    "chooseLogSource": "Choose a log source",
    "alertCWL": "The Log Processing Layer must have access to the CloudWatch Logs log group.",
    "noOutput": "No log output to CloudWatch Logs",
    "noOutputDesc1": "The solution detects that the selected Trail has not enabled log output to CloudWatch Logs. Would you like to enable logging and output to the ",
    "noOutputDesc2": " log group? \n Please note, standard CloudWatch and CloudWatch Logs charges apply.",
    "needEnableLogging": "Please enable the CloudTrail Flow Log",
    "needSameRegion": "Only supports CloudTrail Logs from Amazon S3 in same region, please manually change the s3 bucket and retry again.",
    "diffRegion": "The Trail is saved at {{bucket}} that is in a different region ({{logRegion}}) than the solution' home region ({{homeRegion}}). To proceed, please update the selected CloudTrail's logging configuration to deliver trails to a Amazon S3 bucket at {{homeRegion}} region.",
    "error": {
      "s3Empty": "Please enter the log location of the CloudTrail",
      "cwlEmpty": "Please enter CloudWatch Logs log group name"
    },
    "desc": {
      "ingest": "AWS CloudTrail monitors and records account activity across your AWS infrastructure. It outputs all the data to the specified S3 bucket or a CloudWatch log group.",
      "trailLog": "AWS CloudTrail Server Access Logs"
    }
  },
  "rds": {
    "creation": "RDS logs creation",
    "alert": "The Log Processing Layer must have access to Amazon CloudWatch Logs.",
    "enableAlert1": "Enable Amazon RDS logs requires you have the correct Parameter Group. Find more information in ",
    "enableAlert2": "how to enable logs for an Amazon RDS MySQL instance",
    "dbID": "Database identifier",
    "selectDB": "Select an existing database from the selected AWS account.",
    "dbError": "Please select a database identifier",
    "select": "Select a database identifier",
    "logType": "Log type",
    "logTypeDesc": "Select the logs you want to ingest into log Analytics Engines.",
    "errorLog": " Error log",
    "slowLog": " Slow query log",
    "generalLog": " General log",
    "auditLog": " Audit log",
    "inputDBID": "Database identifier",
    "inputDBIDDesc": "Input the identifier of the database",
    "inputDBIDError": "Please input database identifier",
    "dbType": "Database type",
    "selectTheDBType": "Select the database type",
    "selectDBType": "Select database type",
    "desc": {
      "ingest": "Ingest Amazon RDS/Aurora database instance logs to log analytics engine, and then perform real-time analysis of log data.",
      "rdsLog": "Amazon RDS Server Access Logs"
    }
  },
  "cloudfront": {
    "enabled": "CloudFront logs enabling",
    "alert": "The Log Processing Layer must have access to Amazon S3.",
    "alertRealtime": "The log processing layer must have access to Amazon Kinesis Data Streams.",
    "distribution": "CloudFront Distribution",
    "distributionDesc": "Select an existing CloudFront Distribution from the selected AWS account.",
    "cloudfrontError": "Please select a CloudFront Distribution",
    "cloudfrontWarning": "This CloudFront Distribution has not enable access log yet, please choose another distribution.",
    "selectDistribution": "Select a CloudFront Distribution",
    "distributionId": "CloudFront Distribution ID",
    "distributionIdDesc": "Input the CloudFront Distribution ID. This just for identification purpose.",
    "distributionIdPlace": "Distribution ID",
    "ingestedFields": "Ingested fields",
    "ingestedFieldsDesc": "Select the CloudFront access log fields you want to ingest into the Amazon OpenSearch cluster.",
    "enrichedFields": "Enrichment",
    "enrichedFieldsDesc": "Enrich the log data with additional fields. ",
    "location": "Location",
    "locationDesc": "Enrich the data with additional location information.",
    "osAgent": "OS/User Agent",
    "osAgentDesc": "Enrich the data with additional Operating System and User Agent information.",
    "logType": "Log type",
    "logTypeDesc": "Specify what types of logs you want to collect.",
    "selectLogType": "Please select log type",
    "pleaseConfirm": "Please click the confirm button",
    "existTips": "Existing real-time log configuration detected",
    "existTipsDesc": "The selected distribution has enabled real-time logs with an existing log configuration attached. To use this feature, the solution will create a new real-time log configuration to replace the existing one, please confirm to proceed.",
    "noOutput": "No real-time log output",
    "noOutputDesc": "The selected distribution has not enabled real-time logs. This solution will create real-time log configuration and attach to the selected distribution. Please confirm to proceed.",
    "sampleRate": "Sampling rate",
    "sampleRateDesc": "Enter the percentage of log records delivered, from 1% to 100%.",
    "sampleRateError": "Sampling rate must be a number between 1 and 100.",
    "enterSR": "Enter sampling rate",
    "fields": "Fields",
    "fieldsDesc": "Choose the fields to include in the real-time log configuration.",
    "fieldsTips": "Depends on your fields selection, some visualizations in the sample dashboard might not be available.",
    "chooseFields": "Choose fields",
    "kdsShardNum": "Kinesis Data Streams endpoint shard number",
    "shardNumError": "Shard number must be greater than or equal to 1",
    "kdsShardNumDesc": "Specify the number of Shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second.",
    "enableAS": "Enable Auto Scaling",
    "enableASDesc": "Choose Yes if you want the system automatically scale the shard numbers. The maximum number of scaling within 24 hours is 8.",
    "kdsMaxShard": "Maximum shard number for Kinesis Data Streams endpoint",
    "kdsMaxShardDesc": "Specify maximum number of shards.",
    "maxShardNumError": "Max shard number must be greater than Shard number",
    "manualNotSupportRTL": "The solution does not support manual creation method for real-time logs now.",
    "logProcessNotSupport": "Fields enrichment is not supported for CloudFront realtime logs.",
    "standardLogs": "Standard logs",
    "realtimeLogs": "Real-time logs",
    "selectAllFields": "Select all available fields (recommended)",
    "customFields": "Custom field selection",
    "diffRegion": "The Standard logs is saved at {{bucket}} that is in a different region ({{logRegion}}) than the solution' home region ({{homeRegion}}). To proceed, please update the selected CloudFront's logging configuration to deliver standard logs to a Amazon S3 bucket at {{homeRegion}} region.",
    "desc": {
      "ingest": "CloudFront logs provide detailed records about every request made to a distribution.",
      "cloudfrontLog": "Amazon CloudFront Access Logs"
    }
  },
  "lambda": {
    "alert": "The log processing layer must have access to Amazon CloudWatch Logs.",
    "name": "Lambda function",
    "nameDesc": "Select an existing Lambda function from the selected AWS account.",
    "lambdaError": "Please select a Lambda function",
    "selectLambda": "Select a Lambda function",
    "desc": {
      "ingest": "AWS Lambda automatically monitors Lambda functions on your behalf and sends function metrics to Amazon CloudWatch. Ingest Lambda logs from CloudWatch.",
      "lambdaLog": "AWS Lambda Server Access Logs"
    }
  },
  "elb": {
    "logCreation": "Log creation",
    "title": "Elastic Load Balancing",
    "alb": "Application Load Balancer",
    "albDesc": "Select an existing ALB from the selected AWS account.",
    "selectALB": "Select an Application Loader Balancer",
    "albName": "Application Load Balancer identifier",
    "albNameDesc": "Input the ALB name. This just for identification purpose.",
    "inputALB": "ALB name",
    "elbEmptyError": "Please select an Application Load Balancer",
    "needEnableLogging": "Please enable the ALB logging",
    "noLogOutput": "No log output",
    "noLogOutputDesc": "The log configuration of the selected ALB has not been enabled. Would you like to enable logging and output to the ",
    "desc": {
      "ingest": "Elastic Load Balancing (ELB) provide access log that capture detailed information about requests sent to your load balancer. ELB publishes a log file for each load balancer node every 5 minutes.",
      "elbLog": "ELB Access Logs"
    }
  },
  "waf": {
    "logCreation": "Log creation",
    "title": "AWS WAF",
    "acl": "Web ACL",
    "aclDesc": "Select an existing Web ACL from the selected AWS account.",
    "selectWAF": "Select an Web ACL",
    "aclName": "Web ACL",
    "aclNameDesc": "Input the Web ACL name. This just for identification purpose.",
    "inputWAF": "Web ACL name",
    "aclEmptyError": "Please select an Web ACL",
    "manualAclEmptyError": "Please input Web ACL name",
    "ingestOption": "Ingest Options",
    "ingestOptionDesc": "By default, the solution will ingest sampled requests.",
    "sampledRequest": "Sampled Request",
    "fullRequest": "Full Request",
    "sampleSchedule": "Sampling Interval",
    "sampleScheduleDesc": "Specify the interval for the sampling in minutes. Valid intervals are 2 ~ 180 minutes.",
    "sampleScheduleError": "Sampling Schedule must large than or equal 2 and less than 180",
    "needEnableLogging": "Please enable the Web ACL logging",
    "noLogOutput": "No log output",
    "noLogOutputDesc": "The log configuration of the selected Web ACL has not been enabled. Would you like to enable logging and output to the Amazon S3",
    "diffRegion": "The WAF Logs is saved at {{bucket}} that is in a different region ({{logRegion}}) than the solution' home region ({{homeRegion}}). To proceed, please manually change or disable the logging for the selected WAF ACL, and then try again.",
    "sourceWAFTip": "Solution detects the selected WAF deliver its logs to Amazon S3 directly, the cost of this method is more expensive than delivering through Kinesis Data Firehose (KDF) to Amazon S3. If you want to switch to KDF, you can disable the logging of the select  WAF first, then Solution can automatically enable the KDF-based logging delivery method for you.",
    "sourceWAFTips1": "Solution detects the web ACL's logs directly to ",
    "sourceWAFTips2": "the cost of this method is more expensive (",
    "sourceWAFTips3": "about 2.4 times",
    "sourceWAFTips4": ") than using the Kinesis Data Firehose (KDF) stream to collect and store to Amazon S3. If you want to switch to KDF, you can disable the log configuration of the WAF first, and then Solution can automatically enable the KDF-based log collection configuration for you and create a log analysis pipeline.",
    "desc": {
      "ingest": "WAF access logs provide detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resources, detailed information about the request, and details about the rules that the request matched.",
      "wafLog": "Web ACL Access Logs"
    }
  },
  "vpc": {
    "vpclogEnable": "VPC Flow Log enabling",
    "title": "VPC Flow Log",
    "alert": "Only supports VPC Logs from Amazon S3 in same region, and the Log Processing Layer must have access to Amazon S3.",
    "vpc": "VPC",
    "vpcDesc": "Select an existing VPC from the selected AWS account.",
    "vpcLog": "VPC Flow Logs",
    "vpcEmptyError": "Please select a VPC",
    "notEnableTips": "The VPC has not enabled VPC Flow Logs. The system will enable the logs.",
    "selectVpc": "Select a VPC",
    "vpcName": "VPC ID",
    "vpcNameDesc": "Input the VPC ID which the VPC Flow Logs belongs to.",
    "inputVpc": "Please input VPC ID",
    "needEnableLogging": "Please enable the VPC Flow Log",
    "noLogOutput": "No log output",
    "noLogOutputDesc": "The log configuration of the selected VPC has not been enabled, or doesn't have Amazon S3 as log destination. Would you like to enable logging and output to the Amazon S3 ",
    "logSource": "Log source",
    "logSourceDesc": "Specify the VPC log source location.",
    "logSourceEmptyError": "Please select Log source",
    "logCWLEnabled1": "The solution has create a flow log, ",
    "logCWLEnabled2": " with CloudWatch Logs as destination.",
    "chooseLogSource": "Choose a log source",
    "vpcNoOutput": "No log output to CloudWatch Logs",
    "vpcNoLogOutputDesc": "The selected VPC has not enabled CloudWatch Logs as log destination. Would you like the solution to enable logging with CloudWatch Logs as destination? Please note, standard CloudWatch and CloudWatch Logs charges apply.",
    "selectFlowLog": "Select a flow log",
    "selectFlowLogDesc": "Select a flow log to ingest into OpenSearch",
    "alertCWL": "The Log Processing Layer must have access to CloudWatch Logs log group.",
    "needEnableLoggingCWL": "Please enable the CloudWatch Logs as vpc flow log destination",
    "selectVPCFlowLog": "Please select a vpc flow log",
    "diffRegionTip": "Tips",
    "diffRegion": "The VPC Flow Logs is saved at {{bucket}} that is in a different region ({{logRegion}}) than the solution' home region ({{homeRegion}}). To proceed, please select a different flow log or allow the solution to create a new flow log for the selected VPC.",
    "desc": {
      "ingest": "VPC Flow Logs enable you to capture information about the IP traffic going to and from network interfaces in your VPC."
    }
  },
  "config": {
    "logCreation": "Log creation",
    "title": "AWS Config",
    "configName": "Config",
    "configNameDesc": "Input the Config name from the selected AWS account. This is just for identification purpose.",
    "configNameEmptyError": "The AWS Config name cannot be empty",
    "inputConfig": "AWS Config name",
    "needEnableLogging": "Please enable AWS Config in current region first, and make sure the Amazon S3 bucket which stores logs is same as the Solution region.",
    "alreadyEnabled": "Current Account AWS Config Log Ingestion  has been enabled. If you want to recreate an ingestion, please delete the previously ingestion first",
    "desc": {
      "ingest": "By default, AWS Config delivers configuration history and snapshot files to your Amazon S3 bucket.",
      "configLog": "AWS Config Logs"
    }
  }
}
